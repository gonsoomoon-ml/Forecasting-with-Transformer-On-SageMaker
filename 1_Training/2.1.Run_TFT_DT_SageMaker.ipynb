{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4b27d946-a068-458a-9a19-a4e1f4abfcf0",
   "metadata": {
    "tags": []
   },
   "source": [
    "# [모듈 2.1] 세이지메이커에서 분산 훈련 하기\n",
    "\n",
    "이 노트북은 커널을 'conda_python3' 를 사용합니다.\n",
    "\n",
    "---\n",
    "이 노트북은 PyTorch Lightning 의 Multi GPUs 기능으로 1개의 인스턴스에서 (ml.g4dn.12xlarge) 에서 훈련 합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7444f2fd-db36-4489-a066-8bca587dc171",
   "metadata": {},
   "source": [
    "# 1. 환경 설정\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27b9bd08-1fe9-4306-ae4b-8cd0a99ac564",
   "metadata": {},
   "source": [
    "## 기본 세팅\n",
    "사용하는 패키지는 import 시점에 다시 재로딩 합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e5e68172-0ed2-448b-ad99-0659323e4848",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import sys\n",
    "sys.path.append('./scripts')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "97204ea5-68c0-4f1e-8dec-0318772d38a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sagemaker\n",
    "\n",
    "sagemaker.__version__\n",
    "\n",
    "# sagemaker_session = sagemaker.Session()\n",
    "role = sagemaker.get_execution_role()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "772a23fa-eee6-4b29-945e-751105551487",
   "metadata": {},
   "source": [
    "## 파라미터 세팅"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f41139d8-0a16-4a4b-874e-895745acd082",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_gpus:  8\n",
      "epochs:  2\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import os\n",
    "\n",
    "epochs = 2\n",
    "num_gpus = torch.cuda.device_count()\n",
    "# model_dir = 'model'\n",
    "# num_gpus = 4\n",
    "# train_notebook = True\n",
    "\n",
    "print(\"num_gpus: \", num_gpus)\n",
    "print(\"epochs: \", epochs)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a28bcda9-79ef-4f79-9721-ee13e2618439",
   "metadata": {},
   "source": [
    "# 2. 세이지 메이크 로컬 모드 훈련\n",
    "#### 로컬의 GPU, CPU 여부로 instance_type 결정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "07a8e179-f1a7-40b3-a275-fcba9bf11ede",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tue Mar  7 06:27:29 2023       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 515.65.01    Driver Version: 515.65.01    CUDA Version: 11.7     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  Tesla V100-SXM2...  On   | 00000000:00:17.0 Off |                    0 |\n",
      "| N/A   28C    P0    40W / 300W |      3MiB / 16384MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   1  Tesla V100-SXM2...  On   | 00000000:00:18.0 Off |                    0 |\n",
      "| N/A   27C    P0    41W / 300W |      3MiB / 16384MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   2  Tesla V100-SXM2...  On   | 00000000:00:19.0 Off |                    0 |\n",
      "| N/A   26C    P0    38W / 300W |      3MiB / 16384MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   3  Tesla V100-SXM2...  On   | 00000000:00:1A.0 Off |                    0 |\n",
      "| N/A   27C    P0    41W / 300W |      3MiB / 16384MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   4  Tesla V100-SXM2...  On   | 00000000:00:1B.0 Off |                    0 |\n",
      "| N/A   27C    P0    41W / 300W |      3MiB / 16384MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   5  Tesla V100-SXM2...  On   | 00000000:00:1C.0 Off |                    0 |\n",
      "| N/A   27C    P0    41W / 300W |      3MiB / 16384MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   6  Tesla V100-SXM2...  On   | 00000000:00:1D.0 Off |                    0 |\n",
      "| N/A   28C    P0    41W / 300W |      3MiB / 16384MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   7  Tesla V100-SXM2...  On   | 00000000:00:1E.0 Off |                    0 |\n",
      "| N/A   27C    P0    43W / 300W |      3MiB / 16384MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "|  No running processes found                                                 |\n",
      "+-----------------------------------------------------------------------------+\n",
      "Instance type = local_gpu\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import subprocess\n",
    "\n",
    "\n",
    "try:\n",
    "    if subprocess.call(\"nvidia-smi\") == 0:\n",
    "        ## Set type to GPU if one is present\n",
    "        instance_type = \"local_gpu\"\n",
    "    else:\n",
    "        instance_type = \"local\"        \n",
    "except:\n",
    "    pass\n",
    "\n",
    "print(\"Instance type = \" + instance_type)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e25ed9c-3f2a-40b1-ae9b-6ec7b8b8687f",
   "metadata": {},
   "source": [
    "## 로컬 모드로 훈련 실행\n",
    "- 아래의 두 라인이 로컬모드로 훈련을 지시 합니다.\n",
    "```python\n",
    "    instance_type=instance_type, # local_gpu or local 지정\n",
    "    session = sagemaker.LocalSession(), # 로컬 세션을 사용합니다.\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bdb1893b-20a3-4867-bdac-eec5206e1867",
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperparameters = {'epochs': epochs, \n",
    "                   'n_gpus': num_gpus,\n",
    "                    }  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2e76fe8b-e1ed-4438-8d2d-3a662f37c558",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating z5ar2vrarj-algo-1-zx6c3 ... \n",
      "Creating z5ar2vrarj-algo-1-zx6c3 ... done\n",
      "Attaching to z5ar2vrarj-algo-1-zx6c3\n",
      "\u001b[36mz5ar2vrarj-algo-1-zx6c3 |\u001b[0m 2023-03-07 06:27:41,386 sagemaker-training-toolkit INFO     Imported framework sagemaker_pytorch_container.training\n",
      "\u001b[36mz5ar2vrarj-algo-1-zx6c3 |\u001b[0m 2023-03-07 06:27:41,451 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\n",
      "\u001b[36mz5ar2vrarj-algo-1-zx6c3 |\u001b[0m 2023-03-07 06:27:41,461 sagemaker-training-toolkit INFO     instance_groups entry not present in resource_config\n",
      "\u001b[36mz5ar2vrarj-algo-1-zx6c3 |\u001b[0m 2023-03-07 06:27:41,464 sagemaker_pytorch_container.training INFO     Block until all host DNS lookups succeed.\n",
      "\u001b[36mz5ar2vrarj-algo-1-zx6c3 |\u001b[0m 2023-03-07 06:27:41,471 sagemaker_pytorch_container.training INFO     Invoking user training script.\n",
      "\u001b[36mz5ar2vrarj-algo-1-zx6c3 |\u001b[0m 2023-03-07 06:27:41,537 botocore.credentials INFO     Found credentials from IAM Role: BaseNotebookInstanceEc2InstanceRole\n",
      "\u001b[36mz5ar2vrarj-algo-1-zx6c3 |\u001b[0m 2023-03-07 06:27:41,753 sagemaker-training-toolkit INFO     Installing dependencies from requirements.txt:\n",
      "\u001b[36mz5ar2vrarj-algo-1-zx6c3 |\u001b[0m /opt/conda/bin/python3.8 -m pip install -r requirements.txt\n",
      "\u001b[36mz5ar2vrarj-algo-1-zx6c3 |\u001b[0m Collecting pytorch-forecasting==0.10.3\n",
      "\u001b[36mz5ar2vrarj-algo-1-zx6c3 |\u001b[0m Downloading pytorch_forecasting-0.10.3-py3-none-any.whl (141 kB)\n",
      "\u001b[36mz5ar2vrarj-algo-1-zx6c3 |\u001b[0m ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 141.4/141.4 kB 5.1 MB/s eta 0:00:00\n",
      "\u001b[36mz5ar2vrarj-algo-1-zx6c3 |\u001b[0m \n",
      "\u001b[36mz5ar2vrarj-algo-1-zx6c3 |\u001b[0m Collecting pytorch-lightning==1.6.3\n",
      "\u001b[36mz5ar2vrarj-algo-1-zx6c3 |\u001b[0m Downloading pytorch_lightning-1.6.3-py3-none-any.whl (584 kB)\n",
      "\u001b[36mz5ar2vrarj-algo-1-zx6c3 |\u001b[0m ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 584.0/584.0 kB 43.3 MB/s eta 0:00:00\n",
      "\u001b[36mz5ar2vrarj-algo-1-zx6c3 |\u001b[0m \n",
      "\u001b[36mz5ar2vrarj-algo-1-zx6c3 |\u001b[0m Requirement already satisfied: pyarrow==11.0.0 in /opt/conda/lib/python3.8/site-packages (from -r requirements.txt (line 5)) (11.0.0)\n",
      "\u001b[36mz5ar2vrarj-algo-1-zx6c3 |\u001b[0m Collecting tensorboard==2.12.0\n",
      "\u001b[36mz5ar2vrarj-algo-1-zx6c3 |\u001b[0m Downloading tensorboard-2.12.0-py3-none-any.whl (5.6 MB)\n",
      "\u001b[36mz5ar2vrarj-algo-1-zx6c3 |\u001b[0m ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 5.6/5.6 MB 72.1 MB/s eta 0:00:00\n",
      "\u001b[36mz5ar2vrarj-algo-1-zx6c3 |\u001b[0m \n",
      "\u001b[36mz5ar2vrarj-algo-1-zx6c3 |\u001b[0m Collecting scikit-learn<1.2,>=0.24\n",
      "\u001b[36mz5ar2vrarj-algo-1-zx6c3 |\u001b[0m Downloading scikit_learn-1.1.3-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (31.2 MB)\n",
      "\u001b[36mz5ar2vrarj-algo-1-zx6c3 |\u001b[0m ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 31.2/31.2 MB 45.5 MB/s eta 0:00:00\n",
      "\u001b[36mz5ar2vrarj-algo-1-zx6c3 |\u001b[0m \n",
      "\u001b[36mz5ar2vrarj-algo-1-zx6c3 |\u001b[0m Collecting statsmodels\n",
      "\u001b[36mz5ar2vrarj-algo-1-zx6c3 |\u001b[0m Downloading statsmodels-0.13.5-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (9.9 MB)\n",
      "\u001b[36mz5ar2vrarj-algo-1-zx6c3 |\u001b[0m ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 9.9/9.9 MB 69.9 MB/s eta 0:00:00\n",
      "\u001b[36mz5ar2vrarj-algo-1-zx6c3 |\u001b[0m \n",
      "\u001b[36mz5ar2vrarj-algo-1-zx6c3 |\u001b[0m Requirement already satisfied: matplotlib in /opt/conda/lib/python3.8/site-packages (from pytorch-forecasting==0.10.3->-r requirements.txt (line 2)) (3.7.0)\n",
      "\u001b[36mz5ar2vrarj-algo-1-zx6c3 |\u001b[0m Requirement already satisfied: scipy<2.0,>=1.8 in /opt/conda/lib/python3.8/site-packages (from pytorch-forecasting==0.10.3->-r requirements.txt (line 2)) (1.10.0)\n",
      "\u001b[36mz5ar2vrarj-algo-1-zx6c3 |\u001b[0m Collecting optuna<3.0.0,>=2.3.0\n",
      "\u001b[36mz5ar2vrarj-algo-1-zx6c3 |\u001b[0m Downloading optuna-2.10.1-py3-none-any.whl (308 kB)\n",
      "\u001b[36mz5ar2vrarj-algo-1-zx6c3 |\u001b[0m ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 308.2/308.2 kB 33.7 MB/s eta 0:00:00\n",
      "\u001b[36mz5ar2vrarj-algo-1-zx6c3 |\u001b[0m \n",
      "\u001b[36mz5ar2vrarj-algo-1-zx6c3 |\u001b[0m Requirement already satisfied: pandas<2.0.0,>=1.3.0 in /opt/conda/lib/python3.8/site-packages (from pytorch-forecasting==0.10.3->-r requirements.txt (line 2)) (1.5.3)\n",
      "\u001b[36mz5ar2vrarj-algo-1-zx6c3 |\u001b[0m Requirement already satisfied: torch<2.0,>=1.7 in /opt/conda/lib/python3.8/site-packages (from pytorch-forecasting==0.10.3->-r requirements.txt (line 2)) (1.12.1+cu113)\n",
      "\u001b[36mz5ar2vrarj-algo-1-zx6c3 |\u001b[0m Requirement already satisfied: fsspec[http]!=2021.06.0,>=2021.05.0 in /opt/conda/lib/python3.8/site-packages (from pytorch-lightning==1.6.3->-r requirements.txt (line 4)) (2023.1.0)\n",
      "\u001b[36mz5ar2vrarj-algo-1-zx6c3 |\u001b[0m Requirement already satisfied: numpy>=1.17.2 in /opt/conda/lib/python3.8/site-packages (from pytorch-lightning==1.6.3->-r requirements.txt (line 4)) (1.23.5)\n",
      "\u001b[36mz5ar2vrarj-algo-1-zx6c3 |\u001b[0m Requirement already satisfied: typing-extensions>=4.0.0 in /opt/conda/lib/python3.8/site-packages (from pytorch-lightning==1.6.3->-r requirements.txt (line 4)) (4.4.0)\n",
      "\u001b[36mz5ar2vrarj-algo-1-zx6c3 |\u001b[0m Requirement already satisfied: tqdm>=4.57.0 in /opt/conda/lib/python3.8/site-packages (from pytorch-lightning==1.6.3->-r requirements.txt (line 4)) (4.64.1)\n",
      "\u001b[36mz5ar2vrarj-algo-1-zx6c3 |\u001b[0m Collecting torchmetrics>=0.4.1\n",
      "\u001b[36mz5ar2vrarj-algo-1-zx6c3 |\u001b[0m Downloading torchmetrics-0.11.3-py3-none-any.whl (518 kB)\n",
      "\u001b[36mz5ar2vrarj-algo-1-zx6c3 |\u001b[0m ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 518.6/518.6 kB 46.3 MB/s eta 0:00:00\n",
      "\u001b[36mz5ar2vrarj-algo-1-zx6c3 |\u001b[0m \n",
      "\u001b[36mz5ar2vrarj-algo-1-zx6c3 |\u001b[0m Requirement already satisfied: PyYAML>=5.4 in /opt/conda/lib/python3.8/site-packages (from pytorch-lightning==1.6.3->-r requirements.txt (line 4)) (5.4.1)\n",
      "\u001b[36mz5ar2vrarj-algo-1-zx6c3 |\u001b[0m Requirement already satisfied: packaging>=17.0 in /opt/conda/lib/python3.8/site-packages (from pytorch-lightning==1.6.3->-r requirements.txt (line 4)) (23.0)\n",
      "\u001b[36mz5ar2vrarj-algo-1-zx6c3 |\u001b[0m Collecting pyDeprecate<0.4.0,>=0.3.1\n",
      "\u001b[36mz5ar2vrarj-algo-1-zx6c3 |\u001b[0m Downloading pyDeprecate-0.3.2-py3-none-any.whl (10 kB)\n",
      "\u001b[36mz5ar2vrarj-algo-1-zx6c3 |\u001b[0m Requirement already satisfied: protobuf>=3.19.6 in /opt/conda/lib/python3.8/site-packages (from tensorboard==2.12.0->-r requirements.txt (line 6)) (3.20.2)\n",
      "\u001b[36mz5ar2vrarj-algo-1-zx6c3 |\u001b[0m Requirement already satisfied: wheel>=0.26 in /opt/conda/lib/python3.8/site-packages (from tensorboard==2.12.0->-r requirements.txt (line 6)) (0.38.4)\n",
      "\u001b[36mz5ar2vrarj-algo-1-zx6c3 |\u001b[0m Collecting markdown>=2.6.8\n",
      "\u001b[36mz5ar2vrarj-algo-1-zx6c3 |\u001b[0m Downloading Markdown-3.4.1-py3-none-any.whl (93 kB)\n",
      "\u001b[36mz5ar2vrarj-algo-1-zx6c3 |\u001b[0m ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 93.3/93.3 kB 15.5 MB/s eta 0:00:00\n",
      "\u001b[36mz5ar2vrarj-algo-1-zx6c3 |\u001b[0m Collecting google-auth<3,>=1.6.3\n",
      "\u001b[36mz5ar2vrarj-algo-1-zx6c3 |\u001b[0m Downloading google_auth-2.16.2-py2.py3-none-any.whl (177 kB)\n",
      "\u001b[36mz5ar2vrarj-algo-1-zx6c3 |\u001b[0m ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 177.2/177.2 kB 28.0 MB/s eta 0:00:00\n",
      "\u001b[36mz5ar2vrarj-algo-1-zx6c3 |\u001b[0m \n",
      "\u001b[36mz5ar2vrarj-algo-1-zx6c3 |\u001b[0m Collecting tensorboard-plugin-wit>=1.6.0\n",
      "\u001b[36mz5ar2vrarj-algo-1-zx6c3 |\u001b[0m Downloading tensorboard_plugin_wit-1.8.1-py3-none-any.whl (781 kB)\n",
      "\u001b[36mz5ar2vrarj-algo-1-zx6c3 |\u001b[0m ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 781.3/781.3 kB 18.8 MB/s eta 0:00:00\n",
      "\u001b[36mz5ar2vrarj-algo-1-zx6c3 |\u001b[0m \n",
      "\u001b[36mz5ar2vrarj-algo-1-zx6c3 |\u001b[0m Collecting google-auth-oauthlib<0.5,>=0.4.1\n",
      "\u001b[36mz5ar2vrarj-algo-1-zx6c3 |\u001b[0m Downloading google_auth_oauthlib-0.4.6-py2.py3-none-any.whl (18 kB)\n",
      "\u001b[36mz5ar2vrarj-algo-1-zx6c3 |\u001b[0m Collecting grpcio>=1.48.2\n",
      "\u001b[36mz5ar2vrarj-algo-1-zx6c3 |\u001b[0m Downloading grpcio-1.51.3-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.8 MB)\n",
      "\u001b[36mz5ar2vrarj-algo-1-zx6c3 |\u001b[0m ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 4.8/4.8 MB 77.3 MB/s eta 0:00:00\n",
      "\u001b[36mz5ar2vrarj-algo-1-zx6c3 |\u001b[0m \n",
      "\u001b[36mz5ar2vrarj-algo-1-zx6c3 |\u001b[0m Requirement already satisfied: werkzeug>=1.0.1 in /opt/conda/lib/python3.8/site-packages (from tensorboard==2.12.0->-r requirements.txt (line 6)) (2.2.3)\n",
      "\u001b[36mz5ar2vrarj-algo-1-zx6c3 |\u001b[0m Collecting absl-py>=0.4\n",
      "\u001b[36mz5ar2vrarj-algo-1-zx6c3 |\u001b[0m Downloading absl_py-1.4.0-py3-none-any.whl (126 kB)\n",
      "\u001b[36mz5ar2vrarj-algo-1-zx6c3 |\u001b[0m ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 126.5/126.5 kB 20.1 MB/s eta 0:00:00\n",
      "\u001b[36mz5ar2vrarj-algo-1-zx6c3 |\u001b[0m \n",
      "\u001b[36mz5ar2vrarj-algo-1-zx6c3 |\u001b[0m Collecting tensorboard-data-server<0.8.0,>=0.7.0\n",
      "\u001b[36mz5ar2vrarj-algo-1-zx6c3 |\u001b[0m Downloading tensorboard_data_server-0.7.0-py3-none-manylinux2014_x86_64.whl (6.6 MB)\n",
      "\u001b[36mz5ar2vrarj-algo-1-zx6c3 |\u001b[0m ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 6.6/6.6 MB 87.0 MB/s eta 0:00:00\n",
      "\u001b[36mz5ar2vrarj-algo-1-zx6c3 |\u001b[0m \n",
      "\u001b[36mz5ar2vrarj-algo-1-zx6c3 |\u001b[0m Requirement already satisfied: requests<3,>=2.21.0 in /opt/conda/lib/python3.8/site-packages (from tensorboard==2.12.0->-r requirements.txt (line 6)) (2.28.2)\n",
      "\u001b[36mz5ar2vrarj-algo-1-zx6c3 |\u001b[0m Requirement already satisfied: setuptools>=41.0.0 in /opt/conda/lib/python3.8/site-packages (from tensorboard==2.12.0->-r requirements.txt (line 6)) (65.6.3)\n",
      "\u001b[36mz5ar2vrarj-algo-1-zx6c3 |\u001b[0m Collecting aiohttp!=4.0.0a0,!=4.0.0a1\n",
      "\u001b[36mz5ar2vrarj-algo-1-zx6c3 |\u001b[0m Downloading aiohttp-3.8.4-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.0 MB)\n",
      "\u001b[36mz5ar2vrarj-algo-1-zx6c3 |\u001b[0m ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1.0/1.0 MB 64.4 MB/s eta 0:00:00\n",
      "\u001b[36mz5ar2vrarj-algo-1-zx6c3 |\u001b[0m \n",
      "\u001b[36mz5ar2vrarj-algo-1-zx6c3 |\u001b[0m Requirement already satisfied: six>=1.9.0 in /opt/conda/lib/python3.8/site-packages (from google-auth<3,>=1.6.3->tensorboard==2.12.0->-r requirements.txt (line 6)) (1.16.0)\n",
      "\u001b[36mz5ar2vrarj-algo-1-zx6c3 |\u001b[0m Requirement already satisfied: rsa<5,>=3.1.4 in /opt/conda/lib/python3.8/site-packages (from google-auth<3,>=1.6.3->tensorboard==2.12.0->-r requirements.txt (line 6)) (4.7.2)\n",
      "\u001b[36mz5ar2vrarj-algo-1-zx6c3 |\u001b[0m Collecting cachetools<6.0,>=2.0.0\n",
      "\u001b[36mz5ar2vrarj-algo-1-zx6c3 |\u001b[0m Downloading cachetools-5.3.0-py3-none-any.whl (9.3 kB)\n",
      "\u001b[36mz5ar2vrarj-algo-1-zx6c3 |\u001b[0m Collecting pyasn1-modules>=0.2.1\n",
      "\u001b[36mz5ar2vrarj-algo-1-zx6c3 |\u001b[0m Downloading pyasn1_modules-0.2.8-py2.py3-none-any.whl (155 kB)\n",
      "\u001b[36mz5ar2vrarj-algo-1-zx6c3 |\u001b[0m ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 155.3/155.3 kB 20.5 MB/s eta 0:00:00\n",
      "\u001b[36mz5ar2vrarj-algo-1-zx6c3 |\u001b[0m \n",
      "\u001b[36mz5ar2vrarj-algo-1-zx6c3 |\u001b[0m Collecting requests-oauthlib>=0.7.0\n",
      "\u001b[36mz5ar2vrarj-algo-1-zx6c3 |\u001b[0m Downloading requests_oauthlib-1.3.1-py2.py3-none-any.whl (23 kB)\n",
      "\u001b[36mz5ar2vrarj-algo-1-zx6c3 |\u001b[0m Requirement already satisfied: importlib-metadata>=4.4 in /opt/conda/lib/python3.8/site-packages (from markdown>=2.6.8->tensorboard==2.12.0->-r requirements.txt (line 6)) (4.13.0)\n",
      "\u001b[36mz5ar2vrarj-algo-1-zx6c3 |\u001b[0m Collecting cmaes>=0.8.2\n",
      "\u001b[36mz5ar2vrarj-algo-1-zx6c3 |\u001b[0m Downloading cmaes-0.9.1-py3-none-any.whl (21 kB)\n",
      "\u001b[36mz5ar2vrarj-algo-1-zx6c3 |\u001b[0m Collecting cliff\n",
      "\u001b[36mz5ar2vrarj-algo-1-zx6c3 |\u001b[0m Downloading cliff-4.2.0-py3-none-any.whl (81 kB)\n",
      "\u001b[36mz5ar2vrarj-algo-1-zx6c3 |\u001b[0m ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 81.0/81.0 kB 14.0 MB/s eta 0:00:00\n",
      "\u001b[36mz5ar2vrarj-algo-1-zx6c3 |\u001b[0m \n",
      "\u001b[36mz5ar2vrarj-algo-1-zx6c3 |\u001b[0m Collecting alembic\n",
      "\u001b[36mz5ar2vrarj-algo-1-zx6c3 |\u001b[0m Downloading alembic-1.10.1-py3-none-any.whl (212 kB)\n",
      "\u001b[36mz5ar2vrarj-algo-1-zx6c3 |\u001b[0m ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 212.2/212.2 kB 31.2 MB/s eta 0:00:00\n",
      "\u001b[36mz5ar2vrarj-algo-1-zx6c3 |\u001b[0m \n",
      "\u001b[36mz5ar2vrarj-algo-1-zx6c3 |\u001b[0m Collecting sqlalchemy>=1.1.0\n",
      "\u001b[36mz5ar2vrarj-algo-1-zx6c3 |\u001b[0m Downloading SQLAlchemy-2.0.5.post1-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.8 MB)\n",
      "\u001b[36mz5ar2vrarj-algo-1-zx6c3 |\u001b[0m ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 2.8/2.8 MB 79.1 MB/s eta 0:00:00\n",
      "\u001b[36mz5ar2vrarj-algo-1-zx6c3 |\u001b[0m \n",
      "\u001b[36mz5ar2vrarj-algo-1-zx6c3 |\u001b[0m Collecting colorlog\n",
      "\u001b[36mz5ar2vrarj-algo-1-zx6c3 |\u001b[0m Downloading colorlog-6.7.0-py2.py3-none-any.whl (11 kB)\n",
      "\u001b[36mz5ar2vrarj-algo-1-zx6c3 |\u001b[0m Requirement already satisfied: python-dateutil>=2.8.1 in /opt/conda/lib/python3.8/site-packages (from pandas<2.0.0,>=1.3.0->pytorch-forecasting==0.10.3->-r requirements.txt (line 2)) (2.8.2)\n",
      "\u001b[36mz5ar2vrarj-algo-1-zx6c3 |\u001b[0m Requirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.8/site-packages (from pandas<2.0.0,>=1.3.0->pytorch-forecasting==0.10.3->-r requirements.txt (line 2)) (2022.7.1)\n",
      "\u001b[36mz5ar2vrarj-algo-1-zx6c3 |\u001b[0m Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.8/site-packages (from requests<3,>=2.21.0->tensorboard==2.12.0->-r requirements.txt (line 6)) (3.4)\n",
      "\u001b[36mz5ar2vrarj-algo-1-zx6c3 |\u001b[0m Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.8/site-packages (from requests<3,>=2.21.0->tensorboard==2.12.0->-r requirements.txt (line 6)) (2022.12.7)\n",
      "\u001b[36mz5ar2vrarj-algo-1-zx6c3 |\u001b[0m Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.8/site-packages (from requests<3,>=2.21.0->tensorboard==2.12.0->-r requirements.txt (line 6)) (1.26.14)\n",
      "\u001b[36mz5ar2vrarj-algo-1-zx6c3 |\u001b[0m Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.8/site-packages (from requests<3,>=2.21.0->tensorboard==2.12.0->-r requirements.txt (line 6)) (2.1.1)\n",
      "\u001b[36mz5ar2vrarj-algo-1-zx6c3 |\u001b[0m Requirement already satisfied: threadpoolctl>=2.0.0 in /opt/conda/lib/python3.8/site-packages (from scikit-learn<1.2,>=0.24->pytorch-forecasting==0.10.3->-r requirements.txt (line 2)) (3.1.0)\n",
      "\u001b[36mz5ar2vrarj-algo-1-zx6c3 |\u001b[0m Requirement already satisfied: joblib>=1.0.0 in /opt/conda/lib/python3.8/site-packages (from scikit-learn<1.2,>=0.24->pytorch-forecasting==0.10.3->-r requirements.txt (line 2)) (1.2.0)\n",
      "\u001b[36mz5ar2vrarj-algo-1-zx6c3 |\u001b[0m Requirement already satisfied: MarkupSafe>=2.1.1 in /opt/conda/lib/python3.8/site-packages (from werkzeug>=1.0.1->tensorboard==2.12.0->-r requirements.txt (line 6)) (2.1.2)\n",
      "\u001b[36mz5ar2vrarj-algo-1-zx6c3 |\u001b[0m Requirement already satisfied: importlib-resources>=3.2.0 in /opt/conda/lib/python3.8/site-packages (from matplotlib->pytorch-forecasting==0.10.3->-r requirements.txt (line 2)) (5.10.2)\n",
      "\u001b[36mz5ar2vrarj-algo-1-zx6c3 |\u001b[0m Requirement already satisfied: pyparsing>=2.3.1 in /opt/conda/lib/python3.8/site-packages (from matplotlib->pytorch-forecasting==0.10.3->-r requirements.txt (line 2)) (3.0.9)\n",
      "\u001b[36mz5ar2vrarj-algo-1-zx6c3 |\u001b[0m Requirement already satisfied: pillow>=6.2.0 in /opt/conda/lib/python3.8/site-packages (from matplotlib->pytorch-forecasting==0.10.3->-r requirements.txt (line 2)) (9.4.0)\n",
      "\u001b[36mz5ar2vrarj-algo-1-zx6c3 |\u001b[0m Requirement already satisfied: contourpy>=1.0.1 in /opt/conda/lib/python3.8/site-packages (from matplotlib->pytorch-forecasting==0.10.3->-r requirements.txt (line 2)) (1.0.7)\n",
      "\u001b[36mz5ar2vrarj-algo-1-zx6c3 |\u001b[0m Requirement already satisfied: fonttools>=4.22.0 in /opt/conda/lib/python3.8/site-packages (from matplotlib->pytorch-forecasting==0.10.3->-r requirements.txt (line 2)) (4.38.0)\n",
      "\u001b[36mz5ar2vrarj-algo-1-zx6c3 |\u001b[0m Requirement already satisfied: cycler>=0.10 in /opt/conda/lib/python3.8/site-packages (from matplotlib->pytorch-forecasting==0.10.3->-r requirements.txt (line 2)) (0.11.0)\n",
      "\u001b[36mz5ar2vrarj-algo-1-zx6c3 |\u001b[0m Requirement already satisfied: kiwisolver>=1.0.1 in /opt/conda/lib/python3.8/site-packages (from matplotlib->pytorch-forecasting==0.10.3->-r requirements.txt (line 2)) (1.4.4)\n",
      "\u001b[36mz5ar2vrarj-algo-1-zx6c3 |\u001b[0m Collecting patsy>=0.5.2\n",
      "\u001b[36mz5ar2vrarj-algo-1-zx6c3 |\u001b[0m Downloading patsy-0.5.3-py2.py3-none-any.whl (233 kB)\n",
      "\u001b[36mz5ar2vrarj-algo-1-zx6c3 |\u001b[0m ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 233.8/233.8 kB 32.0 MB/s eta 0:00:00\n",
      "\u001b[36mz5ar2vrarj-algo-1-zx6c3 |\u001b[0m \n",
      "\u001b[36mz5ar2vrarj-algo-1-zx6c3 |\u001b[0m Collecting async-timeout<5.0,>=4.0.0a3\n",
      "\u001b[36mz5ar2vrarj-algo-1-zx6c3 |\u001b[0m Downloading async_timeout-4.0.2-py3-none-any.whl (5.8 kB)\n",
      "\u001b[36mz5ar2vrarj-algo-1-zx6c3 |\u001b[0m Collecting multidict<7.0,>=4.5\n",
      "\u001b[36mz5ar2vrarj-algo-1-zx6c3 |\u001b[0m Downloading multidict-6.0.4-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (121 kB)\n",
      "\u001b[36mz5ar2vrarj-algo-1-zx6c3 |\u001b[0m ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 121.3/121.3 kB 19.6 MB/s eta 0:00:00\n",
      "\u001b[36mz5ar2vrarj-algo-1-zx6c3 |\u001b[0m \n",
      "\u001b[36mz5ar2vrarj-algo-1-zx6c3 |\u001b[0m Collecting frozenlist>=1.1.1\n",
      "\u001b[36mz5ar2vrarj-algo-1-zx6c3 |\u001b[0m Downloading frozenlist-1.3.3-cp38-cp38-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (161 kB)\n",
      "\u001b[36mz5ar2vrarj-algo-1-zx6c3 |\u001b[0m ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 161.3/161.3 kB 25.4 MB/s eta 0:00:00\n",
      "\u001b[36mz5ar2vrarj-algo-1-zx6c3 |\u001b[0m \n",
      "\u001b[36mz5ar2vrarj-algo-1-zx6c3 |\u001b[0m Collecting aiosignal>=1.1.2\n",
      "\u001b[36mz5ar2vrarj-algo-1-zx6c3 |\u001b[0m Downloading aiosignal-1.3.1-py3-none-any.whl (7.6 kB)\n",
      "\u001b[36mz5ar2vrarj-algo-1-zx6c3 |\u001b[0m Requirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.8/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning==1.6.3->-r requirements.txt (line 4)) (22.2.0)\n",
      "\u001b[36mz5ar2vrarj-algo-1-zx6c3 |\u001b[0m Collecting yarl<2.0,>=1.0\n",
      "\u001b[36mz5ar2vrarj-algo-1-zx6c3 |\u001b[0m Downloading yarl-1.8.2-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (262 kB)\n",
      "\u001b[36mz5ar2vrarj-algo-1-zx6c3 |\u001b[0m ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 262.1/262.1 kB 35.4 MB/s eta 0:00:00\n",
      "\u001b[36mz5ar2vrarj-algo-1-zx6c3 |\u001b[0m \n",
      "\u001b[36mz5ar2vrarj-algo-1-zx6c3 |\u001b[0m Requirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.8/site-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard==2.12.0->-r requirements.txt (line 6)) (3.13.0)\n",
      "\u001b[36mz5ar2vrarj-algo-1-zx6c3 |\u001b[0m Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /opt/conda/lib/python3.8/site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard==2.12.0->-r requirements.txt (line 6)) (0.4.8)\n",
      "\u001b[36mz5ar2vrarj-algo-1-zx6c3 |\u001b[0m Collecting oauthlib>=3.0.0\n",
      "\u001b[36mz5ar2vrarj-algo-1-zx6c3 |\u001b[0m Downloading oauthlib-3.2.2-py3-none-any.whl (151 kB)\n",
      "\u001b[36mz5ar2vrarj-algo-1-zx6c3 |\u001b[0m ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 151.7/151.7 kB 20.9 MB/s eta 0:00:00\n",
      "\u001b[36mz5ar2vrarj-algo-1-zx6c3 |\u001b[0m \n",
      "\u001b[36mz5ar2vrarj-algo-1-zx6c3 |\u001b[0m Requirement already satisfied: greenlet!=0.4.17 in /opt/conda/lib/python3.8/site-packages (from sqlalchemy>=1.1.0->optuna<3.0.0,>=2.3.0->pytorch-forecasting==0.10.3->-r requirements.txt (line 2)) (2.0.2)\n",
      "\u001b[36mz5ar2vrarj-algo-1-zx6c3 |\u001b[0m Collecting Mako\n",
      "\u001b[36mz5ar2vrarj-algo-1-zx6c3 |\u001b[0m Downloading Mako-1.2.4-py3-none-any.whl (78 kB)\n",
      "\u001b[36mz5ar2vrarj-algo-1-zx6c3 |\u001b[0m ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 78.7/78.7 kB 14.0 MB/s eta 0:00:00\n",
      "\u001b[36mz5ar2vrarj-algo-1-zx6c3 |\u001b[0m \n",
      "\u001b[36mz5ar2vrarj-algo-1-zx6c3 |\u001b[0m Collecting PrettyTable>=0.7.2\n",
      "\u001b[36mz5ar2vrarj-algo-1-zx6c3 |\u001b[0m Downloading prettytable-3.6.0-py3-none-any.whl (27 kB)\n",
      "\u001b[36mz5ar2vrarj-algo-1-zx6c3 |\u001b[0m Collecting autopage>=0.4.0\n",
      "\u001b[36mz5ar2vrarj-algo-1-zx6c3 |\u001b[0m Downloading autopage-0.5.1-py3-none-any.whl (29 kB)\n",
      "\u001b[36mz5ar2vrarj-algo-1-zx6c3 |\u001b[0m Collecting cmd2>=1.0.0\n",
      "\u001b[36mz5ar2vrarj-algo-1-zx6c3 |\u001b[0m Downloading cmd2-2.4.3-py3-none-any.whl (147 kB)\n",
      "\u001b[36mz5ar2vrarj-algo-1-zx6c3 |\u001b[0m ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 147.2/147.2 kB 17.9 MB/s eta 0:00:00\n",
      "\u001b[36mz5ar2vrarj-algo-1-zx6c3 |\u001b[0m \n",
      "\u001b[36mz5ar2vrarj-algo-1-zx6c3 |\u001b[0m Collecting stevedore>=2.0.1\n",
      "\u001b[36mz5ar2vrarj-algo-1-zx6c3 |\u001b[0m Downloading stevedore-5.0.0-py3-none-any.whl (49 kB)\n",
      "\u001b[36mz5ar2vrarj-algo-1-zx6c3 |\u001b[0m ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 49.6/49.6 kB 8.2 MB/s eta 0:00:00\n",
      "\u001b[36mz5ar2vrarj-algo-1-zx6c3 |\u001b[0m \n",
      "\u001b[36mz5ar2vrarj-algo-1-zx6c3 |\u001b[0m Requirement already satisfied: wcwidth>=0.1.7 in /opt/conda/lib/python3.8/site-packages (from cmd2>=1.0.0->cliff->optuna<3.0.0,>=2.3.0->pytorch-forecasting==0.10.3->-r requirements.txt (line 2)) (0.2.6)\n",
      "\u001b[36mz5ar2vrarj-algo-1-zx6c3 |\u001b[0m Collecting pyperclip>=1.6\n",
      "\u001b[36mz5ar2vrarj-algo-1-zx6c3 |\u001b[0m Downloading pyperclip-1.8.2.tar.gz (20 kB)\n",
      "\u001b[36mz5ar2vrarj-algo-1-zx6c3 |\u001b[0m Preparing metadata (setup.py): started\n",
      "\u001b[36mz5ar2vrarj-algo-1-zx6c3 |\u001b[0m Preparing metadata (setup.py): finished with status 'done'\n",
      "\u001b[36mz5ar2vrarj-algo-1-zx6c3 |\u001b[0m Collecting pbr!=2.1.0,>=2.0.0\n",
      "\u001b[36mz5ar2vrarj-algo-1-zx6c3 |\u001b[0m Downloading pbr-5.11.1-py2.py3-none-any.whl (112 kB)\n",
      "\u001b[36mz5ar2vrarj-algo-1-zx6c3 |\u001b[0m ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 112.7/112.7 kB 18.4 MB/s eta 0:00:00\n",
      "\u001b[36mz5ar2vrarj-algo-1-zx6c3 |\u001b[0m \n",
      "\u001b[36mz5ar2vrarj-algo-1-zx6c3 |\u001b[0m Building wheels for collected packages: pyperclip\n",
      "\u001b[36mz5ar2vrarj-algo-1-zx6c3 |\u001b[0m Building wheel for pyperclip (setup.py): started\n",
      "\u001b[36mz5ar2vrarj-algo-1-zx6c3 |\u001b[0m Building wheel for pyperclip (setup.py): finished with status 'done'\n",
      "\u001b[36mz5ar2vrarj-algo-1-zx6c3 |\u001b[0m Created wheel for pyperclip: filename=pyperclip-1.8.2-py3-none-any.whl size=11124 sha256=9be8f04086833956ad8922511d4ca039272b978e7145aca4ef9fe5bac69e6006\n",
      "\u001b[36mz5ar2vrarj-algo-1-zx6c3 |\u001b[0m Stored in directory: /root/.cache/pip/wheels/7f/1a/65/84ff8c386bec21fca6d220ea1f5498a0367883a78dd5ba6122\n",
      "\u001b[36mz5ar2vrarj-algo-1-zx6c3 |\u001b[0m Successfully built pyperclip\n",
      "\u001b[36mz5ar2vrarj-algo-1-zx6c3 |\u001b[0m Installing collected packages: tensorboard-plugin-wit, pyperclip, tensorboard-data-server, sqlalchemy, pyDeprecate, pyasn1-modules, PrettyTable, pbr, patsy, oauthlib, multidict, Mako, grpcio, frozenlist, colorlog, cmd2, cmaes, cachetools, autopage, async-timeout, absl-py, yarl, torchmetrics, stevedore, scikit-learn, requests-oauthlib, markdown, google-auth, alembic, aiosignal, statsmodels, google-auth-oauthlib, cliff, aiohttp, tensorboard, optuna, pytorch-lightning, pytorch-forecasting\n",
      "\u001b[36mz5ar2vrarj-algo-1-zx6c3 |\u001b[0m Attempting uninstall: scikit-learn\n",
      "\u001b[36mz5ar2vrarj-algo-1-zx6c3 |\u001b[0m Found existing installation: scikit-learn 1.2.1\n",
      "\u001b[36mz5ar2vrarj-algo-1-zx6c3 |\u001b[0m Uninstalling scikit-learn-1.2.1:\n",
      "\u001b[36mz5ar2vrarj-algo-1-zx6c3 |\u001b[0m Successfully uninstalled scikit-learn-1.2.1\n",
      "\u001b[36mz5ar2vrarj-algo-1-zx6c3 |\u001b[0m Successfully installed Mako-1.2.4 PrettyTable-3.6.0 absl-py-1.4.0 aiohttp-3.8.4 aiosignal-1.3.1 alembic-1.10.1 async-timeout-4.0.2 autopage-0.5.1 cachetools-5.3.0 cliff-4.2.0 cmaes-0.9.1 cmd2-2.4.3 colorlog-6.7.0 frozenlist-1.3.3 google-auth-2.16.2 google-auth-oauthlib-0.4.6 grpcio-1.51.3 markdown-3.4.1 multidict-6.0.4 oauthlib-3.2.2 optuna-2.10.1 patsy-0.5.3 pbr-5.11.1 pyDeprecate-0.3.2 pyasn1-modules-0.2.8 pyperclip-1.8.2 pytorch-forecasting-0.10.3 pytorch-lightning-1.6.3 requests-oauthlib-1.3.1 scikit-learn-1.1.3 sqlalchemy-2.0.5.post1 statsmodels-0.13.5 stevedore-5.0.0 tensorboard-2.12.0 tensorboard-data-server-0.7.0 tensorboard-plugin-wit-1.8.1 torchmetrics-0.11.3 yarl-1.8.2\n",
      "\u001b[36mz5ar2vrarj-algo-1-zx6c3 |\u001b[0m WARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\n",
      "\u001b[36mz5ar2vrarj-algo-1-zx6c3 |\u001b[0m [notice] A new release of pip is available: 23.0 -> 23.0.1\n",
      "\u001b[36mz5ar2vrarj-algo-1-zx6c3 |\u001b[0m [notice] To update, run: pip install --upgrade pip\n",
      "\u001b[36mz5ar2vrarj-algo-1-zx6c3 |\u001b[0m 2023-03-07 06:28:04,883 sagemaker-training-toolkit INFO     Waiting for the process to finish and give a return code.\n",
      "\u001b[36mz5ar2vrarj-algo-1-zx6c3 |\u001b[0m 2023-03-07 06:28:04,883 sagemaker-training-toolkit INFO     Done waiting for a return code. Received 0 from exiting process.\n",
      "\u001b[36mz5ar2vrarj-algo-1-zx6c3 |\u001b[0m 2023-03-07 06:28:04,953 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\n",
      "\u001b[36mz5ar2vrarj-algo-1-zx6c3 |\u001b[0m 2023-03-07 06:28:04,964 sagemaker-training-toolkit INFO     instance_groups entry not present in resource_config\n",
      "\u001b[36mz5ar2vrarj-algo-1-zx6c3 |\u001b[0m 2023-03-07 06:28:05,037 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\n",
      "\u001b[36mz5ar2vrarj-algo-1-zx6c3 |\u001b[0m 2023-03-07 06:28:05,048 sagemaker-training-toolkit INFO     instance_groups entry not present in resource_config\n",
      "\u001b[36mz5ar2vrarj-algo-1-zx6c3 |\u001b[0m 2023-03-07 06:28:05,117 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\n",
      "\u001b[36mz5ar2vrarj-algo-1-zx6c3 |\u001b[0m 2023-03-07 06:28:05,127 sagemaker-training-toolkit INFO     instance_groups entry not present in resource_config\n",
      "\u001b[36mz5ar2vrarj-algo-1-zx6c3 |\u001b[0m 2023-03-07 06:28:05,131 sagemaker-training-toolkit INFO     Invoking user script\n",
      "\u001b[36mz5ar2vrarj-algo-1-zx6c3 |\u001b[0m \n",
      "\u001b[36mz5ar2vrarj-algo-1-zx6c3 |\u001b[0m Training Env:\n",
      "\u001b[36mz5ar2vrarj-algo-1-zx6c3 |\u001b[0m \n",
      "\u001b[36mz5ar2vrarj-algo-1-zx6c3 |\u001b[0m {\n",
      "\u001b[36mz5ar2vrarj-algo-1-zx6c3 |\u001b[0m     \"additional_framework_parameters\": {},\n",
      "\u001b[36mz5ar2vrarj-algo-1-zx6c3 |\u001b[0m     \"channel_input_dirs\": {},\n",
      "\u001b[36mz5ar2vrarj-algo-1-zx6c3 |\u001b[0m     \"current_host\": \"algo-1-zx6c3\",\n",
      "\u001b[36mz5ar2vrarj-algo-1-zx6c3 |\u001b[0m     \"current_instance_group\": \"homogeneousCluster\",\n",
      "\u001b[36mz5ar2vrarj-algo-1-zx6c3 |\u001b[0m     \"current_instance_group_hosts\": [],\n",
      "\u001b[36mz5ar2vrarj-algo-1-zx6c3 |\u001b[0m     \"current_instance_type\": \"local\",\n",
      "\u001b[36mz5ar2vrarj-algo-1-zx6c3 |\u001b[0m     \"distribution_hosts\": [\n",
      "\u001b[36mz5ar2vrarj-algo-1-zx6c3 |\u001b[0m         \"algo-1-zx6c3\"\n",
      "\u001b[36mz5ar2vrarj-algo-1-zx6c3 |\u001b[0m     ],\n",
      "\u001b[36mz5ar2vrarj-algo-1-zx6c3 |\u001b[0m     \"distribution_instance_groups\": [],\n",
      "\u001b[36mz5ar2vrarj-algo-1-zx6c3 |\u001b[0m     \"framework_module\": \"sagemaker_pytorch_container.training:main\",\n",
      "\u001b[36mz5ar2vrarj-algo-1-zx6c3 |\u001b[0m     \"hosts\": [\n",
      "\u001b[36mz5ar2vrarj-algo-1-zx6c3 |\u001b[0m         \"algo-1-zx6c3\"\n",
      "\u001b[36mz5ar2vrarj-algo-1-zx6c3 |\u001b[0m     ],\n",
      "\u001b[36mz5ar2vrarj-algo-1-zx6c3 |\u001b[0m     \"hyperparameters\": {\n",
      "\u001b[36mz5ar2vrarj-algo-1-zx6c3 |\u001b[0m         \"epochs\": 2,\n",
      "\u001b[36mz5ar2vrarj-algo-1-zx6c3 |\u001b[0m         \"n_gpus\": 8\n",
      "\u001b[36mz5ar2vrarj-algo-1-zx6c3 |\u001b[0m     },\n",
      "\u001b[36mz5ar2vrarj-algo-1-zx6c3 |\u001b[0m     \"input_config_dir\": \"/opt/ml/input/config\",\n",
      "\u001b[36mz5ar2vrarj-algo-1-zx6c3 |\u001b[0m     \"input_data_config\": {},\n",
      "\u001b[36mz5ar2vrarj-algo-1-zx6c3 |\u001b[0m     \"input_dir\": \"/opt/ml/input\",\n",
      "\u001b[36mz5ar2vrarj-algo-1-zx6c3 |\u001b[0m     \"instance_groups\": [],\n",
      "\u001b[36mz5ar2vrarj-algo-1-zx6c3 |\u001b[0m     \"instance_groups_dict\": {},\n",
      "\u001b[36mz5ar2vrarj-algo-1-zx6c3 |\u001b[0m     \"is_hetero\": false,\n",
      "\u001b[36mz5ar2vrarj-algo-1-zx6c3 |\u001b[0m     \"is_master\": true,\n",
      "\u001b[36mz5ar2vrarj-algo-1-zx6c3 |\u001b[0m     \"is_modelparallel_enabled\": null,\n",
      "\u001b[36mz5ar2vrarj-algo-1-zx6c3 |\u001b[0m     \"is_smddpmprun_installed\": true,\n",
      "\u001b[36mz5ar2vrarj-algo-1-zx6c3 |\u001b[0m     \"job_name\": \"pytorch-training-2023-03-07-06-27-37-899\",\n",
      "\u001b[36mz5ar2vrarj-algo-1-zx6c3 |\u001b[0m     \"log_level\": 20,\n",
      "\u001b[36mz5ar2vrarj-algo-1-zx6c3 |\u001b[0m     \"master_hostname\": \"algo-1-zx6c3\",\n",
      "\u001b[36mz5ar2vrarj-algo-1-zx6c3 |\u001b[0m     \"model_dir\": \"/opt/ml/model\",\n",
      "\u001b[36mz5ar2vrarj-algo-1-zx6c3 |\u001b[0m     \"module_dir\": \"s3://sagemaker-us-east-1-057716757052/pytorch-training-2023-03-07-06-27-37-899/source/sourcedir.tar.gz\",\n",
      "\u001b[36mz5ar2vrarj-algo-1-zx6c3 |\u001b[0m     \"module_name\": \"TFT_Train\",\n",
      "\u001b[36mz5ar2vrarj-algo-1-zx6c3 |\u001b[0m     \"network_interface_name\": \"eth0\",\n",
      "\u001b[36mz5ar2vrarj-algo-1-zx6c3 |\u001b[0m     \"num_cpus\": 64,\n",
      "\u001b[36mz5ar2vrarj-algo-1-zx6c3 |\u001b[0m     \"num_gpus\": 8,\n",
      "\u001b[36mz5ar2vrarj-algo-1-zx6c3 |\u001b[0m     \"num_neurons\": 0,\n",
      "\u001b[36mz5ar2vrarj-algo-1-zx6c3 |\u001b[0m     \"output_data_dir\": \"/opt/ml/output/data\",\n",
      "\u001b[36mz5ar2vrarj-algo-1-zx6c3 |\u001b[0m     \"output_dir\": \"/opt/ml/output\",\n",
      "\u001b[36mz5ar2vrarj-algo-1-zx6c3 |\u001b[0m     \"output_intermediate_dir\": \"/opt/ml/output/intermediate\",\n",
      "\u001b[36mz5ar2vrarj-algo-1-zx6c3 |\u001b[0m     \"resource_config\": {\n",
      "\u001b[36mz5ar2vrarj-algo-1-zx6c3 |\u001b[0m         \"current_host\": \"algo-1-zx6c3\",\n",
      "\u001b[36mz5ar2vrarj-algo-1-zx6c3 |\u001b[0m         \"hosts\": [\n",
      "\u001b[36mz5ar2vrarj-algo-1-zx6c3 |\u001b[0m             \"algo-1-zx6c3\"\n",
      "\u001b[36mz5ar2vrarj-algo-1-zx6c3 |\u001b[0m         ]\n",
      "\u001b[36mz5ar2vrarj-algo-1-zx6c3 |\u001b[0m     },\n",
      "\u001b[36mz5ar2vrarj-algo-1-zx6c3 |\u001b[0m     \"user_entry_point\": \"TFT_Train.py\"\n",
      "\u001b[36mz5ar2vrarj-algo-1-zx6c3 |\u001b[0m }\n",
      "\u001b[36mz5ar2vrarj-algo-1-zx6c3 |\u001b[0m \n",
      "\u001b[36mz5ar2vrarj-algo-1-zx6c3 |\u001b[0m Environment variables:\n",
      "\u001b[36mz5ar2vrarj-algo-1-zx6c3 |\u001b[0m \n",
      "\u001b[36mz5ar2vrarj-algo-1-zx6c3 |\u001b[0m SM_HOSTS=[\"algo-1-zx6c3\"]\n",
      "\u001b[36mz5ar2vrarj-algo-1-zx6c3 |\u001b[0m SM_NETWORK_INTERFACE_NAME=eth0\n",
      "\u001b[36mz5ar2vrarj-algo-1-zx6c3 |\u001b[0m SM_HPS={\"epochs\":2,\"n_gpus\":8}\n",
      "\u001b[36mz5ar2vrarj-algo-1-zx6c3 |\u001b[0m SM_USER_ENTRY_POINT=TFT_Train.py\n",
      "\u001b[36mz5ar2vrarj-algo-1-zx6c3 |\u001b[0m SM_FRAMEWORK_PARAMS={}\n",
      "\u001b[36mz5ar2vrarj-algo-1-zx6c3 |\u001b[0m SM_RESOURCE_CONFIG={\"current_host\":\"algo-1-zx6c3\",\"hosts\":[\"algo-1-zx6c3\"]}\n",
      "\u001b[36mz5ar2vrarj-algo-1-zx6c3 |\u001b[0m SM_INPUT_DATA_CONFIG={}\n",
      "\u001b[36mz5ar2vrarj-algo-1-zx6c3 |\u001b[0m SM_OUTPUT_DATA_DIR=/opt/ml/output/data\n",
      "\u001b[36mz5ar2vrarj-algo-1-zx6c3 |\u001b[0m SM_CHANNELS=[]\n",
      "\u001b[36mz5ar2vrarj-algo-1-zx6c3 |\u001b[0m SM_CURRENT_HOST=algo-1-zx6c3\n",
      "\u001b[36mz5ar2vrarj-algo-1-zx6c3 |\u001b[0m SM_CURRENT_INSTANCE_TYPE=local\n",
      "\u001b[36mz5ar2vrarj-algo-1-zx6c3 |\u001b[0m SM_CURRENT_INSTANCE_GROUP=homogeneousCluster\n",
      "\u001b[36mz5ar2vrarj-algo-1-zx6c3 |\u001b[0m SM_CURRENT_INSTANCE_GROUP_HOSTS=[]\n",
      "\u001b[36mz5ar2vrarj-algo-1-zx6c3 |\u001b[0m SM_INSTANCE_GROUPS=[]\n",
      "\u001b[36mz5ar2vrarj-algo-1-zx6c3 |\u001b[0m SM_INSTANCE_GROUPS_DICT={}\n",
      "\u001b[36mz5ar2vrarj-algo-1-zx6c3 |\u001b[0m SM_DISTRIBUTION_INSTANCE_GROUPS=[]\n",
      "\u001b[36mz5ar2vrarj-algo-1-zx6c3 |\u001b[0m SM_IS_HETERO=false\n",
      "\u001b[36mz5ar2vrarj-algo-1-zx6c3 |\u001b[0m SM_MODULE_NAME=TFT_Train\n",
      "\u001b[36mz5ar2vrarj-algo-1-zx6c3 |\u001b[0m SM_LOG_LEVEL=20\n",
      "\u001b[36mz5ar2vrarj-algo-1-zx6c3 |\u001b[0m SM_FRAMEWORK_MODULE=sagemaker_pytorch_container.training:main\n",
      "\u001b[36mz5ar2vrarj-algo-1-zx6c3 |\u001b[0m SM_INPUT_DIR=/opt/ml/input\n",
      "\u001b[36mz5ar2vrarj-algo-1-zx6c3 |\u001b[0m SM_INPUT_CONFIG_DIR=/opt/ml/input/config\n",
      "\u001b[36mz5ar2vrarj-algo-1-zx6c3 |\u001b[0m SM_OUTPUT_DIR=/opt/ml/output\n",
      "\u001b[36mz5ar2vrarj-algo-1-zx6c3 |\u001b[0m SM_NUM_CPUS=64\n",
      "\u001b[36mz5ar2vrarj-algo-1-zx6c3 |\u001b[0m SM_NUM_GPUS=8\n",
      "\u001b[36mz5ar2vrarj-algo-1-zx6c3 |\u001b[0m SM_NUM_NEURONS=0\n",
      "\u001b[36mz5ar2vrarj-algo-1-zx6c3 |\u001b[0m SM_MODEL_DIR=/opt/ml/model\n",
      "\u001b[36mz5ar2vrarj-algo-1-zx6c3 |\u001b[0m SM_MODULE_DIR=s3://sagemaker-us-east-1-057716757052/pytorch-training-2023-03-07-06-27-37-899/source/sourcedir.tar.gz\n",
      "\u001b[36mz5ar2vrarj-algo-1-zx6c3 |\u001b[0m SM_TRAINING_ENV={\"additional_framework_parameters\":{},\"channel_input_dirs\":{},\"current_host\":\"algo-1-zx6c3\",\"current_instance_group\":\"homogeneousCluster\",\"current_instance_group_hosts\":[],\"current_instance_type\":\"local\",\"distribution_hosts\":[\"algo-1-zx6c3\"],\"distribution_instance_groups\":[],\"framework_module\":\"sagemaker_pytorch_container.training:main\",\"hosts\":[\"algo-1-zx6c3\"],\"hyperparameters\":{\"epochs\":2,\"n_gpus\":8},\"input_config_dir\":\"/opt/ml/input/config\",\"input_data_config\":{},\"input_dir\":\"/opt/ml/input\",\"instance_groups\":[],\"instance_groups_dict\":{},\"is_hetero\":false,\"is_master\":true,\"is_modelparallel_enabled\":null,\"is_smddpmprun_installed\":true,\"job_name\":\"pytorch-training-2023-03-07-06-27-37-899\",\"log_level\":20,\"master_hostname\":\"algo-1-zx6c3\",\"model_dir\":\"/opt/ml/model\",\"module_dir\":\"s3://sagemaker-us-east-1-057716757052/pytorch-training-2023-03-07-06-27-37-899/source/sourcedir.tar.gz\",\"module_name\":\"TFT_Train\",\"network_interface_name\":\"eth0\",\"num_cpus\":64,\"num_gpus\":8,\"num_neurons\":0,\"output_data_dir\":\"/opt/ml/output/data\",\"output_dir\":\"/opt/ml/output\",\"output_intermediate_dir\":\"/opt/ml/output/intermediate\",\"resource_config\":{\"current_host\":\"algo-1-zx6c3\",\"hosts\":[\"algo-1-zx6c3\"]},\"user_entry_point\":\"TFT_Train.py\"}\n",
      "\u001b[36mz5ar2vrarj-algo-1-zx6c3 |\u001b[0m SM_USER_ARGS=[\"--epochs\",\"2\",\"--n_gpus\",\"8\"]\n",
      "\u001b[36mz5ar2vrarj-algo-1-zx6c3 |\u001b[0m SM_OUTPUT_INTERMEDIATE_DIR=/opt/ml/output/intermediate\n",
      "\u001b[36mz5ar2vrarj-algo-1-zx6c3 |\u001b[0m SM_HP_EPOCHS=2\n",
      "\u001b[36mz5ar2vrarj-algo-1-zx6c3 |\u001b[0m SM_HP_N_GPUS=8\n",
      "\u001b[36mz5ar2vrarj-algo-1-zx6c3 |\u001b[0m PYTHONPATH=/opt/ml/code:/opt/conda/bin:/opt/conda/lib/python38.zip:/opt/conda/lib/python3.8:/opt/conda/lib/python3.8/lib-dynload:/opt/conda/lib/python3.8/site-packages:/opt/conda/lib/python3.8/site-packages/smdebug-1.0.24b20230214-py3.8.egg:/opt/conda/lib/python3.8/site-packages/pyinstrument-3.4.2-py3.8.egg:/opt/conda/lib/python3.8/site-packages/pyinstrument_cext-0.2.4-py3.8-linux-x86_64.egg:/opt/conda/lib/python3.8/site-packages/flash_attn-0.1-py3.8-linux-x86_64.egg:/opt/conda/lib/python3.8/site-packages/einops-0.6.0-py3.8.egg\n",
      "\u001b[36mz5ar2vrarj-algo-1-zx6c3 |\u001b[0m \n",
      "\u001b[36mz5ar2vrarj-algo-1-zx6c3 |\u001b[0m Invoking script with the following command:\n",
      "\u001b[36mz5ar2vrarj-algo-1-zx6c3 |\u001b[0m \n",
      "\u001b[36mz5ar2vrarj-algo-1-zx6c3 |\u001b[0m /opt/conda/bin/python3.8 TFT_Train.py --epochs 2 --n_gpus 8\n",
      "\u001b[36mz5ar2vrarj-algo-1-zx6c3 |\u001b[0m \n",
      "\u001b[36mz5ar2vrarj-algo-1-zx6c3 |\u001b[0m \n",
      "\u001b[36mz5ar2vrarj-algo-1-zx6c3 |\u001b[0m 2023-03-07 06:28:09,608 sagemaker-training-toolkit INFO     Exceptions not imported for SageMaker TF as Tensorflow is not installed.\n",
      "\u001b[36mz5ar2vrarj-algo-1-zx6c3 |\u001b[0m Not running on notebook\n",
      "\u001b[36mz5ar2vrarj-algo-1-zx6c3 |\u001b[0m ***** Arguments *****\n",
      "\u001b[36mz5ar2vrarj-algo-1-zx6c3 |\u001b[0m epochs=2\n",
      "\u001b[36mz5ar2vrarj-algo-1-zx6c3 |\u001b[0m seed=100\n",
      "\u001b[36mz5ar2vrarj-algo-1-zx6c3 |\u001b[0m train_batch_size=64\n",
      "\u001b[36mz5ar2vrarj-algo-1-zx6c3 |\u001b[0m model_dir=/opt/ml/model\n",
      "\u001b[36mz5ar2vrarj-algo-1-zx6c3 |\u001b[0m n_gpus=8\n",
      "\u001b[36mz5ar2vrarj-algo-1-zx6c3 |\u001b[0m GPU available: True, used: True\n",
      "\u001b[36mz5ar2vrarj-algo-1-zx6c3 |\u001b[0m TPU available: False, using: 0 TPU cores\n",
      "\u001b[36mz5ar2vrarj-algo-1-zx6c3 |\u001b[0m IPU available: False, using: 0 IPUs\n",
      "\u001b[36mz5ar2vrarj-algo-1-zx6c3 |\u001b[0m HPU available: False, using: 0 HPUs\n",
      "\u001b[36mz5ar2vrarj-algo-1-zx6c3 |\u001b[0m Number of parameters in network: 29.7k\n",
      "\u001b[36mz5ar2vrarj-algo-1-zx6c3 |\u001b[0m Initializing distributed: GLOBAL_RANK: 0, MEMBER: 1/8\n",
      "\u001b[36mz5ar2vrarj-algo-1-zx6c3 |\u001b[0m Initializing distributed: GLOBAL_RANK: 1, MEMBER: 2/8\n",
      "\u001b[36mz5ar2vrarj-algo-1-zx6c3 |\u001b[0m Initializing distributed: GLOBAL_RANK: 2, MEMBER: 3/8\n",
      "\u001b[36mz5ar2vrarj-algo-1-zx6c3 |\u001b[0m Initializing distributed: GLOBAL_RANK: 3, MEMBER: 4/8\n",
      "\u001b[36mz5ar2vrarj-algo-1-zx6c3 |\u001b[0m Initializing distributed: GLOBAL_RANK: 4, MEMBER: 5/8\n",
      "\u001b[36mz5ar2vrarj-algo-1-zx6c3 |\u001b[0m Initializing distributed: GLOBAL_RANK: 5, MEMBER: 6/8\n",
      "\u001b[36mz5ar2vrarj-algo-1-zx6c3 |\u001b[0m Initializing distributed: GLOBAL_RANK: 6, MEMBER: 7/8\n",
      "\u001b[36mz5ar2vrarj-algo-1-zx6c3 |\u001b[0m Initializing distributed: GLOBAL_RANK: 7, MEMBER: 8/8\n",
      "\u001b[36mz5ar2vrarj-algo-1-zx6c3 |\u001b[0m ----------------------------------------------------------------------------------------------------\n",
      "\u001b[36mz5ar2vrarj-algo-1-zx6c3 |\u001b[0m distributed_backend=nccl\n",
      "\u001b[36mz5ar2vrarj-algo-1-zx6c3 |\u001b[0m All distributed processes registered. Starting with 8 processes\n",
      "\u001b[36mz5ar2vrarj-algo-1-zx6c3 |\u001b[0m ----------------------------------------------------------------------------------------------------\n",
      "\u001b[36mz5ar2vrarj-algo-1-zx6c3 |\u001b[0m Missing logger folder: lightning_logs/lightning_logs\n",
      "\u001b[36mz5ar2vrarj-algo-1-zx6c3 |\u001b[0m Missing logger folder: lightning_logs/lightning_logs\n",
      "\u001b[36mz5ar2vrarj-algo-1-zx6c3 |\u001b[0m Missing logger folder: lightning_logs/lightning_logs\n",
      "\u001b[36mz5ar2vrarj-algo-1-zx6c3 |\u001b[0m Missing logger folder: lightning_logs/lightning_logs\n",
      "\u001b[36mz5ar2vrarj-algo-1-zx6c3 |\u001b[0m Missing logger folder: lightning_logs/lightning_logs\n",
      "\u001b[36mz5ar2vrarj-algo-1-zx6c3 |\u001b[0m Missing logger folder: lightning_logs/lightning_logs\n",
      "\u001b[36mz5ar2vrarj-algo-1-zx6c3 |\u001b[0m Missing logger folder: lightning_logs/lightning_logs\n",
      "\u001b[36mz5ar2vrarj-algo-1-zx6c3 |\u001b[0m Missing logger folder: lightning_logs/lightning_logs\n",
      "\u001b[36mz5ar2vrarj-algo-1-zx6c3 |\u001b[0m algo-1-zx6c3:363:363 [0] ofi_init:1288 NCCL WARN NET/OFI Only EFA provider is supported\n",
      "\u001b[36mz5ar2vrarj-algo-1-zx6c3 |\u001b[0m \n",
      "\u001b[36mz5ar2vrarj-algo-1-zx6c3 |\u001b[0m algo-1-zx6c3:363:363 [0] ofi_init:1339 NCCL WARN NET/OFI aws-ofi-nccl initialization failed\n",
      "\u001b[36mz5ar2vrarj-algo-1-zx6c3 |\u001b[0m NCCL version 2.10.3+cuda11.3\n",
      "\u001b[36mz5ar2vrarj-algo-1-zx6c3 |\u001b[0m algo-1-zx6c3:436:436 [1] ofi_init:1288 NCCL WARN NET/OFI Only EFA provider is supported\n",
      "\u001b[36mz5ar2vrarj-algo-1-zx6c3 |\u001b[0m \n",
      "\u001b[36mz5ar2vrarj-algo-1-zx6c3 |\u001b[0m algo-1-zx6c3:509:509 [2] ofi_init:1288 NCCL WARN NET/OFI Only EFA provider is supported\n",
      "\u001b[36mz5ar2vrarj-algo-1-zx6c3 |\u001b[0m \n",
      "\u001b[36mz5ar2vrarj-algo-1-zx6c3 |\u001b[0m algo-1-zx6c3:809:809 [6] ofi_init:1288 NCCL WARN NET/OFI Only EFA provider is supported\n",
      "\u001b[36mz5ar2vrarj-algo-1-zx6c3 |\u001b[0m \n",
      "\u001b[36mz5ar2vrarj-algo-1-zx6c3 |\u001b[0m algo-1-zx6c3:734:734 [5] ofi_init:1288 NCCL WARN NET/OFI Only EFA provider is supported\n",
      "\u001b[36mz5ar2vrarj-algo-1-zx6c3 |\u001b[0m \n",
      "\u001b[36mz5ar2vrarj-algo-1-zx6c3 |\u001b[0m algo-1-zx6c3:884:884 [7] ofi_init:1288 NCCL WARN NET/OFI Only EFA provider is supported\n",
      "\u001b[36mz5ar2vrarj-algo-1-zx6c3 |\u001b[0m \n",
      "\u001b[36mz5ar2vrarj-algo-1-zx6c3 |\u001b[0m algo-1-zx6c3:509:509 [2] ofi_init:1339 NCCL WARN NET/OFI aws-ofi-nccl initialization failed\n",
      "\u001b[36mz5ar2vrarj-algo-1-zx6c3 |\u001b[0m \n",
      "\u001b[36mz5ar2vrarj-algo-1-zx6c3 |\u001b[0m algo-1-zx6c3:809:809 [6] ofi_init:1339 NCCL WARN NET/OFI aws-ofi-nccl initialization failed\n",
      "\u001b[36mz5ar2vrarj-algo-1-zx6c3 |\u001b[0m \n",
      "\u001b[36mz5ar2vrarj-algo-1-zx6c3 |\u001b[0m algo-1-zx6c3:734:734 [5] ofi_init:1339 NCCL WARN NET/OFI aws-ofi-nccl initialization failed\n",
      "\u001b[36mz5ar2vrarj-algo-1-zx6c3 |\u001b[0m \n",
      "\u001b[36mz5ar2vrarj-algo-1-zx6c3 |\u001b[0m algo-1-zx6c3:436:436 [1] ofi_init:1339 NCCL WARN NET/OFI aws-ofi-nccl initialization failed\n",
      "\u001b[36mz5ar2vrarj-algo-1-zx6c3 |\u001b[0m \n",
      "\u001b[36mz5ar2vrarj-algo-1-zx6c3 |\u001b[0m algo-1-zx6c3:884:884 [7] ofi_init:1339 NCCL WARN NET/OFI aws-ofi-nccl initialization failed\n",
      "\u001b[36mz5ar2vrarj-algo-1-zx6c3 |\u001b[0m algo-1-zx6c3:584:584 [3] ofi_init:1288 NCCL WARN NET/OFI Only EFA provider is supported\n",
      "\u001b[36mz5ar2vrarj-algo-1-zx6c3 |\u001b[0m \n",
      "\u001b[36mz5ar2vrarj-algo-1-zx6c3 |\u001b[0m algo-1-zx6c3:584:584 [3] ofi_init:1339 NCCL WARN NET/OFI aws-ofi-nccl initialization failed\n",
      "\u001b[36mz5ar2vrarj-algo-1-zx6c3 |\u001b[0m algo-1-zx6c3:659:659 [4] ofi_init:1288 NCCL WARN NET/OFI Only EFA provider is supported\n",
      "\u001b[36mz5ar2vrarj-algo-1-zx6c3 |\u001b[0m \n",
      "\u001b[36mz5ar2vrarj-algo-1-zx6c3 |\u001b[0m algo-1-zx6c3:659:659 [4] ofi_init:1339 NCCL WARN NET/OFI aws-ofi-nccl initialization failed\n",
      "\u001b[36mz5ar2vrarj-algo-1-zx6c3 |\u001b[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]\n",
      "\u001b[36mz5ar2vrarj-algo-1-zx6c3 |\u001b[0m LOCAL_RANK: 6 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]\n",
      "\u001b[36mz5ar2vrarj-algo-1-zx6c3 |\u001b[0m LOCAL_RANK: 1 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]\n",
      "\u001b[36mz5ar2vrarj-algo-1-zx6c3 |\u001b[0m LOCAL_RANK: 5 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]\n",
      "\u001b[36mz5ar2vrarj-algo-1-zx6c3 |\u001b[0m LOCAL_RANK: 7 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]\n",
      "\u001b[36mz5ar2vrarj-algo-1-zx6c3 |\u001b[0m LOCAL_RANK: 2 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]\n",
      "\u001b[36mz5ar2vrarj-algo-1-zx6c3 |\u001b[0m LOCAL_RANK: 4 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]\n",
      "\u001b[36mz5ar2vrarj-algo-1-zx6c3 |\u001b[0m LOCAL_RANK: 3 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]\n",
      "\u001b[36mz5ar2vrarj-algo-1-zx6c3 |\u001b[0m | Name                               | Type                            | Params\n",
      "\u001b[36mz5ar2vrarj-algo-1-zx6c3 |\u001b[0m ----------------------------------------------------------------------------------------\n",
      "\u001b[36mz5ar2vrarj-algo-1-zx6c3 |\u001b[0m 0  | loss                               | QuantileLoss                    | 0     \n",
      "\u001b[36mz5ar2vrarj-algo-1-zx6c3 |\u001b[0m 1  | logging_metrics                    | ModuleList                      | 0     \n",
      "\u001b[36mz5ar2vrarj-algo-1-zx6c3 |\u001b[0m 2  | input_embeddings                   | MultiEmbedding                  | 1.3 K \n",
      "\u001b[36mz5ar2vrarj-algo-1-zx6c3 |\u001b[0m 3  | prescalers                         | ModuleDict                      | 256   \n",
      "\u001b[36mz5ar2vrarj-algo-1-zx6c3 |\u001b[0m 4  | static_variable_selection          | VariableSelectionNetwork        | 3.4 K \n",
      "\u001b[36mz5ar2vrarj-algo-1-zx6c3 |\u001b[0m 5  | encoder_variable_selection         | VariableSelectionNetwork        | 8.0 K \n",
      "\u001b[36mz5ar2vrarj-algo-1-zx6c3 |\u001b[0m 6  | decoder_variable_selection         | VariableSelectionNetwork        | 2.7 K \n",
      "\u001b[36mz5ar2vrarj-algo-1-zx6c3 |\u001b[0m 7  | static_context_variable_selection  | GatedResidualNetwork            | 1.1 K \n",
      "\u001b[36mz5ar2vrarj-algo-1-zx6c3 |\u001b[0m 8  | static_context_initial_hidden_lstm | GatedResidualNetwork            | 1.1 K \n",
      "\u001b[36mz5ar2vrarj-algo-1-zx6c3 |\u001b[0m 9  | static_context_initial_cell_lstm   | GatedResidualNetwork            | 1.1 K \n",
      "\u001b[36mz5ar2vrarj-algo-1-zx6c3 |\u001b[0m 10 | static_context_enrichment          | GatedResidualNetwork            | 1.1 K \n",
      "\u001b[36mz5ar2vrarj-algo-1-zx6c3 |\u001b[0m 11 | lstm_encoder                       | LSTM                            | 2.2 K \n",
      "\u001b[36mz5ar2vrarj-algo-1-zx6c3 |\u001b[0m 12 | lstm_decoder                       | LSTM                            | 2.2 K \n",
      "\u001b[36mz5ar2vrarj-algo-1-zx6c3 |\u001b[0m 13 | post_lstm_gate_encoder             | GatedLinearUnit                 | 544   \n",
      "\u001b[36mz5ar2vrarj-algo-1-zx6c3 |\u001b[0m 14 | post_lstm_add_norm_encoder         | AddNorm                         | 32    \n",
      "\u001b[36mz5ar2vrarj-algo-1-zx6c3 |\u001b[0m 15 | static_enrichment                  | GatedResidualNetwork            | 1.4 K \n",
      "\u001b[36mz5ar2vrarj-algo-1-zx6c3 |\u001b[0m 16 | multihead_attn                     | InterpretableMultiHeadAttention | 1.1 K \n",
      "\u001b[36mz5ar2vrarj-algo-1-zx6c3 |\u001b[0m 17 | post_attn_gate_norm                | GateAddNorm                     | 576   \n",
      "\u001b[36mz5ar2vrarj-algo-1-zx6c3 |\u001b[0m 18 | pos_wise_ff                        | GatedResidualNetwork            | 1.1 K \n",
      "\u001b[36mz5ar2vrarj-algo-1-zx6c3 |\u001b[0m 19 | pre_output_gate_norm               | GateAddNorm                     | 576   \n",
      "\u001b[36mz5ar2vrarj-algo-1-zx6c3 |\u001b[0m 20 | output_layer                       | Linear                          | 119   \n",
      "\u001b[36mz5ar2vrarj-algo-1-zx6c3 |\u001b[0m ----------------------------------------------------------------------------------------\n",
      "\u001b[36mz5ar2vrarj-algo-1-zx6c3 |\u001b[0m 29.7 K    Trainable params\n",
      "\u001b[36mz5ar2vrarj-algo-1-zx6c3 |\u001b[0m 0         Non-trainable params\n",
      "\u001b[36mz5ar2vrarj-algo-1-zx6c3 |\u001b[0m 29.7 K    Total params\n",
      "\u001b[36mz5ar2vrarj-algo-1-zx6c3 |\u001b[0m 0.119     Total estimated model params size (MB)\n",
      "\u001b[36mz5ar2vrarj-algo-1-zx6c3 |\u001b[0m Sanity Checking: 0it [00:00, ?it/s]\n",
      "\u001b[36mz5ar2vrarj-algo-1-zx6c3 |\u001b[0m [2023-03-07 06:29:46.046 algo-1-zx6c3:809 INFO utils.py:28] RULE_JOB_STOP_SIGNAL_FILENAME: None\n",
      "\u001b[36mz5ar2vrarj-algo-1-zx6c3 |\u001b[0m [2023-03-07 06:29:46.086 algo-1-zx6c3:884 INFO utils.py:28] RULE_JOB_STOP_SIGNAL_FILENAME: None\n",
      "\u001b[36mz5ar2vrarj-algo-1-zx6c3 |\u001b[0m [2023-03-07 06:29:46.090 algo-1-zx6c3:363 INFO utils.py:28] RULE_JOB_STOP_SIGNAL_FILENAME: None\n",
      "\u001b[36mz5ar2vrarj-algo-1-zx6c3 |\u001b[0m [2023-03-07 06:29:46.091 algo-1-zx6c3:436 INFO utils.py:28] RULE_JOB_STOP_SIGNAL_FILENAME: None\n",
      "\u001b[36mz5ar2vrarj-algo-1-zx6c3 |\u001b[0m [2023-03-07 06:29:46.091 algo-1-zx6c3:509 INFO utils.py:28] RULE_JOB_STOP_SIGNAL_FILENAME: None\n",
      "\u001b[36mz5ar2vrarj-algo-1-zx6c3 |\u001b[0m [2023-03-07 06:29:46.092 algo-1-zx6c3:734 INFO utils.py:28] RULE_JOB_STOP_SIGNAL_FILENAME: None\n",
      "\u001b[36mz5ar2vrarj-algo-1-zx6c3 |\u001b[0m [2023-03-07 06:29:46.098 algo-1-zx6c3:659 INFO utils.py:28] RULE_JOB_STOP_SIGNAL_FILENAME: None\n",
      "\u001b[36mz5ar2vrarj-algo-1-zx6c3 |\u001b[0m [2023-03-07 06:29:46.098 algo-1-zx6c3:584 INFO utils.py:28] RULE_JOB_STOP_SIGNAL_FILENAME: None\n",
      "\u001b[36mz5ar2vrarj-algo-1-zx6c3 |\u001b[0m [2023-03-07 06:29:46.206 algo-1-zx6c3:809 INFO profiler_config_parser.py:111] Unable to find config at /opt/ml/input/config/profilerconfig.json. Profiler is disabled.\n",
      "\u001b[36mz5ar2vrarj-algo-1-zx6c3 |\u001b[0m [2023-03-07 06:29:46.242 algo-1-zx6c3:884 INFO profiler_config_parser.py:111] Unable to find config at /opt/ml/input/config/profilerconfig.json. Profiler is disabled.\n",
      "\u001b[36mz5ar2vrarj-algo-1-zx6c3 |\u001b[0m [2023-03-07 06:29:46.249 algo-1-zx6c3:363 INFO profiler_config_parser.py:111] Unable to find config at /opt/ml/input/config/profilerconfig.json. Profiler is disabled.\n",
      "\u001b[36mz5ar2vrarj-algo-1-zx6c3 |\u001b[0m [2023-03-07 06:29:46.250 algo-1-zx6c3:436 INFO profiler_config_parser.py:111] Unable to find config at /opt/ml/input/config/profilerconfig.json. Profiler is disabled.\n",
      "\u001b[36mz5ar2vrarj-algo-1-zx6c3 |\u001b[0m [2023-03-07 06:29:46.251 algo-1-zx6c3:509 INFO profiler_config_parser.py:111] Unable to find config at /opt/ml/input/config/profilerconfig.json. Profiler is disabled.\n",
      "\u001b[36mz5ar2vrarj-algo-1-zx6c3 |\u001b[0m [2023-03-07 06:29:46.251 algo-1-zx6c3:734 INFO profiler_config_parser.py:111] Unable to find config at /opt/ml/input/config/profilerconfig.json. Profiler is disabled.\n",
      "\u001b[36mz5ar2vrarj-algo-1-zx6c3 |\u001b[0m [2023-03-07 06:29:46.261 algo-1-zx6c3:659 INFO profiler_config_parser.py:111] Unable to find config at /opt/ml/input/config/profilerconfig.json. Profiler is disabled.\n",
      "\u001b[36mz5ar2vrarj-algo-1-zx6c3 |\u001b[0m [2023-03-07 06:29:46.263 algo-1-zx6c3:584 INFO profiler_config_parser.py:111] Unable to find config at /opt/ml/input/config/profilerconfig.json. Profiler is disabled.\n",
      "\u001b[36mz5ar2vrarj-algo-1-zx6c3 |\u001b[0m Sanity Checking:   0%|          | 0/1 [00:00<?, ?it/s]\n",
      "\u001b[36mz5ar2vrarj-algo-1-zx6c3 |\u001b[0m Sanity Checking DataLoader 0:   0%|          | 0/1 [00:00<?, ?it/s]\n",
      "\u001b[36mz5ar2vrarj-algo-1-zx6c3 |\u001b[0m Sanity Checking DataLoader 0: 100%|██████████| 1/1 [00:00<00:00,  1.16it/s]\n",
      "\u001b[36mz5ar2vrarj-algo-1-zx6c3 |\u001b[0m Sanity Checking DataLoader 0: 100%|██████████| 1/1 [00:00<00:00,  1.16it/s]\n",
      "\u001b[36mz5ar2vrarj-algo-1-zx6c3 |\u001b[0m \n",
      "\u001b[36mz5ar2vrarj-algo-1-zx6c3 |\u001b[0m Training: 0it [00:00, ?it/s]\n",
      "\u001b[36mz5ar2vrarj-algo-1-zx6c3 |\u001b[0m Training:   0%|          | 0/21 [00:00<?, ?it/s]\n",
      "\u001b[36mz5ar2vrarj-algo-1-zx6c3 |\u001b[0m Epoch 0:   0%|          | 0/21 [00:00<?, ?it/s]\n",
      "\u001b[36mz5ar2vrarj-algo-1-zx6c3 |\u001b[0m [W reducer.cpp:1251] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())\n",
      "\u001b[36mz5ar2vrarj-algo-1-zx6c3 |\u001b[0m [W reducer.cpp:1251] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())\n",
      "\u001b[36mz5ar2vrarj-algo-1-zx6c3 |\u001b[0m [W reducer.cpp:1251] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())\n",
      "\u001b[36mz5ar2vrarj-algo-1-zx6c3 |\u001b[0m [W reducer.cpp:1251] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())\n",
      "\u001b[36mz5ar2vrarj-algo-1-zx6c3 |\u001b[0m [W reducer.cpp:1251] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())\n",
      "\u001b[36mz5ar2vrarj-algo-1-zx6c3 |\u001b[0m [W reducer.cpp:1251] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())\n",
      "\u001b[36mz5ar2vrarj-algo-1-zx6c3 |\u001b[0m [W reducer.cpp:1251] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())\n",
      "\u001b[36mz5ar2vrarj-algo-1-zx6c3 |\u001b[0m [W reducer.cpp:1251] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())\n",
      "\u001b[36mz5ar2vrarj-algo-1-zx6c3 |\u001b[0m Epoch 0:   5%|▍         | 1/21 [00:00<00:09,  2.01it/s]\n",
      "\u001b[36mz5ar2vrarj-algo-1-zx6c3 |\u001b[0m Epoch 0:   5%|▍         | 1/21 [00:00<00:09,  2.01it/s]\n",
      "\u001b[36mz5ar2vrarj-algo-1-zx6c3 |\u001b[0m Epoch 0:   5%|▍         | 1/21 [00:00<00:09,  2.00it/s, loss=399, v_num=0, train_loss_step=399.0]\n",
      "\u001b[36mz5ar2vrarj-algo-1-zx6c3 |\u001b[0m Epoch 0:  10%|▉         | 2/21 [00:00<00:07,  2.51it/s, loss=399, v_num=0, train_loss_step=399.0]\n",
      "\u001b[36mz5ar2vrarj-algo-1-zx6c3 |\u001b[0m Epoch 0:  10%|▉         | 2/21 [00:00<00:07,  2.51it/s, loss=399, v_num=0, train_loss_step=399.0]\n",
      "\u001b[36mz5ar2vrarj-algo-1-zx6c3 |\u001b[0m Epoch 0:  10%|▉         | 2/21 [00:00<00:07,  2.51it/s, loss=343, v_num=0, train_loss_step=286.0]\n",
      "\u001b[36mz5ar2vrarj-algo-1-zx6c3 |\u001b[0m Epoch 0:  14%|█▍        | 3/21 [00:01<00:07,  2.48it/s, loss=343, v_num=0, train_loss_step=286.0]\n",
      "\u001b[36mz5ar2vrarj-algo-1-zx6c3 |\u001b[0m Epoch 0:  14%|█▍        | 3/21 [00:01<00:07,  2.48it/s, loss=343, v_num=0, train_loss_step=286.0]\n",
      "\u001b[36mz5ar2vrarj-algo-1-zx6c3 |\u001b[0m Epoch 0:  14%|█▍        | 3/21 [00:01<00:07,  2.48it/s, loss=360, v_num=0, train_loss_step=396.0]\n",
      "\u001b[36mz5ar2vrarj-algo-1-zx6c3 |\u001b[0m Epoch 0:  19%|█▉        | 4/21 [00:01<00:06,  2.51it/s, loss=360, v_num=0, train_loss_step=396.0]\n",
      "\u001b[36mz5ar2vrarj-algo-1-zx6c3 |\u001b[0m Epoch 0:  19%|█▉        | 4/21 [00:01<00:06,  2.51it/s, loss=360, v_num=0, train_loss_step=396.0]\n",
      "\u001b[36mz5ar2vrarj-algo-1-zx6c3 |\u001b[0m Epoch 0:  19%|█▉        | 4/21 [00:01<00:06,  2.51it/s, loss=343, v_num=0, train_loss_step=290.0]\n",
      "\u001b[36mz5ar2vrarj-algo-1-zx6c3 |\u001b[0m Epoch 0:  24%|██▍       | 5/21 [00:01<00:06,  2.53it/s, loss=343, v_num=0, train_loss_step=290.0]\n",
      "\u001b[36mz5ar2vrarj-algo-1-zx6c3 |\u001b[0m Epoch 0:  24%|██▍       | 5/21 [00:01<00:06,  2.53it/s, loss=343, v_num=0, train_loss_step=290.0]\n",
      "\u001b[36mz5ar2vrarj-algo-1-zx6c3 |\u001b[0m Epoch 0:  24%|██▍       | 5/21 [00:01<00:06,  2.53it/s, loss=347, v_num=0, train_loss_step=361.0]\n",
      "\u001b[36mz5ar2vrarj-algo-1-zx6c3 |\u001b[0m Epoch 0:  29%|██▊       | 6/21 [00:02<00:05,  2.66it/s, loss=347, v_num=0, train_loss_step=361.0]\n",
      "\u001b[36mz5ar2vrarj-algo-1-zx6c3 |\u001b[0m Epoch 0:  29%|██▊       | 6/21 [00:02<00:05,  2.66it/s, loss=347, v_num=0, train_loss_step=361.0]\n",
      "\u001b[36mz5ar2vrarj-algo-1-zx6c3 |\u001b[0m Epoch 0:  29%|██▊       | 6/21 [00:02<00:05,  2.66it/s, loss=325, v_num=0, train_loss_step=218.0]\n",
      "\u001b[36mz5ar2vrarj-algo-1-zx6c3 |\u001b[0m Epoch 0:  33%|███▎      | 7/21 [00:02<00:05,  2.65it/s, loss=325, v_num=0, train_loss_step=218.0]\n",
      "\u001b[36mz5ar2vrarj-algo-1-zx6c3 |\u001b[0m Epoch 0:  33%|███▎      | 7/21 [00:02<00:05,  2.65it/s, loss=325, v_num=0, train_loss_step=218.0]\n",
      "\u001b[36mz5ar2vrarj-algo-1-zx6c3 |\u001b[0m Epoch 0:  33%|███▎      | 7/21 [00:02<00:05,  2.64it/s, loss=326, v_num=0, train_loss_step=327.0]\n",
      "\u001b[36mz5ar2vrarj-algo-1-zx6c3 |\u001b[0m Epoch 0:  38%|███▊      | 8/21 [00:02<00:04,  2.73it/s, loss=326, v_num=0, train_loss_step=327.0]\n",
      "\u001b[36mz5ar2vrarj-algo-1-zx6c3 |\u001b[0m Epoch 0:  38%|███▊      | 8/21 [00:02<00:04,  2.73it/s, loss=326, v_num=0, train_loss_step=327.0]\n",
      "\u001b[36mz5ar2vrarj-algo-1-zx6c3 |\u001b[0m Epoch 0:  38%|███▊      | 8/21 [00:02<00:04,  2.73it/s, loss=319, v_num=0, train_loss_step=273.0]\n",
      "\u001b[36mz5ar2vrarj-algo-1-zx6c3 |\u001b[0m Epoch 0:  43%|████▎     | 9/21 [00:03<00:04,  2.81it/s, loss=319, v_num=0, train_loss_step=273.0]\n",
      "\u001b[36mz5ar2vrarj-algo-1-zx6c3 |\u001b[0m Epoch 0:  43%|████▎     | 9/21 [00:03<00:04,  2.81it/s, loss=319, v_num=0, train_loss_step=273.0]\n",
      "\u001b[36mz5ar2vrarj-algo-1-zx6c3 |\u001b[0m Epoch 0:  43%|████▎     | 9/21 [00:03<00:04,  2.81it/s, loss=324, v_num=0, train_loss_step=364.0]\n",
      "\u001b[36mz5ar2vrarj-algo-1-zx6c3 |\u001b[0m Epoch 0:  48%|████▊     | 10/21 [00:03<00:03,  2.89it/s, loss=324, v_num=0, train_loss_step=364.0]\n",
      "\u001b[36mz5ar2vrarj-algo-1-zx6c3 |\u001b[0m Epoch 0:  48%|████▊     | 10/21 [00:03<00:03,  2.89it/s, loss=324, v_num=0, train_loss_step=364.0]\n",
      "\u001b[36mz5ar2vrarj-algo-1-zx6c3 |\u001b[0m Epoch 0:  48%|████▊     | 10/21 [00:03<00:03,  2.88it/s, loss=320, v_num=0, train_loss_step=284.0]\n",
      "\u001b[36mz5ar2vrarj-algo-1-zx6c3 |\u001b[0m Epoch 0:  52%|█████▏    | 11/21 [00:03<00:03,  2.94it/s, loss=320, v_num=0, train_loss_step=284.0]\n",
      "\u001b[36mz5ar2vrarj-algo-1-zx6c3 |\u001b[0m Epoch 0:  52%|█████▏    | 11/21 [00:03<00:03,  2.94it/s, loss=320, v_num=0, train_loss_step=284.0]\n",
      "\u001b[36mz5ar2vrarj-algo-1-zx6c3 |\u001b[0m Epoch 0:  52%|█████▏    | 11/21 [00:03<00:03,  2.94it/s, loss=313, v_num=0, train_loss_step=238.0]\n",
      "\u001b[36mz5ar2vrarj-algo-1-zx6c3 |\u001b[0m Epoch 0:  57%|█████▋    | 12/21 [00:04<00:03,  2.96it/s, loss=313, v_num=0, train_loss_step=238.0]\n",
      "\u001b[36mz5ar2vrarj-algo-1-zx6c3 |\u001b[0m Epoch 0:  57%|█████▋    | 12/21 [00:04<00:03,  2.96it/s, loss=313, v_num=0, train_loss_step=238.0]\n",
      "\u001b[36mz5ar2vrarj-algo-1-zx6c3 |\u001b[0m Epoch 0:  57%|█████▋    | 12/21 [00:04<00:03,  2.96it/s, loss=309, v_num=0, train_loss_step=268.0]\n",
      "\u001b[36mz5ar2vrarj-algo-1-zx6c3 |\u001b[0m Epoch 0:  62%|██████▏   | 13/21 [00:04<00:02,  2.93it/s, loss=309, v_num=0, train_loss_step=268.0]\n",
      "\u001b[36mz5ar2vrarj-algo-1-zx6c3 |\u001b[0m Epoch 0:  62%|██████▏   | 13/21 [00:04<00:02,  2.93it/s, loss=309, v_num=0, train_loss_step=268.0]\n",
      "\u001b[36mz5ar2vrarj-algo-1-zx6c3 |\u001b[0m Epoch 0:  62%|██████▏   | 13/21 [00:04<00:02,  2.93it/s, loss=307, v_num=0, train_loss_step=288.0]\n",
      "\u001b[36mz5ar2vrarj-algo-1-zx6c3 |\u001b[0m Epoch 0:  67%|██████▋   | 14/21 [00:04<00:02,  2.94it/s, loss=307, v_num=0, train_loss_step=288.0]\n",
      "\u001b[36mz5ar2vrarj-algo-1-zx6c3 |\u001b[0m Epoch 0:  67%|██████▋   | 14/21 [00:04<00:02,  2.94it/s, loss=307, v_num=0, train_loss_step=288.0]\n",
      "\u001b[36mz5ar2vrarj-algo-1-zx6c3 |\u001b[0m Epoch 0:  67%|██████▋   | 14/21 [00:04<00:02,  2.94it/s, loss=304, v_num=0, train_loss_step=262.0]\n",
      "\u001b[36mz5ar2vrarj-algo-1-zx6c3 |\u001b[0m Epoch 0:  71%|███████▏  | 15/21 [00:05<00:02,  2.98it/s, loss=304, v_num=0, train_loss_step=262.0]\n",
      "\u001b[36mz5ar2vrarj-algo-1-zx6c3 |\u001b[0m Epoch 0:  71%|███████▏  | 15/21 [00:05<00:02,  2.98it/s, loss=304, v_num=0, train_loss_step=262.0]\n",
      "\u001b[36mz5ar2vrarj-algo-1-zx6c3 |\u001b[0m Epoch 0:  71%|███████▏  | 15/21 [00:05<00:02,  2.98it/s, loss=300, v_num=0, train_loss_step=246.0]\n",
      "\u001b[36mz5ar2vrarj-algo-1-zx6c3 |\u001b[0m Epoch 0:  76%|███████▌  | 16/21 [00:05<00:01,  3.01it/s, loss=300, v_num=0, train_loss_step=246.0]\n",
      "\u001b[36mz5ar2vrarj-algo-1-zx6c3 |\u001b[0m Epoch 0:  76%|███████▌  | 16/21 [00:05<00:01,  3.01it/s, loss=300, v_num=0, train_loss_step=246.0]\n",
      "\u001b[36mz5ar2vrarj-algo-1-zx6c3 |\u001b[0m Epoch 0:  76%|███████▌  | 16/21 [00:05<00:01,  3.01it/s, loss=296, v_num=0, train_loss_step=238.0]\n",
      "\u001b[36mz5ar2vrarj-algo-1-zx6c3 |\u001b[0m Epoch 0:  81%|████████  | 17/21 [00:05<00:01,  3.02it/s, loss=296, v_num=0, train_loss_step=238.0]\n",
      "\u001b[36mz5ar2vrarj-algo-1-zx6c3 |\u001b[0m Epoch 0:  81%|████████  | 17/21 [00:05<00:01,  3.02it/s, loss=296, v_num=0, train_loss_step=238.0]\n",
      "\u001b[36mz5ar2vrarj-algo-1-zx6c3 |\u001b[0m Epoch 0:  81%|████████  | 17/21 [00:05<00:01,  3.02it/s, loss=292, v_num=0, train_loss_step=221.0]\n",
      "\u001b[36mz5ar2vrarj-algo-1-zx6c3 |\u001b[0m Epoch 0:  86%|████████▌ | 18/21 [00:05<00:00,  3.03it/s, loss=292, v_num=0, train_loss_step=221.0]\n",
      "\u001b[36mz5ar2vrarj-algo-1-zx6c3 |\u001b[0m Epoch 0:  86%|████████▌ | 18/21 [00:05<00:00,  3.03it/s, loss=292, v_num=0, train_loss_step=221.0]\n",
      "\u001b[36mz5ar2vrarj-algo-1-zx6c3 |\u001b[0m Epoch 0:  86%|████████▌ | 18/21 [00:05<00:00,  3.03it/s, loss=285, v_num=0, train_loss_step=167.0]\n",
      "\u001b[36mz5ar2vrarj-algo-1-zx6c3 |\u001b[0m Epoch 0:  90%|█████████ | 19/21 [00:06<00:00,  3.04it/s, loss=285, v_num=0, train_loss_step=167.0]\n",
      "\u001b[36mz5ar2vrarj-algo-1-zx6c3 |\u001b[0m Epoch 0:  90%|█████████ | 19/21 [00:06<00:00,  3.04it/s, loss=285, v_num=0, train_loss_step=167.0]\n",
      "\u001b[36mz5ar2vrarj-algo-1-zx6c3 |\u001b[0m Epoch 0:  90%|█████████ | 19/21 [00:06<00:00,  3.04it/s, loss=279, v_num=0, train_loss_step=167.0]\n",
      "\u001b[36mz5ar2vrarj-algo-1-zx6c3 |\u001b[0m Epoch 0:  95%|█████████▌| 20/21 [00:06<00:00,  3.04it/s, loss=279, v_num=0, train_loss_step=167.0]\n",
      "\u001b[36mz5ar2vrarj-algo-1-zx6c3 |\u001b[0m Epoch 0:  95%|█████████▌| 20/21 [00:06<00:00,  3.04it/s, loss=279, v_num=0, train_loss_step=167.0]\n",
      "\u001b[36mz5ar2vrarj-algo-1-zx6c3 |\u001b[0m Epoch 0:  95%|█████████▌| 20/21 [00:06<00:00,  3.04it/s, loss=276, v_num=0, train_loss_step=234.0]\n",
      "\u001b[36mz5ar2vrarj-algo-1-zx6c3 |\u001b[0m Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "\u001b[36mz5ar2vrarj-algo-1-zx6c3 |\u001b[0m \n",
      "\u001b[36mz5ar2vrarj-algo-1-zx6c3 |\u001b[0m Validation:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "\u001b[36mz5ar2vrarj-algo-1-zx6c3 |\u001b[0m Validation DataLoader 0:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "\u001b[36mz5ar2vrarj-algo-1-zx6c3 |\u001b[0m \n",
      "\u001b[36mz5ar2vrarj-algo-1-zx6c3 |\u001b[0m Validation DataLoader 0: 100%|██████████| 1/1 [00:00<00:00,  3.48it/s]\u001b[A\n",
      "\u001b[36mz5ar2vrarj-algo-1-zx6c3 |\u001b[0m Validation DataLoader 0: 100%|██████████| 1/1 [00:00<00:00,  3.48it/s]\u001b[A\n",
      "\u001b[36mz5ar2vrarj-algo-1-zx6c3 |\u001b[0m Epoch 0: 100%|██████████| 21/21 [00:06<00:00,  3.03it/s, loss=276, v_num=0, train_loss_step=234.0]\n",
      "\u001b[36mz5ar2vrarj-algo-1-zx6c3 |\u001b[0m Epoch 0: 100%|██████████| 21/21 [00:06<00:00,  3.03it/s, loss=276, v_num=0, train_loss_step=234.0]\n",
      "\u001b[36mz5ar2vrarj-algo-1-zx6c3 |\u001b[0m Epoch 0: 100%|██████████| 21/21 [00:07<00:00,  2.74it/s, loss=276, v_num=0, train_loss_step=234.0, val_loss=273.0]\n",
      "\u001b[36mz5ar2vrarj-algo-1-zx6c3 |\u001b[0m \u001b[A\n",
      "\u001b[36mz5ar2vrarj-algo-1-zx6c3 |\u001b[0m Epoch 0: 100%|██████████| 21/21 [00:07<00:00,  2.74it/s, loss=276, v_num=0, train_loss_step=234.0, val_loss=273.0, train_loss_epoch=284.0]\n",
      "Epoch 1:   0%|          | 0/21 [00:00<?, ?it/s, loss=276, v_num=0, train_loss_step=234.0, val_loss=273.0, train_loss_epoch=284.0]loss=273.0, train_loss_epoch=284.0]         \n",
      "\u001b[36mz5ar2vrarj-algo-1-zx6c3 |\u001b[0m Epoch 1:   5%|▍         | 1/21 [00:00<00:04,  4.00it/s, loss=276, v_num=0, train_loss_step=234.0, val_loss=273.0, train_loss_epoch=284.0]\n",
      "\u001b[36mz5ar2vrarj-algo-1-zx6c3 |\u001b[0m Epoch 1:   5%|▍         | 1/21 [00:00<00:05,  4.00it/s, loss=276, v_num=0, train_loss_step=234.0, val_loss=273.0, train_loss_epoch=284.0]\n",
      "\u001b[36mz5ar2vrarj-algo-1-zx6c3 |\u001b[0m Epoch 1:   5%|▍         | 1/21 [00:00<00:05,  3.99it/s, loss=265, v_num=0, train_loss_step=162.0, val_loss=273.0, train_loss_epoch=284.0]\n",
      "\u001b[36mz5ar2vrarj-algo-1-zx6c3 |\u001b[0m Epoch 1:  10%|▉         | 2/21 [00:00<00:04,  3.86it/s, loss=265, v_num=0, train_loss_step=162.0, val_loss=273.0, train_loss_epoch=284.0]\n",
      "\u001b[36mz5ar2vrarj-algo-1-zx6c3 |\u001b[0m Epoch 1:  10%|▉         | 2/21 [00:00<00:04,  3.86it/s, loss=265, v_num=0, train_loss_step=162.0, val_loss=273.0, train_loss_epoch=284.0]\n",
      "\u001b[36mz5ar2vrarj-algo-1-zx6c3 |\u001b[0m Epoch 1:  10%|▉         | 2/21 [00:00<00:04,  3.85it/s, loss=261, v_num=0, train_loss_step=219.0, val_loss=273.0, train_loss_epoch=284.0]\n",
      "\u001b[36mz5ar2vrarj-algo-1-zx6c3 |\u001b[0m Epoch 1:  14%|█▍        | 3/21 [00:00<00:05,  3.34it/s, loss=261, v_num=0, train_loss_step=219.0, val_loss=273.0, train_loss_epoch=284.0]\n",
      "\u001b[36mz5ar2vrarj-algo-1-zx6c3 |\u001b[0m Epoch 1:  14%|█▍        | 3/21 [00:00<00:05,  3.34it/s, loss=261, v_num=0, train_loss_step=219.0, val_loss=273.0, train_loss_epoch=284.0]\n",
      "\u001b[36mz5ar2vrarj-algo-1-zx6c3 |\u001b[0m Epoch 1:  14%|█▍        | 3/21 [00:00<00:05,  3.34it/s, loss=252, v_num=0, train_loss_step=220.0, val_loss=273.0, train_loss_epoch=284.0]\n",
      "Epoch 1:  19%|█▉        | 4/21 [00:01<00:05,  3.30it/s, loss=252, v_num=0, train_loss_step=220.0, val_loss=273.0, train_loss_epoch=284.0]loss=273.0, train_loss_epoch=284.0]\n",
      "\u001b[36mz5ar2vrarj-algo-1-zx6c3 |\u001b[0m Epoch 1:  19%|█▉        | 4/21 [00:01<00:05,  3.30it/s, loss=245, v_num=0, train_loss_step=137.0, val_loss=273.0, train_loss_epoch=284.0]\n",
      "\u001b[36mz5ar2vrarj-algo-1-zx6c3 |\u001b[0m Epoch 1:  24%|██▍       | 5/21 [00:01<00:04,  3.29it/s, loss=245, v_num=0, train_loss_step=137.0, val_loss=273.0, train_loss_epoch=284.0]\n",
      "\u001b[36mz5ar2vrarj-algo-1-zx6c3 |\u001b[0m Epoch 1:  24%|██▍       | 5/21 [00:01<00:04,  3.29it/s, loss=245, v_num=0, train_loss_step=137.0, val_loss=273.0, train_loss_epoch=284.0]\n",
      "\u001b[36mz5ar2vrarj-algo-1-zx6c3 |\u001b[0m Epoch 1:  24%|██▍       | 5/21 [00:01<00:04,  3.29it/s, loss=235, v_num=0, train_loss_step=176.0, val_loss=273.0, train_loss_epoch=284.0]\n",
      "\u001b[36mz5ar2vrarj-algo-1-zx6c3 |\u001b[0m Epoch 1:  29%|██▊       | 6/21 [00:01<00:04,  3.27it/s, loss=235, v_num=0, train_loss_step=176.0, val_loss=273.0, train_loss_epoch=284.0]\n",
      "\u001b[36mz5ar2vrarj-algo-1-zx6c3 |\u001b[0m Epoch 1:  29%|██▊       | 6/21 [00:01<00:04,  3.27it/s, loss=235, v_num=0, train_loss_step=176.0, val_loss=273.0, train_loss_epoch=284.0]\n",
      "\u001b[36mz5ar2vrarj-algo-1-zx6c3 |\u001b[0m Epoch 1:  29%|██▊       | 6/21 [00:01<00:04,  3.27it/s, loss=235, v_num=0, train_loss_step=208.0, val_loss=273.0, train_loss_epoch=284.0]\n",
      "\u001b[36mz5ar2vrarj-algo-1-zx6c3 |\u001b[0m Epoch 1:  33%|███▎      | 7/21 [00:02<00:04,  3.29it/s, loss=235, v_num=0, train_loss_step=208.0, val_loss=273.0, train_loss_epoch=284.0]\n",
      "\u001b[36mz5ar2vrarj-algo-1-zx6c3 |\u001b[0m Epoch 1:  33%|███▎      | 7/21 [00:02<00:04,  3.29it/s, loss=235, v_num=0, train_loss_step=208.0, val_loss=273.0, train_loss_epoch=284.0]\n",
      "\u001b[36mz5ar2vrarj-algo-1-zx6c3 |\u001b[0m Epoch 1:  33%|███▎      | 7/21 [00:02<00:04,  3.29it/s, loss=228, v_num=0, train_loss_step=188.0, val_loss=273.0, train_loss_epoch=284.0]\n",
      "\u001b[36mz5ar2vrarj-algo-1-zx6c3 |\u001b[0m Epoch 1:  38%|███▊      | 8/21 [00:02<00:04,  3.16it/s, loss=228, v_num=0, train_loss_step=188.0, val_loss=273.0, train_loss_epoch=284.0]\n",
      "\u001b[36mz5ar2vrarj-algo-1-zx6c3 |\u001b[0m Epoch 1:  38%|███▊      | 8/21 [00:02<00:04,  3.16it/s, loss=228, v_num=0, train_loss_step=188.0, val_loss=273.0, train_loss_epoch=284.0]\n",
      "\u001b[36mz5ar2vrarj-algo-1-zx6c3 |\u001b[0m Epoch 1:  38%|███▊      | 8/21 [00:02<00:04,  3.16it/s, loss=222, v_num=0, train_loss_step=156.0, val_loss=273.0, train_loss_epoch=284.0]\n",
      "\u001b[36mz5ar2vrarj-algo-1-zx6c3 |\u001b[0m Epoch 1:  43%|████▎     | 9/21 [00:02<00:03,  3.05it/s, loss=222, v_num=0, train_loss_step=156.0, val_loss=273.0, train_loss_epoch=284.0]\n",
      "\u001b[36mz5ar2vrarj-algo-1-zx6c3 |\u001b[0m Epoch 1:  43%|████▎     | 9/21 [00:02<00:03,  3.04it/s, loss=222, v_num=0, train_loss_step=156.0, val_loss=273.0, train_loss_epoch=284.0]\n",
      "\u001b[36mz5ar2vrarj-algo-1-zx6c3 |\u001b[0m Epoch 1:  43%|████▎     | 9/21 [00:02<00:03,  3.04it/s, loss=213, v_num=0, train_loss_step=184.0, val_loss=273.0, train_loss_epoch=284.0]\n",
      "\u001b[36mz5ar2vrarj-algo-1-zx6c3 |\u001b[0m Epoch 1:  48%|████▊     | 10/21 [00:03<00:03,  3.02it/s, loss=213, v_num=0, train_loss_step=184.0, val_loss=273.0, train_loss_epoch=284.0]\n",
      "\u001b[36mz5ar2vrarj-algo-1-zx6c3 |\u001b[0m Epoch 1:  48%|████▊     | 10/21 [00:03<00:03,  3.02it/s, loss=213, v_num=0, train_loss_step=184.0, val_loss=273.0, train_loss_epoch=284.0]\n",
      "\u001b[36mz5ar2vrarj-algo-1-zx6c3 |\u001b[0m Epoch 1:  48%|████▊     | 10/21 [00:03<00:03,  3.01it/s, loss=206, v_num=0, train_loss_step=135.0, val_loss=273.0, train_loss_epoch=284.0]\n",
      "Epoch 1:  52%|█████▏    | 11/21 [00:03<00:03,  3.06it/s, loss=206, v_num=0, train_loss_step=135.0, val_loss=273.0, train_loss_epoch=284.0]loss=273.0, train_loss_epoch=284.0]\n",
      "\u001b[36mz5ar2vrarj-algo-1-zx6c3 |\u001b[0m Epoch 1:  52%|█████▏    | 11/21 [00:03<00:03,  3.06it/s, loss=201, v_num=0, train_loss_step=154.0, val_loss=273.0, train_loss_epoch=284.0]\n",
      "\u001b[36mz5ar2vrarj-algo-1-zx6c3 |\u001b[0m Epoch 1:  57%|█████▋    | 12/21 [00:03<00:02,  3.04it/s, loss=201, v_num=0, train_loss_step=154.0, val_loss=273.0, train_loss_epoch=284.0]\n",
      "\u001b[36mz5ar2vrarj-algo-1-zx6c3 |\u001b[0m Epoch 1:  57%|█████▋    | 12/21 [00:03<00:02,  3.04it/s, loss=201, v_num=0, train_loss_step=154.0, val_loss=273.0, train_loss_epoch=284.0]\n",
      "\u001b[36mz5ar2vrarj-algo-1-zx6c3 |\u001b[0m Epoch 1:  57%|█████▋    | 12/21 [00:03<00:02,  3.04it/s, loss=196, v_num=0, train_loss_step=160.0, val_loss=273.0, train_loss_epoch=284.0]\n",
      "\u001b[36mz5ar2vrarj-algo-1-zx6c3 |\u001b[0m Epoch 1:  62%|██████▏   | 13/21 [00:04<00:02,  3.09it/s, loss=196, v_num=0, train_loss_step=160.0, val_loss=273.0, train_loss_epoch=284.0]\n",
      "\u001b[36mz5ar2vrarj-algo-1-zx6c3 |\u001b[0m Epoch 1:  62%|██████▏   | 13/21 [00:04<00:02,  3.09it/s, loss=196, v_num=0, train_loss_step=160.0, val_loss=273.0, train_loss_epoch=284.0]\n",
      "\u001b[36mz5ar2vrarj-algo-1-zx6c3 |\u001b[0m Epoch 1:  62%|██████▏   | 13/21 [00:04<00:02,  3.09it/s, loss=195, v_num=0, train_loss_step=263.0, val_loss=273.0, train_loss_epoch=284.0]\n",
      "\u001b[36mz5ar2vrarj-algo-1-zx6c3 |\u001b[0m Epoch 1:  67%|██████▋   | 14/21 [00:04<00:02,  3.07it/s, loss=195, v_num=0, train_loss_step=263.0, val_loss=273.0, train_loss_epoch=284.0]\n",
      "\u001b[36mz5ar2vrarj-algo-1-zx6c3 |\u001b[0m Epoch 1:  67%|██████▋   | 14/21 [00:04<00:02,  3.07it/s, loss=195, v_num=0, train_loss_step=263.0, val_loss=273.0, train_loss_epoch=284.0]\n",
      "\u001b[36mz5ar2vrarj-algo-1-zx6c3 |\u001b[0m Epoch 1:  67%|██████▋   | 14/21 [00:04<00:02,  3.07it/s, loss=189, v_num=0, train_loss_step=138.0, val_loss=273.0, train_loss_epoch=284.0]\n",
      "\u001b[36mz5ar2vrarj-algo-1-zx6c3 |\u001b[0m Epoch 1:  71%|███████▏  | 15/21 [00:04<00:01,  3.11it/s, loss=189, v_num=0, train_loss_step=138.0, val_loss=273.0, train_loss_epoch=284.0]\n",
      "\u001b[36mz5ar2vrarj-algo-1-zx6c3 |\u001b[0m Epoch 1:  71%|███████▏  | 15/21 [00:04<00:01,  3.11it/s, loss=189, v_num=0, train_loss_step=138.0, val_loss=273.0, train_loss_epoch=284.0]\n",
      "\u001b[36mz5ar2vrarj-algo-1-zx6c3 |\u001b[0m Epoch 1:  71%|███████▏  | 15/21 [00:04<00:01,  3.11it/s, loss=185, v_num=0, train_loss_step=182.0, val_loss=273.0, train_loss_epoch=284.0]\n",
      "\u001b[36mz5ar2vrarj-algo-1-zx6c3 |\u001b[0m Epoch 1:  76%|███████▌  | 16/21 [00:05<00:01,  3.12it/s, loss=185, v_num=0, train_loss_step=182.0, val_loss=273.0, train_loss_epoch=284.0]\n",
      "\u001b[36mz5ar2vrarj-algo-1-zx6c3 |\u001b[0m Epoch 1:  76%|███████▌  | 16/21 [00:05<00:01,  3.12it/s, loss=185, v_num=0, train_loss_step=182.0, val_loss=273.0, train_loss_epoch=284.0]\n",
      "\u001b[36mz5ar2vrarj-algo-1-zx6c3 |\u001b[0m Epoch 1:  76%|███████▌  | 16/21 [00:05<00:01,  3.12it/s, loss=180, v_num=0, train_loss_step=134.0, val_loss=273.0, train_loss_epoch=284.0]\n",
      "\u001b[36mz5ar2vrarj-algo-1-zx6c3 |\u001b[0m Epoch 1:  81%|████████  | 17/21 [00:05<00:01,  3.15it/s, loss=180, v_num=0, train_loss_step=134.0, val_loss=273.0, train_loss_epoch=284.0]\n",
      "\u001b[36mz5ar2vrarj-algo-1-zx6c3 |\u001b[0m Epoch 1:  81%|████████  | 17/21 [00:05<00:01,  3.15it/s, loss=180, v_num=0, train_loss_step=134.0, val_loss=273.0, train_loss_epoch=284.0]\n",
      "\u001b[36mz5ar2vrarj-algo-1-zx6c3 |\u001b[0m Epoch 1:  81%|████████  | 17/21 [00:05<00:01,  3.15it/s, loss=177, v_num=0, train_loss_step=158.0, val_loss=273.0, train_loss_epoch=284.0]\n",
      "\u001b[36mz5ar2vrarj-algo-1-zx6c3 |\u001b[0m Epoch 1:  86%|████████▌ | 18/21 [00:05<00:00,  3.17it/s, loss=177, v_num=0, train_loss_step=158.0, val_loss=273.0, train_loss_epoch=284.0]\n",
      "\u001b[36mz5ar2vrarj-algo-1-zx6c3 |\u001b[0m Epoch 1:  86%|████████▌ | 18/21 [00:05<00:00,  3.17it/s, loss=177, v_num=0, train_loss_step=158.0, val_loss=273.0, train_loss_epoch=284.0]\n",
      "\u001b[36mz5ar2vrarj-algo-1-zx6c3 |\u001b[0m Epoch 1:  86%|████████▌ | 18/21 [00:05<00:00,  3.17it/s, loss=178, v_num=0, train_loss_step=187.0, val_loss=273.0, train_loss_epoch=284.0]\n",
      "\u001b[36mz5ar2vrarj-algo-1-zx6c3 |\u001b[0m Epoch 1:  90%|█████████ | 19/21 [00:06<00:00,  3.16it/s, loss=178, v_num=0, train_loss_step=187.0, val_loss=273.0, train_loss_epoch=284.0]\n",
      "\u001b[36mz5ar2vrarj-algo-1-zx6c3 |\u001b[0m Epoch 1:  90%|█████████ | 19/21 [00:06<00:00,  3.16it/s, loss=178, v_num=0, train_loss_step=187.0, val_loss=273.0, train_loss_epoch=284.0]\n",
      "\u001b[36mz5ar2vrarj-algo-1-zx6c3 |\u001b[0m Epoch 1:  90%|█████████ | 19/21 [00:06<00:00,  3.16it/s, loss=177, v_num=0, train_loss_step=147.0, val_loss=273.0, train_loss_epoch=284.0]\n",
      "\u001b[36mz5ar2vrarj-algo-1-zx6c3 |\u001b[0m Epoch 1:  95%|█████████▌| 20/21 [00:06<00:00,  3.16it/s, loss=177, v_num=0, train_loss_step=147.0, val_loss=273.0, train_loss_epoch=284.0]\n",
      "\u001b[36mz5ar2vrarj-algo-1-zx6c3 |\u001b[0m Epoch 1:  95%|█████████▌| 20/21 [00:06<00:00,  3.16it/s, loss=177, v_num=0, train_loss_step=147.0, val_loss=273.0, train_loss_epoch=284.0]\n",
      "\u001b[36mz5ar2vrarj-algo-1-zx6c3 |\u001b[0m Epoch 1:  95%|█████████▌| 20/21 [00:06<00:00,  3.16it/s, loss=173, v_num=0, train_loss_step=152.0, val_loss=273.0, train_loss_epoch=284.0]\n",
      "\u001b[36mz5ar2vrarj-algo-1-zx6c3 |\u001b[0m \n",
      "\u001b[36mz5ar2vrarj-algo-1-zx6c3 |\u001b[0m Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "\u001b[36mz5ar2vrarj-algo-1-zx6c3 |\u001b[0m \n",
      "\u001b[36mz5ar2vrarj-algo-1-zx6c3 |\u001b[0m Validation:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "\u001b[36mz5ar2vrarj-algo-1-zx6c3 |\u001b[0m Validation DataLoader 0:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "\u001b[36mz5ar2vrarj-algo-1-zx6c3 |\u001b[0m \n",
      "\u001b[36mz5ar2vrarj-algo-1-zx6c3 |\u001b[0m Validation DataLoader 0: 100%|██████████| 1/1 [00:00<00:00,  3.06it/s]\u001b[A\n",
      "\u001b[36mz5ar2vrarj-algo-1-zx6c3 |\u001b[0m Validation DataLoader 0: 100%|██████████| 1/1 [00:00<00:00,  3.06it/s]\u001b[A\n",
      "\u001b[36mz5ar2vrarj-algo-1-zx6c3 |\u001b[0m Epoch 1: 100%|██████████| 21/21 [00:06<00:00,  3.13it/s, loss=173, v_num=0, train_loss_step=152.0, val_loss=273.0, train_loss_epoch=284.0]\n",
      "\u001b[36mz5ar2vrarj-algo-1-zx6c3 |\u001b[0m Epoch 1: 100%|██████████| 21/21 [00:06<00:00,  3.13it/s, loss=173, v_num=0, train_loss_step=152.0, val_loss=273.0, train_loss_epoch=284.0]\n",
      "\u001b[36mz5ar2vrarj-algo-1-zx6c3 |\u001b[0m Epoch 1: 100%|██████████| 21/21 [00:07<00:00,  2.81it/s, loss=173, v_num=0, train_loss_step=152.0, val_loss=213.0, train_loss_epoch=284.0]\n",
      "\u001b[36mz5ar2vrarj-algo-1-zx6c3 |\u001b[0m \u001b[A\n",
      "\u001b[36mz5ar2vrarj-algo-1-zx6c3 |\u001b[0m Epoch 1: 100%|██████████| 21/21 [00:07<00:00,  2.81it/s, loss=173, v_num=0, train_loss_step=152.0, val_loss=213.0, train_loss_epoch=164.0]\n",
      "\u001b[36mz5ar2vrarj-algo-1-zx6c3 |\u001b[0m Epoch 1: 100%|██████████| 21/21 [00:07<00:00,  2.75it/s, loss=173, v_num=0, train_loss_step=152.0, val_loss=213.0, train_loss_epoch=164.0]\n",
      "\u001b[36mz5ar2vrarj-algo-1-zx6c3 |\u001b[0m 2023-03-07 06:30:06,544 sagemaker-training-toolkit INFO     Waiting for the process to finish and give a return code.\n",
      "\u001b[36mz5ar2vrarj-algo-1-zx6c3 |\u001b[0m 2023-03-07 06:30:06,545 sagemaker-training-toolkit INFO     Done waiting for a return code. Received 0 from exiting process.\n",
      "\u001b[36mz5ar2vrarj-algo-1-zx6c3 |\u001b[0m 2023-03-07 06:30:06,545 sagemaker-training-toolkit INFO     Reporting training SUCCESS\n",
      "\u001b[36mz5ar2vrarj-algo-1-zx6c3 exited with code 0\n",
      "\u001b[0mAborting on container exit...\n",
      "===== Job Complete =====\n"
     ]
    }
   ],
   "source": [
    "from sagemaker.pytorch import PyTorch\n",
    "import os\n",
    "import subprocess\n",
    "\n",
    "\n",
    "local_estimator = PyTorch(\n",
    "    entry_point=\"TFT_Train.py\",    \n",
    "    source_dir='src',    \n",
    "    role=role,\n",
    "    framework_version='1.12.1',    \n",
    "    py_version='py38',        \n",
    "    instance_count=1,\n",
    "    instance_type=instance_type, # local_gpu or local 지정\n",
    "    session = sagemaker.LocalSession(), # 로컬 세션을 사용합니다.\n",
    "    hyperparameters= hyperparameters               \n",
    "    \n",
    ")\n",
    "local_estimator.fit()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d49513c-1e54-4fae-b1c6-6fe5ff5839a9",
   "metadata": {
    "tags": []
   },
   "source": [
    "# 3. SageMaker Cloud Mode\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5d1bb25-6912-4e62-a46a-e01682f20d8c",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 파라미터 셋업"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a6e47078-88f2-470a-adcc-82ae4c0305a9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "instance_type = 'ml.g4dn.12xlarge' # AMD Radeon Pro V520 4장 GPU\n",
    "\n",
    "hyperparameters = {'epochs': epochs, \n",
    "                    }  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "10515ee2-b28f-49d6-9d8f-d09bdb3f9ecd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sagemaker.pytorch import PyTorch\n",
    "import os\n",
    "\n",
    "estimator = PyTorch(\n",
    "    entry_point=\"TFT_Train.py\",    \n",
    "    source_dir='src',    \n",
    "    role=role,\n",
    "    framework_version='1.12.1',    \n",
    "    py_version='py38',     \n",
    "    instance_count=1,\n",
    "    instance_type=instance_type, # local_gpu or local 지정\n",
    "    session = sagemaker.Session(),\n",
    "    hyperparameters= hyperparameters               \n",
    "    \n",
    ")\n",
    "estimator.fit(wait=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "66e74128-03f0-48d1-b279-29f25d29e559",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-03-07 06:33:33 Starting - Starting the training job...\n",
      "2023-03-07 06:34:01 Starting - Preparing the instances for trainingProfilerReport-1678170813: InProgress\n",
      "......\n",
      "2023-03-07 06:34:53 Downloading - Downloading input data...\n",
      "2023-03-07 06:35:33 Training - Downloading the training image............\n",
      "2023-03-07 06:37:33 Training - Training image download completed. Training in progress.......\u001b[34mbash: cannot set terminal process group (-1): Inappropriate ioctl for device\u001b[0m\n",
      "\u001b[34mbash: no job control in this shell\u001b[0m\n",
      "\u001b[34m2023-03-07 06:38:19,326 sagemaker-training-toolkit INFO     Imported framework sagemaker_pytorch_container.training\u001b[0m\n",
      "\u001b[34m2023-03-07 06:38:19,363 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\u001b[0m\n",
      "\u001b[34m2023-03-07 06:38:19,373 sagemaker_pytorch_container.training INFO     Block until all host DNS lookups succeed.\u001b[0m\n",
      "\u001b[34m2023-03-07 06:38:19,375 sagemaker_pytorch_container.training INFO     Invoking user training script.\u001b[0m\n",
      "\u001b[34m2023-03-07 06:38:19,581 sagemaker-training-toolkit INFO     Installing dependencies from requirements.txt:\u001b[0m\n",
      "\u001b[34m/opt/conda/bin/python3.8 -m pip install -r requirements.txt\u001b[0m\n",
      "\u001b[34mCollecting pytorch-forecasting==0.10.3\u001b[0m\n",
      "\u001b[34mDownloading pytorch_forecasting-0.10.3-py3-none-any.whl (141 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 141.4/141.4 kB 6.6 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mCollecting pytorch-lightning==1.6.3\u001b[0m\n",
      "\u001b[34mDownloading pytorch_lightning-1.6.3-py3-none-any.whl (584 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 584.0/584.0 kB 73.5 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pyarrow==11.0.0 in /opt/conda/lib/python3.8/site-packages (from -r requirements.txt (line 5)) (11.0.0)\u001b[0m\n",
      "\u001b[34mCollecting tensorboard==2.12.0\u001b[0m\n",
      "\u001b[34mDownloading tensorboard-2.12.0-py3-none-any.whl (5.6 MB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 5.6/5.6 MB 119.9 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mCollecting optuna<3.0.0,>=2.3.0\u001b[0m\n",
      "\u001b[34mDownloading optuna-2.10.1-py3-none-any.whl (308 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 308.2/308.2 kB 52.0 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: torch<2.0,>=1.7 in /opt/conda/lib/python3.8/site-packages (from pytorch-forecasting==0.10.3->-r requirements.txt (line 2)) (1.12.1+cu113)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: scipy<2.0,>=1.8 in /opt/conda/lib/python3.8/site-packages (from pytorch-forecasting==0.10.3->-r requirements.txt (line 2)) (1.10.0)\u001b[0m\n",
      "\u001b[34mCollecting statsmodels\u001b[0m\n",
      "\u001b[34mDownloading statsmodels-0.13.5-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (9.9 MB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 9.9/9.9 MB 120.0 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pandas<2.0.0,>=1.3.0 in /opt/conda/lib/python3.8/site-packages (from pytorch-forecasting==0.10.3->-r requirements.txt (line 2)) (1.5.3)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: matplotlib in /opt/conda/lib/python3.8/site-packages (from pytorch-forecasting==0.10.3->-r requirements.txt (line 2)) (3.7.0)\u001b[0m\n",
      "\u001b[34mCollecting scikit-learn<1.2,>=0.24\u001b[0m\n",
      "\u001b[34mDownloading scikit_learn-1.1.3-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (31.2 MB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 31.2/31.2 MB 64.5 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mCollecting pyDeprecate<0.4.0,>=0.3.1\u001b[0m\n",
      "\u001b[34mDownloading pyDeprecate-0.3.2-py3-none-any.whl (10 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: typing-extensions>=4.0.0 in /opt/conda/lib/python3.8/site-packages (from pytorch-lightning==1.6.3->-r requirements.txt (line 4)) (4.4.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: packaging>=17.0 in /opt/conda/lib/python3.8/site-packages (from pytorch-lightning==1.6.3->-r requirements.txt (line 4)) (23.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: PyYAML>=5.4 in /opt/conda/lib/python3.8/site-packages (from pytorch-lightning==1.6.3->-r requirements.txt (line 4)) (5.4.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: fsspec[http]!=2021.06.0,>=2021.05.0 in /opt/conda/lib/python3.8/site-packages (from pytorch-lightning==1.6.3->-r requirements.txt (line 4)) (2023.1.0)\u001b[0m\n",
      "\u001b[34mCollecting torchmetrics>=0.4.1\u001b[0m\n",
      "\u001b[34mDownloading torchmetrics-0.11.3-py3-none-any.whl (518 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 518.6/518.6 kB 74.1 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: numpy>=1.17.2 in /opt/conda/lib/python3.8/site-packages (from pytorch-lightning==1.6.3->-r requirements.txt (line 4)) (1.23.5)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: tqdm>=4.57.0 in /opt/conda/lib/python3.8/site-packages (from pytorch-lightning==1.6.3->-r requirements.txt (line 4)) (4.64.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: requests<3,>=2.21.0 in /opt/conda/lib/python3.8/site-packages (from tensorboard==2.12.0->-r requirements.txt (line 6)) (2.28.2)\u001b[0m\n",
      "\u001b[34mCollecting grpcio>=1.48.2\u001b[0m\n",
      "\u001b[34mDownloading grpcio-1.51.3-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.8 MB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 4.8/4.8 MB 122.0 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: werkzeug>=1.0.1 in /opt/conda/lib/python3.8/site-packages (from tensorboard==2.12.0->-r requirements.txt (line 6)) (2.2.3)\u001b[0m\n",
      "\u001b[34mCollecting google-auth-oauthlib<0.5,>=0.4.1\u001b[0m\n",
      "\u001b[34mDownloading google_auth_oauthlib-0.4.6-py2.py3-none-any.whl (18 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: wheel>=0.26 in /opt/conda/lib/python3.8/site-packages (from tensorboard==2.12.0->-r requirements.txt (line 6)) (0.38.4)\u001b[0m\n",
      "\u001b[34mCollecting markdown>=2.6.8\u001b[0m\n",
      "\u001b[34mDownloading Markdown-3.4.1-py3-none-any.whl (93 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 93.3/93.3 kB 25.5 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: protobuf>=3.19.6 in /opt/conda/lib/python3.8/site-packages (from tensorboard==2.12.0->-r requirements.txt (line 6)) (3.20.2)\u001b[0m\n",
      "\u001b[34mCollecting tensorboard-plugin-wit>=1.6.0\u001b[0m\n",
      "\u001b[34mDownloading tensorboard_plugin_wit-1.8.1-py3-none-any.whl (781 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 781.3/781.3 kB 83.9 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: setuptools>=41.0.0 in /opt/conda/lib/python3.8/site-packages (from tensorboard==2.12.0->-r requirements.txt (line 6)) (65.6.3)\u001b[0m\n",
      "\u001b[34mCollecting tensorboard-data-server<0.8.0,>=0.7.0\u001b[0m\n",
      "\u001b[34mDownloading tensorboard_data_server-0.7.0-py3-none-manylinux2014_x86_64.whl (6.6 MB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 6.6/6.6 MB 130.8 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mCollecting google-auth<3,>=1.6.3\u001b[0m\n",
      "\u001b[34mDownloading google_auth-2.16.2-py2.py3-none-any.whl (177 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 177.2/177.2 kB 40.2 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mCollecting absl-py>=0.4\u001b[0m\n",
      "\u001b[34mDownloading absl_py-1.4.0-py3-none-any.whl (126 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 126.5/126.5 kB 27.5 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mCollecting aiohttp!=4.0.0a0,!=4.0.0a1\u001b[0m\n",
      "\u001b[34mDownloading aiohttp-3.8.4-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.0 MB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1.0/1.0 MB 81.2 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: six>=1.9.0 in /opt/conda/lib/python3.8/site-packages (from google-auth<3,>=1.6.3->tensorboard==2.12.0->-r requirements.txt (line 6)) (1.16.0)\u001b[0m\n",
      "\u001b[34mCollecting cachetools<6.0,>=2.0.0\u001b[0m\n",
      "\u001b[34mDownloading cachetools-5.3.0-py3-none-any.whl (9.3 kB)\u001b[0m\n",
      "\u001b[34mCollecting pyasn1-modules>=0.2.1\u001b[0m\n",
      "\u001b[34mDownloading pyasn1_modules-0.2.8-py2.py3-none-any.whl (155 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 155.3/155.3 kB 24.3 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: rsa<5,>=3.1.4 in /opt/conda/lib/python3.8/site-packages (from google-auth<3,>=1.6.3->tensorboard==2.12.0->-r requirements.txt (line 6)) (4.7.2)\u001b[0m\n",
      "\u001b[34mCollecting requests-oauthlib>=0.7.0\u001b[0m\n",
      "\u001b[34mDownloading requests_oauthlib-1.3.1-py2.py3-none-any.whl (23 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: importlib-metadata>=4.4 in /opt/conda/lib/python3.8/site-packages (from markdown>=2.6.8->tensorboard==2.12.0->-r requirements.txt (line 6)) (4.13.0)\u001b[0m\n",
      "\u001b[34mCollecting cmaes>=0.8.2\u001b[0m\n",
      "\u001b[34mDownloading cmaes-0.9.1-py3-none-any.whl (21 kB)\u001b[0m\n",
      "\u001b[34mCollecting cliff\u001b[0m\n",
      "\u001b[34mDownloading cliff-4.2.0-py3-none-any.whl (81 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 81.0/81.0 kB 16.6 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mCollecting alembic\u001b[0m\n",
      "\u001b[34mDownloading alembic-1.10.1-py3-none-any.whl (212 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 212.2/212.2 kB 48.7 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mCollecting sqlalchemy>=1.1.0\u001b[0m\n",
      "\u001b[34mDownloading SQLAlchemy-2.0.5.post1-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.8 MB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 2.8/2.8 MB 124.0 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mCollecting colorlog\u001b[0m\n",
      "\u001b[34mDownloading colorlog-6.7.0-py2.py3-none-any.whl (11 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: python-dateutil>=2.8.1 in /opt/conda/lib/python3.8/site-packages (from pandas<2.0.0,>=1.3.0->pytorch-forecasting==0.10.3->-r requirements.txt (line 2)) (2.8.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.8/site-packages (from pandas<2.0.0,>=1.3.0->pytorch-forecasting==0.10.3->-r requirements.txt (line 2)) (2022.7.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.8/site-packages (from requests<3,>=2.21.0->tensorboard==2.12.0->-r requirements.txt (line 6)) (1.26.14)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.8/site-packages (from requests<3,>=2.21.0->tensorboard==2.12.0->-r requirements.txt (line 6)) (2022.12.7)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.8/site-packages (from requests<3,>=2.21.0->tensorboard==2.12.0->-r requirements.txt (line 6)) (3.4)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.8/site-packages (from requests<3,>=2.21.0->tensorboard==2.12.0->-r requirements.txt (line 6)) (2.1.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: threadpoolctl>=2.0.0 in /opt/conda/lib/python3.8/site-packages (from scikit-learn<1.2,>=0.24->pytorch-forecasting==0.10.3->-r requirements.txt (line 2)) (3.1.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: joblib>=1.0.0 in /opt/conda/lib/python3.8/site-packages (from scikit-learn<1.2,>=0.24->pytorch-forecasting==0.10.3->-r requirements.txt (line 2)) (1.2.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: MarkupSafe>=2.1.1 in /opt/conda/lib/python3.8/site-packages (from werkzeug>=1.0.1->tensorboard==2.12.0->-r requirements.txt (line 6)) (2.1.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: kiwisolver>=1.0.1 in /opt/conda/lib/python3.8/site-packages (from matplotlib->pytorch-forecasting==0.10.3->-r requirements.txt (line 2)) (1.4.4)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pyparsing>=2.3.1 in /opt/conda/lib/python3.8/site-packages (from matplotlib->pytorch-forecasting==0.10.3->-r requirements.txt (line 2)) (3.0.9)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: contourpy>=1.0.1 in /opt/conda/lib/python3.8/site-packages (from matplotlib->pytorch-forecasting==0.10.3->-r requirements.txt (line 2)) (1.0.7)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pillow>=6.2.0 in /opt/conda/lib/python3.8/site-packages (from matplotlib->pytorch-forecasting==0.10.3->-r requirements.txt (line 2)) (9.4.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: importlib-resources>=3.2.0 in /opt/conda/lib/python3.8/site-packages (from matplotlib->pytorch-forecasting==0.10.3->-r requirements.txt (line 2)) (5.10.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: fonttools>=4.22.0 in /opt/conda/lib/python3.8/site-packages (from matplotlib->pytorch-forecasting==0.10.3->-r requirements.txt (line 2)) (4.38.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: cycler>=0.10 in /opt/conda/lib/python3.8/site-packages (from matplotlib->pytorch-forecasting==0.10.3->-r requirements.txt (line 2)) (0.11.0)\u001b[0m\n",
      "\u001b[34mCollecting patsy>=0.5.2\u001b[0m\n",
      "\u001b[34mDownloading patsy-0.5.3-py2.py3-none-any.whl (233 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 233.8/233.8 kB 44.1 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mCollecting multidict<7.0,>=4.5\u001b[0m\n",
      "\u001b[34mDownloading multidict-6.0.4-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (121 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 121.3/121.3 kB 26.8 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mCollecting aiosignal>=1.1.2\u001b[0m\n",
      "\u001b[34mDownloading aiosignal-1.3.1-py3-none-any.whl (7.6 kB)\u001b[0m\n",
      "\u001b[34mCollecting frozenlist>=1.1.1\u001b[0m\n",
      "\u001b[34mDownloading frozenlist-1.3.3-cp38-cp38-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (161 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 161.3/161.3 kB 38.6 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.8/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning==1.6.3->-r requirements.txt (line 4)) (22.2.0)\u001b[0m\n",
      "\u001b[34mCollecting async-timeout<5.0,>=4.0.0a3\u001b[0m\n",
      "\u001b[34mDownloading async_timeout-4.0.2-py3-none-any.whl (5.8 kB)\u001b[0m\n",
      "\u001b[34mCollecting yarl<2.0,>=1.0\u001b[0m\n",
      "\u001b[34mDownloading yarl-1.8.2-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (262 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 262.1/262.1 kB 58.3 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.8/site-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard==2.12.0->-r requirements.txt (line 6)) (3.13.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /opt/conda/lib/python3.8/site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard==2.12.0->-r requirements.txt (line 6)) (0.4.8)\u001b[0m\n",
      "\u001b[34mCollecting oauthlib>=3.0.0\u001b[0m\n",
      "\u001b[34mDownloading oauthlib-3.2.2-py3-none-any.whl (151 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 151.7/151.7 kB 40.4 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: greenlet!=0.4.17 in /opt/conda/lib/python3.8/site-packages (from sqlalchemy>=1.1.0->optuna<3.0.0,>=2.3.0->pytorch-forecasting==0.10.3->-r requirements.txt (line 2)) (2.0.2)\u001b[0m\n",
      "\u001b[34mCollecting Mako\u001b[0m\n",
      "\u001b[34mDownloading Mako-1.2.4-py3-none-any.whl (78 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 78.7/78.7 kB 18.1 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mCollecting cmd2>=1.0.0\u001b[0m\n",
      "\u001b[34mDownloading cmd2-2.4.3-py3-none-any.whl (147 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 147.2/147.2 kB 30.5 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mCollecting stevedore>=2.0.1\u001b[0m\n",
      "\u001b[34mDownloading stevedore-5.0.0-py3-none-any.whl (49 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 49.6/49.6 kB 9.5 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mCollecting autopage>=0.4.0\u001b[0m\n",
      "\u001b[34mDownloading autopage-0.5.1-py3-none-any.whl (29 kB)\u001b[0m\n",
      "\u001b[34mCollecting PrettyTable>=0.7.2\u001b[0m\n",
      "\u001b[34mDownloading prettytable-3.6.0-py3-none-any.whl (27 kB)\u001b[0m\n",
      "\u001b[34mCollecting pyperclip>=1.6\u001b[0m\n",
      "\u001b[34mDownloading pyperclip-1.8.2.tar.gz (20 kB)\u001b[0m\n",
      "\u001b[34mPreparing metadata (setup.py): started\u001b[0m\n",
      "\u001b[34mPreparing metadata (setup.py): finished with status 'done'\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: wcwidth>=0.1.7 in /opt/conda/lib/python3.8/site-packages (from cmd2>=1.0.0->cliff->optuna<3.0.0,>=2.3.0->pytorch-forecasting==0.10.3->-r requirements.txt (line 2)) (0.2.6)\u001b[0m\n",
      "\u001b[34mCollecting pbr!=2.1.0,>=2.0.0\u001b[0m\n",
      "\u001b[34mDownloading pbr-5.11.1-py2.py3-none-any.whl (112 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 112.7/112.7 kB 17.1 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mBuilding wheels for collected packages: pyperclip\u001b[0m\n",
      "\u001b[34mBuilding wheel for pyperclip (setup.py): started\u001b[0m\n",
      "\u001b[34mBuilding wheel for pyperclip (setup.py): finished with status 'done'\u001b[0m\n",
      "\u001b[34mCreated wheel for pyperclip: filename=pyperclip-1.8.2-py3-none-any.whl size=11124 sha256=7bbafb502f0c92465a23775625df9d568e3e7b0185c9a60e3c973f56954f0a2e\u001b[0m\n",
      "\u001b[34mStored in directory: /root/.cache/pip/wheels/7f/1a/65/84ff8c386bec21fca6d220ea1f5498a0367883a78dd5ba6122\u001b[0m\n",
      "\u001b[34mSuccessfully built pyperclip\u001b[0m\n",
      "\u001b[34mInstalling collected packages: tensorboard-plugin-wit, pyperclip, tensorboard-data-server, sqlalchemy, pyDeprecate, pyasn1-modules, PrettyTable, pbr, patsy, oauthlib, multidict, Mako, grpcio, frozenlist, colorlog, cmd2, cmaes, cachetools, autopage, async-timeout, absl-py, yarl, torchmetrics, stevedore, scikit-learn, requests-oauthlib, markdown, google-auth, alembic, aiosignal, statsmodels, google-auth-oauthlib, cliff, aiohttp, tensorboard, optuna, pytorch-lightning, pytorch-forecasting\u001b[0m\n",
      "\u001b[34mAttempting uninstall: scikit-learn\u001b[0m\n",
      "\u001b[34mFound existing installation: scikit-learn 1.2.1\u001b[0m\n",
      "\u001b[34mUninstalling scikit-learn-1.2.1:\u001b[0m\n",
      "\u001b[34mSuccessfully uninstalled scikit-learn-1.2.1\u001b[0m\n",
      "\u001b[34mSuccessfully installed Mako-1.2.4 PrettyTable-3.6.0 absl-py-1.4.0 aiohttp-3.8.4 aiosignal-1.3.1 alembic-1.10.1 async-timeout-4.0.2 autopage-0.5.1 cachetools-5.3.0 cliff-4.2.0 cmaes-0.9.1 cmd2-2.4.3 colorlog-6.7.0 frozenlist-1.3.3 google-auth-2.16.2 google-auth-oauthlib-0.4.6 grpcio-1.51.3 markdown-3.4.1 multidict-6.0.4 oauthlib-3.2.2 optuna-2.10.1 patsy-0.5.3 pbr-5.11.1 pyDeprecate-0.3.2 pyasn1-modules-0.2.8 pyperclip-1.8.2 pytorch-forecasting-0.10.3 pytorch-lightning-1.6.3 requests-oauthlib-1.3.1 scikit-learn-1.1.3 sqlalchemy-2.0.5.post1 statsmodels-0.13.5 stevedore-5.0.0 tensorboard-2.12.0 tensorboard-data-server-0.7.0 tensorboard-plugin-wit-1.8.1 torchmetrics-0.11.3 yarl-1.8.2\u001b[0m\n",
      "\u001b[34mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n",
      "\u001b[34m[notice] A new release of pip is available: 23.0 -> 23.0.1\u001b[0m\n",
      "\u001b[34m[notice] To update, run: pip install --upgrade pip\u001b[0m\n",
      "\u001b[34m2023-03-07 06:38:35,047 sagemaker-training-toolkit INFO     Waiting for the process to finish and give a return code.\u001b[0m\n",
      "\u001b[34m2023-03-07 06:38:35,047 sagemaker-training-toolkit INFO     Done waiting for a return code. Received 0 from exiting process.\u001b[0m\n",
      "\u001b[34m2023-03-07 06:38:35,086 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\u001b[0m\n",
      "\u001b[34m2023-03-07 06:38:35,134 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\u001b[0m\n",
      "\u001b[34m2023-03-07 06:38:35,182 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\u001b[0m\n",
      "\u001b[34m2023-03-07 06:38:35,192 sagemaker-training-toolkit INFO     Invoking user script\u001b[0m\n",
      "\u001b[34mTraining Env:\u001b[0m\n",
      "\u001b[34m{\n",
      "    \"additional_framework_parameters\": {},\n",
      "    \"channel_input_dirs\": {},\n",
      "    \"current_host\": \"algo-1\",\n",
      "    \"current_instance_group\": \"homogeneousCluster\",\n",
      "    \"current_instance_group_hosts\": [\n",
      "        \"algo-1\"\n",
      "    ],\n",
      "    \"current_instance_type\": \"ml.g4dn.12xlarge\",\n",
      "    \"distribution_hosts\": [],\n",
      "    \"distribution_instance_groups\": [],\n",
      "    \"framework_module\": \"sagemaker_pytorch_container.training:main\",\n",
      "    \"hosts\": [\n",
      "        \"algo-1\"\n",
      "    ],\n",
      "    \"hyperparameters\": {\n",
      "        \"epochs\": 2\n",
      "    },\n",
      "    \"input_config_dir\": \"/opt/ml/input/config\",\n",
      "    \"input_data_config\": {},\n",
      "    \"input_dir\": \"/opt/ml/input\",\n",
      "    \"instance_groups\": [\n",
      "        \"homogeneousCluster\"\n",
      "    ],\n",
      "    \"instance_groups_dict\": {\n",
      "        \"homogeneousCluster\": {\n",
      "            \"instance_group_name\": \"homogeneousCluster\",\n",
      "            \"instance_type\": \"ml.g4dn.12xlarge\",\n",
      "            \"hosts\": [\n",
      "                \"algo-1\"\n",
      "            ]\n",
      "        }\n",
      "    },\n",
      "    \"is_hetero\": false,\n",
      "    \"is_master\": true,\n",
      "    \"is_modelparallel_enabled\": null,\n",
      "    \"is_smddpmprun_installed\": true,\n",
      "    \"job_name\": \"pytorch-training-2023-03-07-06-33-33-202\",\n",
      "    \"log_level\": 20,\n",
      "    \"master_hostname\": \"algo-1\",\n",
      "    \"model_dir\": \"/opt/ml/model\",\n",
      "    \"module_dir\": \"s3://sagemaker-us-east-1-057716757052/pytorch-training-2023-03-07-06-33-33-202/source/sourcedir.tar.gz\",\n",
      "    \"module_name\": \"TFT_Train\",\n",
      "    \"network_interface_name\": \"eth0\",\n",
      "    \"num_cpus\": 48,\n",
      "    \"num_gpus\": 4,\n",
      "    \"num_neurons\": 0,\n",
      "    \"output_data_dir\": \"/opt/ml/output/data\",\n",
      "    \"output_dir\": \"/opt/ml/output\",\n",
      "    \"output_intermediate_dir\": \"/opt/ml/output/intermediate\",\n",
      "    \"resource_config\": {\n",
      "        \"current_host\": \"algo-1\",\n",
      "        \"current_instance_type\": \"ml.g4dn.12xlarge\",\n",
      "        \"current_group_name\": \"homogeneousCluster\",\n",
      "        \"hosts\": [\n",
      "            \"algo-1\"\n",
      "        ],\n",
      "        \"instance_groups\": [\n",
      "            {\n",
      "                \"instance_group_name\": \"homogeneousCluster\",\n",
      "                \"instance_type\": \"ml.g4dn.12xlarge\",\n",
      "                \"hosts\": [\n",
      "                    \"algo-1\"\n",
      "                ]\n",
      "            }\n",
      "        ],\n",
      "        \"network_interface_name\": \"eth0\"\n",
      "    },\n",
      "    \"user_entry_point\": \"TFT_Train.py\"\u001b[0m\n",
      "\u001b[34m}\u001b[0m\n",
      "\u001b[34mEnvironment variables:\u001b[0m\n",
      "\u001b[34mSM_HOSTS=[\"algo-1\"]\u001b[0m\n",
      "\u001b[34mSM_NETWORK_INTERFACE_NAME=eth0\u001b[0m\n",
      "\u001b[34mSM_HPS={\"epochs\":2}\u001b[0m\n",
      "\u001b[34mSM_USER_ENTRY_POINT=TFT_Train.py\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_PARAMS={}\u001b[0m\n",
      "\u001b[34mSM_RESOURCE_CONFIG={\"current_group_name\":\"homogeneousCluster\",\"current_host\":\"algo-1\",\"current_instance_type\":\"ml.g4dn.12xlarge\",\"hosts\":[\"algo-1\"],\"instance_groups\":[{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.g4dn.12xlarge\"}],\"network_interface_name\":\"eth0\"}\u001b[0m\n",
      "\u001b[34mSM_INPUT_DATA_CONFIG={}\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DATA_DIR=/opt/ml/output/data\u001b[0m\n",
      "\u001b[34mSM_CHANNELS=[]\u001b[0m\n",
      "\u001b[34mSM_CURRENT_HOST=algo-1\u001b[0m\n",
      "\u001b[34mSM_CURRENT_INSTANCE_TYPE=ml.g4dn.12xlarge\u001b[0m\n",
      "\u001b[34mSM_CURRENT_INSTANCE_GROUP=homogeneousCluster\u001b[0m\n",
      "\u001b[34mSM_CURRENT_INSTANCE_GROUP_HOSTS=[\"algo-1\"]\u001b[0m\n",
      "\u001b[34mSM_INSTANCE_GROUPS=[\"homogeneousCluster\"]\u001b[0m\n",
      "\u001b[34mSM_INSTANCE_GROUPS_DICT={\"homogeneousCluster\":{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.g4dn.12xlarge\"}}\u001b[0m\n",
      "\u001b[34mSM_DISTRIBUTION_INSTANCE_GROUPS=[]\u001b[0m\n",
      "\u001b[34mSM_IS_HETERO=false\u001b[0m\n",
      "\u001b[34mSM_MODULE_NAME=TFT_Train\u001b[0m\n",
      "\u001b[34mSM_LOG_LEVEL=20\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_MODULE=sagemaker_pytorch_container.training:main\u001b[0m\n",
      "\u001b[34mSM_INPUT_DIR=/opt/ml/input\u001b[0m\n",
      "\u001b[34mSM_INPUT_CONFIG_DIR=/opt/ml/input/config\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DIR=/opt/ml/output\u001b[0m\n",
      "\u001b[34mSM_NUM_CPUS=48\u001b[0m\n",
      "\u001b[34mSM_NUM_GPUS=4\u001b[0m\n",
      "\u001b[34mSM_NUM_NEURONS=0\u001b[0m\n",
      "\u001b[34mSM_MODEL_DIR=/opt/ml/model\u001b[0m\n",
      "\u001b[34mSM_MODULE_DIR=s3://sagemaker-us-east-1-057716757052/pytorch-training-2023-03-07-06-33-33-202/source/sourcedir.tar.gz\u001b[0m\n",
      "\u001b[34mSM_TRAINING_ENV={\"additional_framework_parameters\":{},\"channel_input_dirs\":{},\"current_host\":\"algo-1\",\"current_instance_group\":\"homogeneousCluster\",\"current_instance_group_hosts\":[\"algo-1\"],\"current_instance_type\":\"ml.g4dn.12xlarge\",\"distribution_hosts\":[],\"distribution_instance_groups\":[],\"framework_module\":\"sagemaker_pytorch_container.training:main\",\"hosts\":[\"algo-1\"],\"hyperparameters\":{\"epochs\":2},\"input_config_dir\":\"/opt/ml/input/config\",\"input_data_config\":{},\"input_dir\":\"/opt/ml/input\",\"instance_groups\":[\"homogeneousCluster\"],\"instance_groups_dict\":{\"homogeneousCluster\":{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.g4dn.12xlarge\"}},\"is_hetero\":false,\"is_master\":true,\"is_modelparallel_enabled\":null,\"is_smddpmprun_installed\":true,\"job_name\":\"pytorch-training-2023-03-07-06-33-33-202\",\"log_level\":20,\"master_hostname\":\"algo-1\",\"model_dir\":\"/opt/ml/model\",\"module_dir\":\"s3://sagemaker-us-east-1-057716757052/pytorch-training-2023-03-07-06-33-33-202/source/sourcedir.tar.gz\",\"module_name\":\"TFT_Train\",\"network_interface_name\":\"eth0\",\"num_cpus\":48,\"num_gpus\":4,\"num_neurons\":0,\"output_data_dir\":\"/opt/ml/output/data\",\"output_dir\":\"/opt/ml/output\",\"output_intermediate_dir\":\"/opt/ml/output/intermediate\",\"resource_config\":{\"current_group_name\":\"homogeneousCluster\",\"current_host\":\"algo-1\",\"current_instance_type\":\"ml.g4dn.12xlarge\",\"hosts\":[\"algo-1\"],\"instance_groups\":[{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.g4dn.12xlarge\"}],\"network_interface_name\":\"eth0\"},\"user_entry_point\":\"TFT_Train.py\"}\u001b[0m\n",
      "\u001b[34mSM_USER_ARGS=[\"--epochs\",\"2\"]\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_INTERMEDIATE_DIR=/opt/ml/output/intermediate\u001b[0m\n",
      "\u001b[34mSM_HP_EPOCHS=2\u001b[0m\n",
      "\u001b[34mPYTHONPATH=/opt/ml/code:/opt/conda/bin:/opt/conda/lib/python38.zip:/opt/conda/lib/python3.8:/opt/conda/lib/python3.8/lib-dynload:/opt/conda/lib/python3.8/site-packages:/opt/conda/lib/python3.8/site-packages/smdebug-1.0.24b20230214-py3.8.egg:/opt/conda/lib/python3.8/site-packages/pyinstrument-3.4.2-py3.8.egg:/opt/conda/lib/python3.8/site-packages/pyinstrument_cext-0.2.4-py3.8-linux-x86_64.egg:/opt/conda/lib/python3.8/site-packages/flash_attn-0.1-py3.8-linux-x86_64.egg:/opt/conda/lib/python3.8/site-packages/einops-0.6.0-py3.8.egg\u001b[0m\n",
      "\u001b[34mInvoking script with the following command:\u001b[0m\n",
      "\u001b[34m/opt/conda/bin/python3.8 TFT_Train.py --epochs 2\u001b[0m\n",
      "\u001b[34m2023-03-07 06:38:37,589 sagemaker-training-toolkit INFO     Exceptions not imported for SageMaker TF as Tensorflow is not installed.\u001b[0m\n",
      "\u001b[34mNot running on notebook\u001b[0m\n",
      "\u001b[34m***** Arguments *****\u001b[0m\n",
      "\u001b[34mepochs=2\u001b[0m\n",
      "\u001b[34mseed=100\u001b[0m\n",
      "\u001b[34mtrain_batch_size=64\u001b[0m\n",
      "\u001b[34mmodel_dir=/opt/ml/model\u001b[0m\n",
      "\u001b[34mn_gpus=4\u001b[0m\n",
      "\u001b[34mGPU available: True, used: True\u001b[0m\n",
      "\u001b[34mTPU available: False, using: 0 TPU cores\u001b[0m\n",
      "\u001b[34mIPU available: False, using: 0 IPUs\u001b[0m\n",
      "\u001b[34mHPU available: False, using: 0 HPUs\u001b[0m\n",
      "\u001b[34mNumber of parameters in network: 29.7k\u001b[0m\n",
      "\u001b[34mInitializing distributed: GLOBAL_RANK: 0, MEMBER: 1/4\u001b[0m\n",
      "\u001b[34mInitializing distributed: GLOBAL_RANK: 1, MEMBER: 2/4\u001b[0m\n",
      "\u001b[34mInitializing distributed: GLOBAL_RANK: 2, MEMBER: 3/4\u001b[0m\n",
      "\u001b[34mInitializing distributed: GLOBAL_RANK: 3, MEMBER: 4/4\u001b[0m\n",
      "\u001b[34m----------------------------------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[34mdistributed_backend=nccl\u001b[0m\n",
      "\u001b[34mAll distributed processes registered. Starting with 4 processes\u001b[0m\n",
      "\u001b[34m----------------------------------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[34mMissing logger folder: lightning_logs/lightning_logs\u001b[0m\n",
      "\u001b[34mMissing logger folder: lightning_logs/lightning_logs\u001b[0m\n",
      "\u001b[34mMissing logger folder: lightning_logs/lightning_logs\u001b[0m\n",
      "\u001b[34mMissing logger folder: lightning_logs/lightning_logs\u001b[0m\n",
      "\u001b[34mNCCL version 2.10.3+cuda11.3\u001b[0m\n",
      "\u001b[34mLOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3]\u001b[0m\n",
      "\u001b[34mLOCAL_RANK: 2 - CUDA_VISIBLE_DEVICES: [0,1,2,3]\u001b[0m\n",
      "\u001b[34mLOCAL_RANK: 1 - CUDA_VISIBLE_DEVICES: [0,1,2,3]\u001b[0m\n",
      "\u001b[34mLOCAL_RANK: 3 - CUDA_VISIBLE_DEVICES: [0,1,2,3]\u001b[0m\n",
      "\u001b[34m| Name                               | Type                            | Params\u001b[0m\n",
      "\u001b[34m----------------------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[34m0  | loss                               | QuantileLoss                    | 0     \u001b[0m\n",
      "\u001b[34m1  | logging_metrics                    | ModuleList                      | 0     \u001b[0m\n",
      "\u001b[34m2  | input_embeddings                   | MultiEmbedding                  | 1.3 K \u001b[0m\n",
      "\u001b[34m3  | prescalers                         | ModuleDict                      | 256   \u001b[0m\n",
      "\u001b[34m4  | static_variable_selection          | VariableSelectionNetwork        | 3.4 K \u001b[0m\n",
      "\u001b[34m5  | encoder_variable_selection         | VariableSelectionNetwork        | 8.0 K \u001b[0m\n",
      "\u001b[34m6  | decoder_variable_selection         | VariableSelectionNetwork        | 2.7 K \u001b[0m\n",
      "\u001b[34m7  | static_context_variable_selection  | GatedResidualNetwork            | 1.1 K \u001b[0m\n",
      "\u001b[34m8  | static_context_initial_hidden_lstm | GatedResidualNetwork            | 1.1 K \u001b[0m\n",
      "\u001b[34m9  | static_context_initial_cell_lstm   | GatedResidualNetwork            | 1.1 K \u001b[0m\n",
      "\u001b[34m10 | static_context_enrichment          | GatedResidualNetwork            | 1.1 K \u001b[0m\n",
      "\u001b[34m11 | lstm_encoder                       | LSTM                            | 2.2 K \u001b[0m\n",
      "\u001b[34m12 | lstm_decoder                       | LSTM                            | 2.2 K \u001b[0m\n",
      "\u001b[34m13 | post_lstm_gate_encoder             | GatedLinearUnit                 | 544   \u001b[0m\n",
      "\u001b[34m14 | post_lstm_add_norm_encoder         | AddNorm                         | 32    \u001b[0m\n",
      "\u001b[34m15 | static_enrichment                  | GatedResidualNetwork            | 1.4 K \u001b[0m\n",
      "\u001b[34m16 | multihead_attn                     | InterpretableMultiHeadAttention | 1.1 K \u001b[0m\n",
      "\u001b[34m17 | post_attn_gate_norm                | GateAddNorm                     | 576   \u001b[0m\n",
      "\u001b[34m18 | pos_wise_ff                        | GatedResidualNetwork            | 1.1 K \u001b[0m\n",
      "\u001b[34m19 | pre_output_gate_norm               | GateAddNorm                     | 576   \u001b[0m\n",
      "\u001b[34m20 | output_layer                       | Linear                          | 119   \u001b[0m\n",
      "\u001b[34m----------------------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[34m29.7 K    Trainable params\u001b[0m\n",
      "\u001b[34m0         Non-trainable params\u001b[0m\n",
      "\u001b[34m29.7 K    Total params\u001b[0m\n",
      "\u001b[34m0.119     Total estimated model params size (MB)\u001b[0m\n",
      "\u001b[34mSanity Checking: 0it [00:00, ?it/s]\u001b[0m\n",
      "\u001b[34m[2023-03-07 06:38:59.705 algo-1:302 INFO utils.py:28] RULE_JOB_STOP_SIGNAL_FILENAME: None\u001b[0m\n",
      "\u001b[34m[2023-03-07 06:38:59.706 algo-1:420 INFO utils.py:28] RULE_JOB_STOP_SIGNAL_FILENAME: None\u001b[0m\n",
      "\u001b[34m[2023-03-07 06:38:59.706 algo-1:245 INFO utils.py:28] RULE_JOB_STOP_SIGNAL_FILENAME: None\u001b[0m\n",
      "\u001b[34m[2023-03-07 06:38:59.710 algo-1:361 INFO utils.py:28] RULE_JOB_STOP_SIGNAL_FILENAME: None\u001b[0m\n",
      "\u001b[34m[2023-03-07 06:38:59.836 algo-1:302 INFO profiler_config_parser.py:111] User has disabled profiler.\u001b[0m\n",
      "\u001b[34m[2023-03-07 06:38:59.837 algo-1:420 INFO profiler_config_parser.py:111] User has disabled profiler.\u001b[0m\n",
      "\u001b[34m[2023-03-07 06:38:59.838 algo-1:302 INFO json_config.py:92] Creating hook from json_config at /opt/ml/input/config/debughookconfig.json.\u001b[0m\n",
      "\u001b[34m[2023-03-07 06:38:59.838 algo-1:302 INFO hook.py:206] tensorboard_dir has not been set for the hook. SMDebug will not be exporting tensorboard summaries.\u001b[0m\n",
      "\u001b[34m[2023-03-07 06:38:59.838 algo-1:420 INFO json_config.py:92] Creating hook from json_config at /opt/ml/input/config/debughookconfig.json.\u001b[0m\n",
      "\u001b[34m[2023-03-07 06:38:59.838 algo-1:245 INFO profiler_config_parser.py:111] User has disabled profiler.\u001b[0m\n",
      "\u001b[34m[2023-03-07 06:38:59.838 algo-1:302 INFO hook.py:259] Saving to /opt/ml/output/tensors\u001b[0m\n",
      "\u001b[34m[2023-03-07 06:38:59.838 algo-1:420 INFO hook.py:206] tensorboard_dir has not been set for the hook. SMDebug will not be exporting tensorboard summaries.\u001b[0m\n",
      "\u001b[34m[2023-03-07 06:38:59.838 algo-1:302 INFO state_store.py:77] The checkpoint config file /opt/ml/input/config/checkpointconfig.json does not exist.\u001b[0m\n",
      "\u001b[34m[2023-03-07 06:38:59.839 algo-1:420 INFO hook.py:259] Saving to /opt/ml/output/tensors\u001b[0m\n",
      "\u001b[34m[2023-03-07 06:38:59.839 algo-1:420 INFO state_store.py:77] The checkpoint config file /opt/ml/input/config/checkpointconfig.json does not exist.\u001b[0m\n",
      "\u001b[34m[2023-03-07 06:38:59.839 algo-1:245 INFO json_config.py:92] Creating hook from json_config at /opt/ml/input/config/debughookconfig.json.\u001b[0m\n",
      "\u001b[34m[2023-03-07 06:38:59.840 algo-1:245 INFO hook.py:206] tensorboard_dir has not been set for the hook. SMDebug will not be exporting tensorboard summaries.\u001b[0m\n",
      "\u001b[34m[2023-03-07 06:38:59.840 algo-1:245 INFO hook.py:259] Saving to /opt/ml/output/tensors\u001b[0m\n",
      "\u001b[34m[2023-03-07 06:38:59.840 algo-1:245 INFO state_store.py:77] The checkpoint config file /opt/ml/input/config/checkpointconfig.json does not exist.\u001b[0m\n",
      "\u001b[34m[2023-03-07 06:38:59.846 algo-1:361 INFO profiler_config_parser.py:111] User has disabled profiler.\u001b[0m\n",
      "\u001b[34m[2023-03-07 06:38:59.847 algo-1:361 INFO json_config.py:92] Creating hook from json_config at /opt/ml/input/config/debughookconfig.json.\u001b[0m\n",
      "\u001b[34m[2023-03-07 06:38:59.847 algo-1:361 INFO hook.py:206] tensorboard_dir has not been set for the hook. SMDebug will not be exporting tensorboard summaries.\u001b[0m\n",
      "\u001b[34m[2023-03-07 06:38:59.848 algo-1:361 INFO hook.py:259] Saving to /opt/ml/output/tensors\u001b[0m\n",
      "\u001b[34m[2023-03-07 06:38:59.848 algo-1:361 INFO state_store.py:77] The checkpoint config file /opt/ml/input/config/checkpointconfig.json does not exist.\u001b[0m\n",
      "\u001b[34mSanity Checking:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34mSanity Checking DataLoader 0:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34mSanity Checking DataLoader 0: 100%|██████████| 1/1 [00:00<00:00,  1.89it/s]\u001b[0m\n",
      "\u001b[34mSanity Checking DataLoader 0: 100%|██████████| 1/1 [00:00<00:00,  1.89it/s]\u001b[0m\n",
      "\u001b[34mTraining: 0it [00:00, ?it/s]\u001b[0m\n",
      "\u001b[34mTraining:   0%|          | 0/41 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34mEpoch 0:   0%|          | 0/41 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34m[W reducer.cpp:1251] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())\u001b[0m\n",
      "\u001b[34m[W reducer.cpp:1251] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())\u001b[0m\n",
      "\u001b[34m[W reducer.cpp:1251] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())\u001b[0m\n",
      "\u001b[34m[W reducer.cpp:1251] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())\u001b[0m\n",
      "\u001b[34mEpoch 0:   2%|▏         | 1/41 [00:00<00:12,  3.13it/s]\u001b[0m\n",
      "\u001b[34mEpoch 0:   2%|▏         | 1/41 [00:00<00:12,  3.13it/s]\u001b[0m\n",
      "\u001b[34mEpoch 0:   2%|▏         | 1/41 [00:00<00:12,  3.13it/s, loss=351, v_num=0, train_loss_step=351.0]\u001b[0m\n",
      "\u001b[34mEpoch 0:   5%|▍         | 2/41 [00:00<00:09,  4.12it/s, loss=351, v_num=0, train_loss_step=351.0]#015Epoch 0:   5%|▍         | 2/41 [00:00<00:09,  4.12it/s, loss=351, v_num=0, train_loss_step=351.0]\u001b[0m\n",
      "\u001b[34mEpoch 0:   5%|▍         | 2/41 [00:00<00:09,  4.11it/s, loss=350, v_num=0, train_loss_step=350.0]\u001b[0m\n",
      "\u001b[34mEpoch 0:   7%|▋         | 3/41 [00:00<00:08,  4.55it/s, loss=350, v_num=0, train_loss_step=350.0]#015Epoch 0:   7%|▋         | 3/41 [00:00<00:08,  4.55it/s, loss=350, v_num=0, train_loss_step=350.0]\u001b[0m\n",
      "\u001b[34mEpoch 0:   7%|▋         | 3/41 [00:00<00:08,  4.55it/s, loss=360, v_num=0, train_loss_step=379.0]\u001b[0m\n",
      "\u001b[34mEpoch 0:  10%|▉         | 4/41 [00:00<00:07,  4.80it/s, loss=360, v_num=0, train_loss_step=379.0]#015Epoch 0:  10%|▉         | 4/41 [00:00<00:07,  4.80it/s, loss=360, v_num=0, train_loss_step=379.0]\u001b[0m\n",
      "\u001b[34mEpoch 0:  10%|▉         | 4/41 [00:00<00:07,  4.80it/s, loss=331, v_num=0, train_loss_step=246.0]\u001b[0m\n",
      "\u001b[34mEpoch 0:  12%|█▏        | 5/41 [00:01<00:07,  4.91it/s, loss=331, v_num=0, train_loss_step=246.0]\u001b[0m\n",
      "\u001b[34mEpoch 0:  12%|█▏        | 5/41 [00:01<00:07,  4.91it/s, loss=331, v_num=0, train_loss_step=246.0]\u001b[0m\n",
      "\u001b[34mEpoch 0:  12%|█▏        | 5/41 [00:01<00:07,  4.91it/s, loss=348, v_num=0, train_loss_step=413.0]\u001b[0m\n",
      "\u001b[34mEpoch 0:  15%|█▍        | 6/41 [00:01<00:07,  4.91it/s, loss=348, v_num=0, train_loss_step=413.0]#015Epoch 0:  15%|█▍        | 6/41 [00:01<00:07,  4.91it/s, loss=348, v_num=0, train_loss_step=413.0]\u001b[0m\n",
      "\u001b[34mEpoch 0:  15%|█▍        | 6/41 [00:01<00:07,  4.91it/s, loss=342, v_num=0, train_loss_step=315.0]\u001b[0m\n",
      "\u001b[34mEpoch 0:  17%|█▋        | 7/41 [00:01<00:06,  4.93it/s, loss=342, v_num=0, train_loss_step=315.0]\u001b[0m\n",
      "\u001b[34mEpoch 0:  17%|█▋        | 7/41 [00:01<00:06,  4.93it/s, loss=342, v_num=0, train_loss_step=315.0]\u001b[0m\n",
      "\u001b[34mEpoch 0:  17%|█▋        | 7/41 [00:01<00:06,  4.93it/s, loss=331, v_num=0, train_loss_step=264.0]\u001b[0m\n",
      "\u001b[34mEpoch 0:  20%|█▉        | 8/41 [00:01<00:06,  4.95it/s, loss=331, v_num=0, train_loss_step=264.0]#015Epoch 0:  20%|█▉        | 8/41 [00:01<00:06,  4.95it/s, loss=331, v_num=0, train_loss_step=264.0]\u001b[0m\n",
      "\u001b[34mEpoch 0:  20%|█▉        | 8/41 [00:01<00:06,  4.95it/s, loss=326, v_num=0, train_loss_step=293.0]\u001b[0m\n",
      "\u001b[34mEpoch 0:  22%|██▏       | 9/41 [00:01<00:06,  4.95it/s, loss=326, v_num=0, train_loss_step=293.0]#015Epoch 0:  22%|██▏       | 9/41 [00:01<00:06,  4.95it/s, loss=326, v_num=0, train_loss_step=293.0]\u001b[0m\n",
      "\u001b[34mEpoch 0:  22%|██▏       | 9/41 [00:01<00:06,  4.95it/s, loss=333, v_num=0, train_loss_step=386.0]\u001b[0m\n",
      "\u001b[34mEpoch 0:  24%|██▍       | 10/41 [00:02<00:06,  4.96it/s, loss=333, v_num=0, train_loss_step=386.0]#015Epoch 0:  24%|██▍       | 10/41 [00:02<00:06,  4.96it/s, loss=333, v_num=0, train_loss_step=386.0]\u001b[0m\n",
      "\u001b[34mEpoch 0:  24%|██▍       | 10/41 [00:02<00:06,  4.96it/s, loss=328, v_num=0, train_loss_step=280.0]\u001b[0m\n",
      "\u001b[34mEpoch 0:  27%|██▋       | 11/41 [00:02<00:06,  4.97it/s, loss=328, v_num=0, train_loss_step=280.0]#015Epoch 0:  27%|██▋       | 11/41 [00:02<00:06,  4.97it/s, loss=328, v_num=0, train_loss_step=280.0]\u001b[0m\n",
      "\u001b[34mEpoch 0:  27%|██▋       | 11/41 [00:02<00:06,  4.97it/s, loss=325, v_num=0, train_loss_step=296.0]\u001b[0m\n",
      "\u001b[34mEpoch 0:  29%|██▉       | 12/41 [00:02<00:05,  4.97it/s, loss=325, v_num=0, train_loss_step=296.0]#015Epoch 0:  29%|██▉       | 12/41 [00:02<00:05,  4.97it/s, loss=325, v_num=0, train_loss_step=296.0]\u001b[0m\n",
      "\u001b[34mEpoch 0:  29%|██▉       | 12/41 [00:02<00:05,  4.97it/s, loss=314, v_num=0, train_loss_step=193.0]\u001b[0m\n",
      "\u001b[34mEpoch 0:  32%|███▏      | 13/41 [00:02<00:05,  5.02it/s, loss=314, v_num=0, train_loss_step=193.0]\u001b[0m\n",
      "\u001b[34mEpoch 0:  32%|███▏      | 13/41 [00:02<00:05,  5.02it/s, loss=314, v_num=0, train_loss_step=193.0]\u001b[0m\n",
      "\u001b[34mEpoch 0:  32%|███▏      | 13/41 [00:02<00:05,  5.02it/s, loss=310, v_num=0, train_loss_step=263.0]\u001b[0m\n",
      "\u001b[34mEpoch 0:  34%|███▍      | 14/41 [00:02<00:05,  5.06it/s, loss=310, v_num=0, train_loss_step=263.0]#015Epoch 0:  34%|███▍      | 14/41 [00:02<00:05,  5.06it/s, loss=310, v_num=0, train_loss_step=263.0]\u001b[0m\n",
      "\u001b[34mEpoch 0:  34%|███▍      | 14/41 [00:02<00:05,  5.06it/s, loss=308, v_num=0, train_loss_step=282.0]\u001b[0m\n",
      "\u001b[34mEpoch 0:  37%|███▋      | 15/41 [00:02<00:05,  5.10it/s, loss=308, v_num=0, train_loss_step=282.0]\u001b[0m\n",
      "\u001b[34mEpoch 0:  37%|███▋      | 15/41 [00:02<00:05,  5.10it/s, loss=308, v_num=0, train_loss_step=282.0]\u001b[0m\n",
      "\u001b[34mEpoch 0:  37%|███▋      | 15/41 [00:02<00:05,  5.10it/s, loss=301, v_num=0, train_loss_step=198.0]\u001b[0m\n",
      "\u001b[34mEpoch 0:  39%|███▉      | 16/41 [00:03<00:04,  5.13it/s, loss=301, v_num=0, train_loss_step=198.0]\u001b[0m\n",
      "\u001b[34mEpoch 0:  39%|███▉      | 16/41 [00:03<00:04,  5.13it/s, loss=301, v_num=0, train_loss_step=198.0]\u001b[0m\n",
      "\u001b[34mEpoch 0:  39%|███▉      | 16/41 [00:03<00:04,  5.13it/s, loss=296, v_num=0, train_loss_step=230.0]\u001b[0m\n",
      "\u001b[34mEpoch 0:  41%|████▏     | 17/41 [00:03<00:04,  5.16it/s, loss=296, v_num=0, train_loss_step=230.0]\u001b[0m\n",
      "\u001b[34mEpoch 0:  41%|████▏     | 17/41 [00:03<00:04,  5.16it/s, loss=296, v_num=0, train_loss_step=230.0]\u001b[0m\n",
      "\u001b[34mEpoch 0:  41%|████▏     | 17/41 [00:03<00:04,  5.16it/s, loss=295, v_num=0, train_loss_step=277.0]\u001b[0m\n",
      "\u001b[34mEpoch 0:  44%|████▍     | 18/41 [00:03<00:04,  5.17it/s, loss=295, v_num=0, train_loss_step=277.0]\u001b[0m\n",
      "\u001b[34mEpoch 0:  44%|████▍     | 18/41 [00:03<00:04,  5.17it/s, loss=295, v_num=0, train_loss_step=277.0]\u001b[0m\n",
      "\u001b[34mEpoch 0:  44%|████▍     | 18/41 [00:03<00:04,  5.17it/s, loss=293, v_num=0, train_loss_step=254.0]\u001b[0m\n",
      "\u001b[34mEpoch 0:  46%|████▋     | 19/41 [00:03<00:04,  5.19it/s, loss=293, v_num=0, train_loss_step=254.0]#015Epoch 0:  46%|████▋     | 19/41 [00:03<00:04,  5.19it/s, loss=293, v_num=0, train_loss_step=254.0]\u001b[0m\n",
      "\u001b[34mEpoch 0:  46%|████▋     | 19/41 [00:03<00:04,  5.19it/s, loss=289, v_num=0, train_loss_step=221.0]\u001b[0m\n",
      "\u001b[34mEpoch 0:  49%|████▉     | 20/41 [00:03<00:04,  5.21it/s, loss=289, v_num=0, train_loss_step=221.0]\u001b[0m\n",
      "\u001b[34mEpoch 0:  49%|████▉     | 20/41 [00:03<00:04,  5.21it/s, loss=289, v_num=0, train_loss_step=221.0]\u001b[0m\n",
      "\u001b[34mEpoch 0:  49%|████▉     | 20/41 [00:03<00:04,  5.21it/s, loss=285, v_num=0, train_loss_step=206.0]\u001b[0m\n",
      "\u001b[34mEpoch 0:  51%|█████     | 21/41 [00:04<00:03,  5.24it/s, loss=285, v_num=0, train_loss_step=206.0]\u001b[0m\n",
      "\u001b[34mEpoch 0:  51%|█████     | 21/41 [00:04<00:03,  5.24it/s, loss=285, v_num=0, train_loss_step=206.0]\u001b[0m\n",
      "\u001b[34mEpoch 0:  51%|█████     | 21/41 [00:04<00:03,  5.24it/s, loss=277, v_num=0, train_loss_step=202.0]\u001b[0m\n",
      "\u001b[34mEpoch 0:  54%|█████▎    | 22/41 [00:04<00:03,  5.26it/s, loss=277, v_num=0, train_loss_step=202.0]\u001b[0m\n",
      "\u001b[34mEpoch 0:  54%|█████▎    | 22/41 [00:04<00:03,  5.26it/s, loss=277, v_num=0, train_loss_step=202.0]\u001b[0m\n",
      "\u001b[34mEpoch 0:  54%|█████▎    | 22/41 [00:04<00:03,  5.26it/s, loss=270, v_num=0, train_loss_step=198.0]\u001b[0m\n",
      "\u001b[34mEpoch 0:  56%|█████▌    | 23/41 [00:04<00:03,  5.28it/s, loss=270, v_num=0, train_loss_step=198.0]#015Epoch 0:  56%|█████▌    | 23/41 [00:04<00:03,  5.28it/s, loss=270, v_num=0, train_loss_step=198.0]\u001b[0m\n",
      "\u001b[34mEpoch 0:  56%|█████▌    | 23/41 [00:04<00:03,  5.28it/s, loss=262, v_num=0, train_loss_step=227.0]\u001b[0m\n",
      "\u001b[34mEpoch 0:  59%|█████▊    | 24/41 [00:04<00:03,  5.28it/s, loss=262, v_num=0, train_loss_step=227.0]#015Epoch 0:  59%|█████▊    | 24/41 [00:04<00:03,  5.28it/s, loss=262, v_num=0, train_loss_step=227.0]\u001b[0m\n",
      "\u001b[34mEpoch 0:  59%|█████▊    | 24/41 [00:04<00:03,  5.28it/s, loss=261, v_num=0, train_loss_step=232.0]\u001b[0m\n",
      "\u001b[34mEpoch 0:  61%|██████    | 25/41 [00:04<00:03,  5.30it/s, loss=261, v_num=0, train_loss_step=232.0]\u001b[0m\n",
      "\u001b[34mEpoch 0:  61%|██████    | 25/41 [00:04<00:03,  5.30it/s, loss=261, v_num=0, train_loss_step=232.0]\u001b[0m\n",
      "\u001b[34mEpoch 0:  61%|██████    | 25/41 [00:04<00:03,  5.30it/s, loss=253, v_num=0, train_loss_step=247.0]\u001b[0m\n",
      "\u001b[34mEpoch 0:  63%|██████▎   | 26/41 [00:04<00:02,  5.31it/s, loss=253, v_num=0, train_loss_step=247.0]#015Epoch 0:  63%|██████▎   | 26/41 [00:04<00:02,  5.31it/s, loss=253, v_num=0, train_loss_step=247.0]\u001b[0m\n",
      "\u001b[34mEpoch 0:  63%|██████▎   | 26/41 [00:04<00:02,  5.31it/s, loss=247, v_num=0, train_loss_step=190.0]\u001b[0m\n",
      "\u001b[34mEpoch 0:  66%|██████▌   | 27/41 [00:05<00:02,  5.33it/s, loss=247, v_num=0, train_loss_step=190.0]#015Epoch 0:  66%|██████▌   | 27/41 [00:05<00:02,  5.33it/s, loss=247, v_num=0, train_loss_step=190.0]\u001b[0m\n",
      "\u001b[34mEpoch 0:  66%|██████▌   | 27/41 [00:05<00:02,  5.33it/s, loss=244, v_num=0, train_loss_step=210.0]\u001b[0m\n",
      "\u001b[34mEpoch 0:  68%|██████▊   | 28/41 [00:05<00:02,  5.34it/s, loss=244, v_num=0, train_loss_step=210.0]#015Epoch 0:  68%|██████▊   | 28/41 [00:05<00:02,  5.34it/s, loss=244, v_num=0, train_loss_step=210.0]\u001b[0m\n",
      "\u001b[34mEpoch 0:  68%|██████▊   | 28/41 [00:05<00:02,  5.34it/s, loss=240, v_num=0, train_loss_step=202.0]\u001b[0m\n",
      "\u001b[34mEpoch 0:  71%|███████   | 29/41 [00:05<00:02,  5.36it/s, loss=240, v_num=0, train_loss_step=202.0]\u001b[0m\n",
      "\u001b[34mEpoch 0:  71%|███████   | 29/41 [00:05<00:02,  5.36it/s, loss=240, v_num=0, train_loss_step=202.0]\u001b[0m\n",
      "\u001b[34mEpoch 0:  71%|███████   | 29/41 [00:05<00:02,  5.36it/s, loss=229, v_num=0, train_loss_step=166.0]\u001b[0m\n",
      "\u001b[34mEpoch 0:  73%|███████▎  | 30/41 [00:05<00:02,  5.36it/s, loss=229, v_num=0, train_loss_step=166.0]\u001b[0m\n",
      "\u001b[34mEpoch 0:  73%|███████▎  | 30/41 [00:05<00:02,  5.36it/s, loss=229, v_num=0, train_loss_step=166.0]\u001b[0m\n",
      "\u001b[34mEpoch 0:  73%|███████▎  | 30/41 [00:05<00:02,  5.36it/s, loss=222, v_num=0, train_loss_step=145.0]\u001b[0m\n",
      "\u001b[34mEpoch 0:  76%|███████▌  | 31/41 [00:05<00:01,  5.37it/s, loss=222, v_num=0, train_loss_step=145.0]#015Epoch 0:  76%|███████▌  | 31/41 [00:05<00:01,  5.37it/s, loss=222, v_num=0, train_loss_step=145.0]\u001b[0m\n",
      "\u001b[34mEpoch 0:  76%|███████▌  | 31/41 [00:05<00:01,  5.37it/s, loss=217, v_num=0, train_loss_step=193.0]\u001b[0m\n",
      "\u001b[34mEpoch 0:  78%|███████▊  | 32/41 [00:05<00:01,  5.38it/s, loss=217, v_num=0, train_loss_step=193.0]\u001b[0m\n",
      "\u001b[34mEpoch 0:  78%|███████▊  | 32/41 [00:05<00:01,  5.38it/s, loss=217, v_num=0, train_loss_step=193.0]\u001b[0m\n",
      "\u001b[34mEpoch 0:  78%|███████▊  | 32/41 [00:05<00:01,  5.38it/s, loss=215, v_num=0, train_loss_step=164.0]\u001b[0m\n",
      "\u001b[34mEpoch 0:  80%|████████  | 33/41 [00:06<00:01,  5.39it/s, loss=215, v_num=0, train_loss_step=164.0]#015Epoch 0:  80%|████████  | 33/41 [00:06<00:01,  5.39it/s, loss=215, v_num=0, train_loss_step=164.0]\u001b[0m\n",
      "\u001b[34mEpoch 0:  80%|████████  | 33/41 [00:06<00:01,  5.39it/s, loss=211, v_num=0, train_loss_step=176.0]\u001b[0m\n",
      "\u001b[34mEpoch 0:  83%|████████▎ | 34/41 [00:06<00:01,  5.40it/s, loss=211, v_num=0, train_loss_step=176.0]#015Epoch 0:  83%|████████▎ | 34/41 [00:06<00:01,  5.40it/s, loss=211, v_num=0, train_loss_step=176.0]\u001b[0m\n",
      "\u001b[34mEpoch 0:  83%|████████▎ | 34/41 [00:06<00:01,  5.40it/s, loss=204, v_num=0, train_loss_step=145.0]\u001b[0m\n",
      "\u001b[34mEpoch 0:  85%|████████▌ | 35/41 [00:06<00:01,  5.40it/s, loss=204, v_num=0, train_loss_step=145.0]\u001b[0m\n",
      "\u001b[34mEpoch 0:  85%|████████▌ | 35/41 [00:06<00:01,  5.40it/s, loss=204, v_num=0, train_loss_step=145.0]\u001b[0m\n",
      "\u001b[34mEpoch 0:  85%|████████▌ | 35/41 [00:06<00:01,  5.40it/s, loss=202, v_num=0, train_loss_step=157.0]\u001b[0m\n",
      "\u001b[34mEpoch 0:  88%|████████▊ | 36/41 [00:06<00:00,  5.40it/s, loss=202, v_num=0, train_loss_step=157.0]#015Epoch 0:  88%|████████▊ | 36/41 [00:06<00:00,  5.40it/s, loss=202, v_num=0, train_loss_step=157.0]\u001b[0m\n",
      "\u001b[34mEpoch 0:  88%|████████▊ | 36/41 [00:06<00:00,  5.40it/s, loss=199, v_num=0, train_loss_step=179.0]\u001b[0m\n",
      "\u001b[34mEpoch 0:  90%|█████████ | 37/41 [00:06<00:00,  5.41it/s, loss=199, v_num=0, train_loss_step=179.0]#015Epoch 0:  90%|█████████ | 37/41 [00:06<00:00,  5.41it/s, loss=199, v_num=0, train_loss_step=179.0]\u001b[0m\n",
      "\u001b[34mEpoch 0:  90%|█████████ | 37/41 [00:06<00:00,  5.40it/s, loss=192, v_num=0, train_loss_step=128.0]\u001b[0m\n",
      "\u001b[34mEpoch 0:  93%|█████████▎| 38/41 [00:07<00:00,  5.41it/s, loss=192, v_num=0, train_loss_step=128.0]\u001b[0m\n",
      "\u001b[34mEpoch 0:  93%|█████████▎| 38/41 [00:07<00:00,  5.41it/s, loss=192, v_num=0, train_loss_step=128.0]\u001b[0m\n",
      "\u001b[34mEpoch 0:  93%|█████████▎| 38/41 [00:07<00:00,  5.41it/s, loss=188, v_num=0, train_loss_step=180.0]\u001b[0m\n",
      "\u001b[34mEpoch 0:  95%|█████████▌| 39/41 [00:07<00:00,  5.42it/s, loss=188, v_num=0, train_loss_step=180.0]\u001b[0m\n",
      "\u001b[34mEpoch 0:  95%|█████████▌| 39/41 [00:07<00:00,  5.42it/s, loss=188, v_num=0, train_loss_step=180.0]\u001b[0m\n",
      "\u001b[34mEpoch 0:  95%|█████████▌| 39/41 [00:07<00:00,  5.42it/s, loss=186, v_num=0, train_loss_step=174.0]\u001b[0m\n",
      "\u001b[34mEpoch 0:  98%|█████████▊| 40/41 [00:07<00:00,  5.43it/s, loss=186, v_num=0, train_loss_step=174.0]#015Epoch 0:  98%|█████████▊| 40/41 [00:07<00:00,  5.43it/s, loss=186, v_num=0, train_loss_step=174.0]\u001b[0m\n",
      "\u001b[34mEpoch 0:  98%|█████████▊| 40/41 [00:07<00:00,  5.43it/s, loss=183, v_num=0, train_loss_step=142.0]\u001b[0m\n",
      "\u001b[34mValidation: 0it [00:00, ?it/s]#033[A\u001b[0m\n",
      "\u001b[34mValidation:   0%|          | 0/1 [00:00<?, ?it/s]#033[A\u001b[0m\n",
      "\u001b[34mValidation DataLoader 0:   0%|          | 0/1 [00:00<?, ?it/s]#033[A\u001b[0m\n",
      "\u001b[34mValidation DataLoader 0: 100%|██████████| 1/1 [00:00<00:00,  4.77it/s]#033[A\u001b[0m\n",
      "\u001b[34mValidation DataLoader 0: 100%|██████████| 1/1 [00:00<00:00,  4.77it/s]#033[A\u001b[0m\n",
      "\u001b[34mEpoch 0: 100%|██████████| 41/41 [00:07<00:00,  5.36it/s, loss=183, v_num=0, train_loss_step=142.0]\u001b[0m\n",
      "\u001b[34mEpoch 0: 100%|██████████| 41/41 [00:07<00:00,  5.36it/s, loss=183, v_num=0, train_loss_step=142.0]\u001b[0m\n",
      "\u001b[34mEpoch 0: 100%|██████████| 41/41 [00:08<00:00,  5.00it/s, loss=183, v_num=0, train_loss_step=142.0, val_loss=223.0]\u001b[0m\n",
      "\u001b[34m#033[A\u001b[0m\n",
      "\u001b[34mEpoch 0: 100%|██████████| 41/41 [00:08<00:00,  4.99it/s, loss=183, v_num=0, train_loss_step=142.0, val_loss=223.0, train_loss_epoch=231.0]\u001b[0m\n",
      "\u001b[34mEpoch 0:   0%|          | 0/41 [00:00<?, ?it/s, loss=183, v_num=0, train_loss_step=142.0, val_loss=223.0, train_loss_epoch=231.0]\u001b[0m\n",
      "\u001b[34mEpoch 1:   0%|          | 0/41 [00:00<?, ?it/s, loss=183, v_num=0, train_loss_step=142.0, val_loss=223.0, train_loss_epoch=231.0]\u001b[0m\n",
      "\u001b[34mEpoch 1:   2%|▏         | 1/41 [00:00<00:07,  5.29it/s, loss=183, v_num=0, train_loss_step=142.0, val_loss=223.0, train_loss_epoch=231.0]#015Epoch 1:   2%|▏         | 1/41 [00:00<00:07,  5.29it/s, loss=183, v_num=0, train_loss_step=142.0, val_loss=223.0, train_loss_epoch=231.0]\u001b[0m\n",
      "\u001b[34mEpoch 1:   2%|▏         | 1/41 [00:00<00:07,  5.28it/s, loss=180, v_num=0, train_loss_step=136.0, val_loss=223.0, train_loss_epoch=231.0]\u001b[0m\n",
      "\u001b[34mEpoch 1:   5%|▍         | 2/41 [00:00<00:07,  5.21it/s, loss=180, v_num=0, train_loss_step=136.0, val_loss=223.0, train_loss_epoch=231.0]\u001b[0m\n",
      "\u001b[34mEpoch 1:   5%|▍         | 2/41 [00:00<00:07,  5.21it/s, loss=180, v_num=0, train_loss_step=136.0, val_loss=223.0, train_loss_epoch=231.0]\u001b[0m\n",
      "\u001b[34mEpoch 1:   5%|▍         | 2/41 [00:00<00:07,  5.20it/s, loss=175, v_num=0, train_loss_step=113.0, val_loss=223.0, train_loss_epoch=231.0]\u001b[0m\n",
      "\u001b[34mEpoch 1:   7%|▋         | 3/41 [00:00<00:07,  5.33it/s, loss=175, v_num=0, train_loss_step=113.0, val_loss=223.0, train_loss_epoch=231.0]\u001b[0m\n",
      "\u001b[34mEpoch 1:   7%|▋         | 3/41 [00:00<00:07,  5.33it/s, loss=175, v_num=0, train_loss_step=113.0, val_loss=223.0, train_loss_epoch=231.0]\u001b[0m\n",
      "\u001b[34mEpoch 1:   7%|▋         | 3/41 [00:00<00:07,  5.32it/s, loss=172, v_num=0, train_loss_step=157.0, val_loss=223.0, train_loss_epoch=231.0]\u001b[0m\n",
      "\u001b[34mEpoch 1:  10%|▉         | 4/41 [00:00<00:06,  5.39it/s, loss=172, v_num=0, train_loss_step=157.0, val_loss=223.0, train_loss_epoch=231.0]\u001b[0m\n",
      "\u001b[34mEpoch 1:  10%|▉         | 4/41 [00:00<00:06,  5.39it/s, loss=172, v_num=0, train_loss_step=157.0, val_loss=223.0, train_loss_epoch=231.0]\u001b[0m\n",
      "\u001b[34mEpoch 1:  10%|▉         | 4/41 [00:00<00:06,  5.39it/s, loss=168, v_num=0, train_loss_step=158.0, val_loss=223.0, train_loss_epoch=231.0]\u001b[0m\n",
      "\u001b[34mEpoch 1:  12%|█▏        | 5/41 [00:00<00:06,  5.42it/s, loss=168, v_num=0, train_loss_step=158.0, val_loss=223.0, train_loss_epoch=231.0]\u001b[0m\n",
      "\u001b[34mEpoch 1:  12%|█▏        | 5/41 [00:00<00:06,  5.42it/s, loss=168, v_num=0, train_loss_step=158.0, val_loss=223.0, train_loss_epoch=231.0]\u001b[0m\n",
      "\u001b[34mEpoch 1:  12%|█▏        | 5/41 [00:00<00:06,  5.42it/s, loss=166, v_num=0, train_loss_step=206.0, val_loss=223.0, train_loss_epoch=231.0]\u001b[0m\n",
      "\u001b[34mEpoch 1:  15%|█▍        | 6/41 [00:01<00:06,  5.45it/s, loss=166, v_num=0, train_loss_step=206.0, val_loss=223.0, train_loss_epoch=231.0]#015Epoch 1:  15%|█▍        | 6/41 [00:01<00:06,  5.45it/s, loss=166, v_num=0, train_loss_step=206.0, val_loss=223.0, train_loss_epoch=231.0]\u001b[0m\n",
      "\u001b[34mEpoch 1:  15%|█▍        | 6/41 [00:01<00:06,  5.44it/s, loss=163, v_num=0, train_loss_step=131.0, val_loss=223.0, train_loss_epoch=231.0]\u001b[0m\n",
      "\u001b[34mEpoch 1:  17%|█▋        | 7/41 [00:01<00:06,  5.48it/s, loss=163, v_num=0, train_loss_step=131.0, val_loss=223.0, train_loss_epoch=231.0]\u001b[0m\n",
      "\u001b[34mEpoch 1:  17%|█▋        | 7/41 [00:01<00:06,  5.48it/s, loss=163, v_num=0, train_loss_step=131.0, val_loss=223.0, train_loss_epoch=231.0]\u001b[0m\n",
      "\u001b[34mEpoch 1:  17%|█▋        | 7/41 [00:01<00:06,  5.48it/s, loss=158, v_num=0, train_loss_step=114.0, val_loss=223.0, train_loss_epoch=231.0]\u001b[0m\n",
      "\u001b[34mEpoch 1:  20%|█▉        | 8/41 [00:01<00:06,  5.46it/s, loss=158, v_num=0, train_loss_step=114.0, val_loss=223.0, train_loss_epoch=231.0]\u001b[0m\n",
      "\u001b[34mEpoch 1:  20%|█▉        | 8/41 [00:01<00:06,  5.46it/s, loss=158, v_num=0, train_loss_step=114.0, val_loss=223.0, train_loss_epoch=231.0]\u001b[0m\n",
      "\u001b[34mEpoch 1:  20%|█▉        | 8/41 [00:01<00:06,  5.45it/s, loss=155, v_num=0, train_loss_step=137.0, val_loss=223.0, train_loss_epoch=231.0]\u001b[0m\n",
      "\u001b[34mEpoch 1:  22%|██▏       | 9/41 [00:01<00:05,  5.48it/s, loss=155, v_num=0, train_loss_step=137.0, val_loss=223.0, train_loss_epoch=231.0]#015Epoch 1:  22%|██▏       | 9/41 [00:01<00:05,  5.48it/s, loss=155, v_num=0, train_loss_step=137.0, val_loss=223.0, train_loss_epoch=231.0]\u001b[0m\n",
      "\u001b[34mEpoch 1:  22%|██▏       | 9/41 [00:01<00:05,  5.48it/s, loss=155, v_num=0, train_loss_step=164.0, val_loss=223.0, train_loss_epoch=231.0]\u001b[0m\n",
      "\u001b[34mEpoch 1:  24%|██▍       | 10/41 [00:01<00:05,  5.49it/s, loss=155, v_num=0, train_loss_step=164.0, val_loss=223.0, train_loss_epoch=231.0]#015Epoch 1:  24%|██▍       | 10/41 [00:01<00:05,  5.49it/s, loss=155, v_num=0, train_loss_step=164.0, val_loss=223.0, train_loss_epoch=231.0]\u001b[0m\n",
      "\u001b[34mEpoch 1:  24%|██▍       | 10/41 [00:01<00:05,  5.49it/s, loss=154, v_num=0, train_loss_step=125.0, val_loss=223.0, train_loss_epoch=231.0]\u001b[0m\n",
      "\u001b[34mEpoch 1:  27%|██▋       | 11/41 [00:01<00:05,  5.51it/s, loss=154, v_num=0, train_loss_step=125.0, val_loss=223.0, train_loss_epoch=231.0]\u001b[0m\n",
      "\u001b[34mEpoch 1:  27%|██▋       | 11/41 [00:01<00:05,  5.51it/s, loss=154, v_num=0, train_loss_step=125.0, val_loss=223.0, train_loss_epoch=231.0]\u001b[0m\n",
      "\u001b[34mEpoch 1:  27%|██▋       | 11/41 [00:01<00:05,  5.51it/s, loss=153, v_num=0, train_loss_step=172.0, val_loss=223.0, train_loss_epoch=231.0]\u001b[0m\n",
      "\u001b[34mEpoch 1:  29%|██▉       | 12/41 [00:02<00:05,  5.52it/s, loss=153, v_num=0, train_loss_step=172.0, val_loss=223.0, train_loss_epoch=231.0]#015Epoch 1:  29%|██▉       | 12/41 [00:02<00:05,  5.52it/s, loss=153, v_num=0, train_loss_step=172.0, val_loss=223.0, train_loss_epoch=231.0]\u001b[0m\n",
      "\u001b[34mEpoch 1:  29%|██▉       | 12/41 [00:02<00:05,  5.52it/s, loss=154, v_num=0, train_loss_step=182.0, val_loss=223.0, train_loss_epoch=231.0]\u001b[0m\n",
      "\u001b[34mEpoch 1:  32%|███▏      | 13/41 [00:02<00:05,  5.53it/s, loss=154, v_num=0, train_loss_step=182.0, val_loss=223.0, train_loss_epoch=231.0]#015Epoch 1:  32%|███▏      | 13/41 [00:02<00:05,  5.52it/s, loss=154, v_num=0, train_loss_step=182.0, val_loss=223.0, train_loss_epoch=231.0]\u001b[0m\n",
      "\u001b[34mEpoch 1:  32%|███▏      | 13/41 [00:02<00:05,  5.52it/s, loss=154, v_num=0, train_loss_step=170.0, val_loss=223.0, train_loss_epoch=231.0]\u001b[0m\n",
      "\u001b[34mEpoch 1:  34%|███▍      | 14/41 [00:02<00:04,  5.52it/s, loss=154, v_num=0, train_loss_step=170.0, val_loss=223.0, train_loss_epoch=231.0]\u001b[0m\n",
      "\u001b[34mEpoch 1:  34%|███▍      | 14/41 [00:02<00:04,  5.52it/s, loss=154, v_num=0, train_loss_step=170.0, val_loss=223.0, train_loss_epoch=231.0]\u001b[0m\n",
      "\u001b[34mEpoch 1:  34%|███▍      | 14/41 [00:02<00:04,  5.51it/s, loss=153, v_num=0, train_loss_step=135.0, val_loss=223.0, train_loss_epoch=231.0]\u001b[0m\n",
      "\u001b[34mEpoch 1:  37%|███▋      | 15/41 [00:02<00:04,  5.52it/s, loss=153, v_num=0, train_loss_step=135.0, val_loss=223.0, train_loss_epoch=231.0]#015Epoch 1:  37%|███▋      | 15/41 [00:02<00:04,  5.52it/s, loss=153, v_num=0, train_loss_step=135.0, val_loss=223.0, train_loss_epoch=231.0]\u001b[0m\n",
      "\u001b[34mEpoch 1:  37%|███▋      | 15/41 [00:02<00:04,  5.52it/s, loss=151, v_num=0, train_loss_step=109.0, val_loss=223.0, train_loss_epoch=231.0]\u001b[0m\n",
      "\u001b[34mEpoch 1:  39%|███▉      | 16/41 [00:02<00:04,  5.53it/s, loss=151, v_num=0, train_loss_step=109.0, val_loss=223.0, train_loss_epoch=231.0]#015Epoch 1:  39%|███▉      | 16/41 [00:02<00:04,  5.53it/s, loss=151, v_num=0, train_loss_step=109.0, val_loss=223.0, train_loss_epoch=231.0]\u001b[0m\n",
      "\u001b[34mEpoch 1:  39%|███▉      | 16/41 [00:02<00:04,  5.53it/s, loss=150, v_num=0, train_loss_step=161.0, val_loss=223.0, train_loss_epoch=231.0]\u001b[0m\n",
      "\u001b[34mEpoch 1:  41%|████▏     | 17/41 [00:03<00:04,  5.54it/s, loss=150, v_num=0, train_loss_step=161.0, val_loss=223.0, train_loss_epoch=231.0]#015Epoch 1:  41%|████▏     | 17/41 [00:03<00:04,  5.54it/s, loss=150, v_num=0, train_loss_step=161.0, val_loss=223.0, train_loss_epoch=231.0]\u001b[0m\n",
      "\u001b[34mEpoch 1:  41%|████▏     | 17/41 [00:03<00:04,  5.54it/s, loss=151, v_num=0, train_loss_step=149.0, val_loss=223.0, train_loss_epoch=231.0]\u001b[0m\n",
      "\u001b[34mEpoch 1:  44%|████▍     | 18/41 [00:03<00:04,  5.55it/s, loss=151, v_num=0, train_loss_step=149.0, val_loss=223.0, train_loss_epoch=231.0]#015Epoch 1:  44%|████▍     | 18/41 [00:03<00:04,  5.55it/s, loss=151, v_num=0, train_loss_step=149.0, val_loss=223.0, train_loss_epoch=231.0]\u001b[0m\n",
      "\u001b[34mEpoch 1:  44%|████▍     | 18/41 [00:03<00:04,  5.55it/s, loss=149, v_num=0, train_loss_step=142.0, val_loss=223.0, train_loss_epoch=231.0]\u001b[0m\n",
      "\u001b[34mEpoch 1:  46%|████▋     | 19/41 [00:03<00:03,  5.55it/s, loss=149, v_num=0, train_loss_step=142.0, val_loss=223.0, train_loss_epoch=231.0]#015Epoch 1:  46%|████▋     | 19/41 [00:03<00:03,  5.55it/s, loss=149, v_num=0, train_loss_step=142.0, val_loss=223.0, train_loss_epoch=231.0]\u001b[0m\n",
      "\u001b[34mEpoch 1:  46%|████▋     | 19/41 [00:03<00:03,  5.55it/s, loss=144, v_num=0, train_loss_step=84.80, val_loss=223.0, train_loss_epoch=231.0]\u001b[0m\n",
      "\u001b[34mEpoch 1:  49%|████▉     | 20/41 [00:03<00:03,  5.54it/s, loss=144, v_num=0, train_loss_step=84.80, val_loss=223.0, train_loss_epoch=231.0]#015Epoch 1:  49%|████▉     | 20/41 [00:03<00:03,  5.54it/s, loss=144, v_num=0, train_loss_step=84.80, val_loss=223.0, train_loss_epoch=231.0]\u001b[0m\n",
      "\u001b[34mEpoch 1:  49%|████▉     | 20/41 [00:03<00:03,  5.54it/s, loss=143, v_num=0, train_loss_step=115.0, val_loss=223.0, train_loss_epoch=231.0]\u001b[0m\n",
      "\u001b[34mEpoch 1:  51%|█████     | 21/41 [00:03<00:03,  5.55it/s, loss=143, v_num=0, train_loss_step=115.0, val_loss=223.0, train_loss_epoch=231.0]#015Epoch 1:  51%|█████     | 21/41 [00:03<00:03,  5.55it/s, loss=143, v_num=0, train_loss_step=115.0, val_loss=223.0, train_loss_epoch=231.0]\u001b[0m\n",
      "\u001b[34mEpoch 1:  51%|█████     | 21/41 [00:03<00:03,  5.55it/s, loss=142, v_num=0, train_loss_step=117.0, val_loss=223.0, train_loss_epoch=231.0]\u001b[0m\n",
      "\u001b[34mEpoch 1:  54%|█████▎    | 22/41 [00:03<00:03,  5.56it/s, loss=142, v_num=0, train_loss_step=117.0, val_loss=223.0, train_loss_epoch=231.0]\u001b[0m\n",
      "\u001b[34mEpoch 1:  54%|█████▎    | 22/41 [00:03<00:03,  5.56it/s, loss=142, v_num=0, train_loss_step=117.0, val_loss=223.0, train_loss_epoch=231.0]\u001b[0m\n",
      "\u001b[34mEpoch 1:  54%|█████▎    | 22/41 [00:03<00:03,  5.56it/s, loss=144, v_num=0, train_loss_step=160.0, val_loss=223.0, train_loss_epoch=231.0]\u001b[0m\n",
      "\u001b[34mEpoch 1:  56%|█████▌    | 23/41 [00:04<00:03,  5.56it/s, loss=144, v_num=0, train_loss_step=160.0, val_loss=223.0, train_loss_epoch=231.0]#015Epoch 1:  56%|█████▌    | 23/41 [00:04<00:03,  5.56it/s, loss=144, v_num=0, train_loss_step=160.0, val_loss=223.0, train_loss_epoch=231.0]\u001b[0m\n",
      "\u001b[34mEpoch 1:  56%|█████▌    | 23/41 [00:04<00:03,  5.56it/s, loss=144, v_num=0, train_loss_step=138.0, val_loss=223.0, train_loss_epoch=231.0]\u001b[0m\n",
      "\u001b[34mEpoch 1:  59%|█████▊    | 24/41 [00:04<00:03,  5.57it/s, loss=144, v_num=0, train_loss_step=138.0, val_loss=223.0, train_loss_epoch=231.0]#015Epoch 1:  59%|█████▊    | 24/41 [00:04<00:03,  5.57it/s, loss=144, v_num=0, train_loss_step=138.0, val_loss=223.0, train_loss_epoch=231.0]\u001b[0m\n",
      "\u001b[34mEpoch 1:  59%|█████▊    | 24/41 [00:04<00:03,  5.57it/s, loss=142, v_num=0, train_loss_step=130.0, val_loss=223.0, train_loss_epoch=231.0]\u001b[0m\n",
      "\u001b[34mEpoch 1:  61%|██████    | 25/41 [00:04<00:02,  5.57it/s, loss=142, v_num=0, train_loss_step=130.0, val_loss=223.0, train_loss_epoch=231.0]#015Epoch 1:  61%|██████    | 25/41 [00:04<00:02,  5.57it/s, loss=142, v_num=0, train_loss_step=130.0, val_loss=223.0, train_loss_epoch=231.0]\u001b[0m\n",
      "\u001b[34mEpoch 1:  61%|██████    | 25/41 [00:04<00:02,  5.57it/s, loss=143, v_num=0, train_loss_step=220.0, val_loss=223.0, train_loss_epoch=231.0]\u001b[0m\n",
      "\u001b[34mEpoch 1:  63%|██████▎   | 26/41 [00:04<00:02,  5.57it/s, loss=143, v_num=0, train_loss_step=220.0, val_loss=223.0, train_loss_epoch=231.0]#015Epoch 1:  63%|██████▎   | 26/41 [00:04<00:02,  5.57it/s, loss=143, v_num=0, train_loss_step=220.0, val_loss=223.0, train_loss_epoch=231.0]\u001b[0m\n",
      "\u001b[34mEpoch 1:  63%|██████▎   | 26/41 [00:04<00:02,  5.57it/s, loss=143, v_num=0, train_loss_step=140.0, val_loss=223.0, train_loss_epoch=231.0]\u001b[0m\n",
      "\u001b[34mEpoch 1:  66%|██████▌   | 27/41 [00:04<00:02,  5.57it/s, loss=143, v_num=0, train_loss_step=140.0, val_loss=223.0, train_loss_epoch=231.0]\u001b[0m\n",
      "\u001b[34mEpoch 1:  66%|██████▌   | 27/41 [00:04<00:02,  5.57it/s, loss=143, v_num=0, train_loss_step=140.0, val_loss=223.0, train_loss_epoch=231.0]\u001b[0m\n",
      "\u001b[34mEpoch 1:  66%|██████▌   | 27/41 [00:04<00:02,  5.57it/s, loss=144, v_num=0, train_loss_step=131.0, val_loss=223.0, train_loss_epoch=231.0]\u001b[0m\n",
      "\u001b[34mEpoch 1:  68%|██████▊   | 28/41 [00:05<00:02,  5.58it/s, loss=144, v_num=0, train_loss_step=131.0, val_loss=223.0, train_loss_epoch=231.0]#015Epoch 1:  68%|██████▊   | 28/41 [00:05<00:02,  5.58it/s, loss=144, v_num=0, train_loss_step=131.0, val_loss=223.0, train_loss_epoch=231.0]\u001b[0m\n",
      "\u001b[34mEpoch 1:  68%|██████▊   | 28/41 [00:05<00:02,  5.58it/s, loss=144, v_num=0, train_loss_step=139.0, val_loss=223.0, train_loss_epoch=231.0]\u001b[0m\n",
      "\u001b[34mEpoch 1:  71%|███████   | 29/41 [00:05<00:02,  5.58it/s, loss=144, v_num=0, train_loss_step=139.0, val_loss=223.0, train_loss_epoch=231.0]#015Epoch 1:  71%|███████   | 29/41 [00:05<00:02,  5.58it/s, loss=144, v_num=0, train_loss_step=139.0, val_loss=223.0, train_loss_epoch=231.0]\u001b[0m\n",
      "\u001b[34mEpoch 1:  71%|███████   | 29/41 [00:05<00:02,  5.58it/s, loss=145, v_num=0, train_loss_step=172.0, val_loss=223.0, train_loss_epoch=231.0]\u001b[0m\n",
      "\u001b[34mEpoch 1:  73%|███████▎  | 30/41 [00:05<00:01,  5.59it/s, loss=145, v_num=0, train_loss_step=172.0, val_loss=223.0, train_loss_epoch=231.0]#015Epoch 1:  73%|███████▎  | 30/41 [00:05<00:01,  5.59it/s, loss=145, v_num=0, train_loss_step=172.0, val_loss=223.0, train_loss_epoch=231.0]\u001b[0m\n",
      "\u001b[34mEpoch 1:  73%|███████▎  | 30/41 [00:05<00:01,  5.59it/s, loss=144, v_num=0, train_loss_step=106.0, val_loss=223.0, train_loss_epoch=231.0]\u001b[0m\n",
      "\u001b[34mEpoch 1:  76%|███████▌  | 31/41 [00:05<00:01,  5.59it/s, loss=144, v_num=0, train_loss_step=106.0, val_loss=223.0, train_loss_epoch=231.0]\u001b[0m\n",
      "\u001b[34mEpoch 1:  76%|███████▌  | 31/41 [00:05<00:01,  5.59it/s, loss=144, v_num=0, train_loss_step=106.0, val_loss=223.0, train_loss_epoch=231.0]\u001b[0m\n",
      "\u001b[34mEpoch 1:  76%|███████▌  | 31/41 [00:05<00:01,  5.59it/s, loss=140, v_num=0, train_loss_step=102.0, val_loss=223.0, train_loss_epoch=231.0]\u001b[0m\n",
      "\u001b[34mEpoch 1:  78%|███████▊  | 32/41 [00:05<00:01,  5.58it/s, loss=140, v_num=0, train_loss_step=102.0, val_loss=223.0, train_loss_epoch=231.0]#015Epoch 1:  78%|███████▊  | 32/41 [00:05<00:01,  5.58it/s, loss=140, v_num=0, train_loss_step=102.0, val_loss=223.0, train_loss_epoch=231.0]\u001b[0m\n",
      "\u001b[34mEpoch 1:  78%|███████▊  | 32/41 [00:05<00:01,  5.58it/s, loss=137, v_num=0, train_loss_step=120.0, val_loss=223.0, train_loss_epoch=231.0]\u001b[0m\n",
      "\u001b[34mEpoch 1:  80%|████████  | 33/41 [00:05<00:01,  5.59it/s, loss=137, v_num=0, train_loss_step=120.0, val_loss=223.0, train_loss_epoch=231.0]\u001b[0m\n",
      "\u001b[34mEpoch 1:  80%|████████  | 33/41 [00:05<00:01,  5.59it/s, loss=137, v_num=0, train_loss_step=120.0, val_loss=223.0, train_loss_epoch=231.0]\u001b[0m\n",
      "\u001b[34mEpoch 1:  80%|████████  | 33/41 [00:05<00:01,  5.59it/s, loss=136, v_num=0, train_loss_step=152.0, val_loss=223.0, train_loss_epoch=231.0]\u001b[0m\n",
      "\u001b[34mEpoch 1:  83%|████████▎ | 34/41 [00:06<00:01,  5.59it/s, loss=136, v_num=0, train_loss_step=152.0, val_loss=223.0, train_loss_epoch=231.0]\u001b[0m\n",
      "\u001b[34mEpoch 1:  83%|████████▎ | 34/41 [00:06<00:01,  5.59it/s, loss=136, v_num=0, train_loss_step=152.0, val_loss=223.0, train_loss_epoch=231.0]\u001b[0m\n",
      "\u001b[34mEpoch 1:  83%|████████▎ | 34/41 [00:06<00:01,  5.59it/s, loss=136, v_num=0, train_loss_step=132.0, val_loss=223.0, train_loss_epoch=231.0]\u001b[0m\n",
      "\u001b[34mEpoch 1:  85%|████████▌ | 35/41 [00:06<00:01,  5.60it/s, loss=136, v_num=0, train_loss_step=132.0, val_loss=223.0, train_loss_epoch=231.0]\u001b[0m\n",
      "\u001b[34mEpoch 1:  85%|████████▌ | 35/41 [00:06<00:01,  5.60it/s, loss=136, v_num=0, train_loss_step=132.0, val_loss=223.0, train_loss_epoch=231.0]\u001b[0m\n",
      "\u001b[34mEpoch 1:  85%|████████▌ | 35/41 [00:06<00:01,  5.60it/s, loss=140, v_num=0, train_loss_step=187.0, val_loss=223.0, train_loss_epoch=231.0]\u001b[0m\n",
      "\u001b[34mEpoch 1:  88%|████████▊ | 36/41 [00:06<00:00,  5.60it/s, loss=140, v_num=0, train_loss_step=187.0, val_loss=223.0, train_loss_epoch=231.0]#015Epoch 1:  88%|████████▊ | 36/41 [00:06<00:00,  5.60it/s, loss=140, v_num=0, train_loss_step=187.0, val_loss=223.0, train_loss_epoch=231.0]\u001b[0m\n",
      "\u001b[34mEpoch 1:  88%|████████▊ | 36/41 [00:06<00:00,  5.60it/s, loss=140, v_num=0, train_loss_step=155.0, val_loss=223.0, train_loss_epoch=231.0]\u001b[0m\n",
      "\u001b[34mEpoch 1:  90%|█████████ | 37/41 [00:06<00:00,  5.61it/s, loss=140, v_num=0, train_loss_step=155.0, val_loss=223.0, train_loss_epoch=231.0]\u001b[0m\n",
      "\u001b[34mEpoch 1:  90%|█████████ | 37/41 [00:06<00:00,  5.61it/s, loss=140, v_num=0, train_loss_step=155.0, val_loss=223.0, train_loss_epoch=231.0]\u001b[0m\n",
      "\u001b[34mEpoch 1:  90%|█████████ | 37/41 [00:06<00:00,  5.61it/s, loss=139, v_num=0, train_loss_step=132.0, val_loss=223.0, train_loss_epoch=231.0]\u001b[0m\n",
      "\u001b[34mEpoch 1:  93%|█████████▎| 38/41 [00:06<00:00,  5.60it/s, loss=139, v_num=0, train_loss_step=132.0, val_loss=223.0, train_loss_epoch=231.0]\u001b[0m\n",
      "\u001b[34mEpoch 1:  93%|█████████▎| 38/41 [00:06<00:00,  5.60it/s, loss=139, v_num=0, train_loss_step=132.0, val_loss=223.0, train_loss_epoch=231.0]\u001b[0m\n",
      "\u001b[34mEpoch 1:  93%|█████████▎| 38/41 [00:06<00:00,  5.60it/s, loss=139, v_num=0, train_loss_step=154.0, val_loss=223.0, train_loss_epoch=231.0]\u001b[0m\n",
      "\u001b[34mEpoch 1:  95%|█████████▌| 39/41 [00:06<00:00,  5.61it/s, loss=139, v_num=0, train_loss_step=154.0, val_loss=223.0, train_loss_epoch=231.0]\u001b[0m\n",
      "\u001b[34mEpoch 1:  95%|█████████▌| 39/41 [00:06<00:00,  5.61it/s, loss=139, v_num=0, train_loss_step=154.0, val_loss=223.0, train_loss_epoch=231.0]\u001b[0m\n",
      "\u001b[34mEpoch 1:  95%|█████████▌| 39/41 [00:06<00:00,  5.61it/s, loss=144, v_num=0, train_loss_step=177.0, val_loss=223.0, train_loss_epoch=231.0]\u001b[0m\n",
      "\u001b[34mEpoch 1:  98%|█████████▊| 40/41 [00:07<00:00,  5.61it/s, loss=144, v_num=0, train_loss_step=177.0, val_loss=223.0, train_loss_epoch=231.0]\u001b[0m\n",
      "\u001b[34mEpoch 1:  98%|█████████▊| 40/41 [00:07<00:00,  5.61it/s, loss=144, v_num=0, train_loss_step=177.0, val_loss=223.0, train_loss_epoch=231.0]\u001b[0m\n",
      "\u001b[34mEpoch 1:  98%|█████████▊| 40/41 [00:07<00:00,  5.61it/s, loss=143, v_num=0, train_loss_step=98.70, val_loss=223.0, train_loss_epoch=231.0]\u001b[0m\n",
      "\u001b[34mValidation: 0it [00:00, ?it/s]#033[A\u001b[0m\n",
      "\u001b[34mValidation:   0%|          | 0/1 [00:00<?, ?it/s]#033[A\u001b[0m\n",
      "\u001b[34mValidation DataLoader 0:   0%|          | 0/1 [00:00<?, ?it/s]#033[A\u001b[0m\n",
      "\u001b[34mValidation DataLoader 0: 100%|██████████| 1/1 [00:00<00:00,  4.66it/s]#033[A\u001b[0m\n",
      "\u001b[34mValidation DataLoader 0: 100%|██████████| 1/1 [00:00<00:00,  4.66it/s]#033[A\u001b[0m\n",
      "\u001b[34mEpoch 1: 100%|██████████| 41/41 [00:07<00:00,  5.54it/s, loss=143, v_num=0, train_loss_step=98.70, val_loss=223.0, train_loss_epoch=231.0]\u001b[0m\n",
      "\u001b[34mEpoch 1: 100%|██████████| 41/41 [00:07<00:00,  5.54it/s, loss=143, v_num=0, train_loss_step=98.70, val_loss=223.0, train_loss_epoch=231.0]\u001b[0m\n",
      "\u001b[34mEpoch 1: 100%|██████████| 41/41 [00:08<00:00,  5.06it/s, loss=143, v_num=0, train_loss_step=98.70, val_loss=192.0, train_loss_epoch=231.0]\u001b[0m\n",
      "\u001b[34m#015                                                                      #033[A\u001b[0m\n",
      "\u001b[34mEpoch 1: 100%|██████████| 41/41 [00:08<00:00,  5.06it/s, loss=143, v_num=0, train_loss_step=98.70, val_loss=192.0, train_loss_epoch=145.0]\u001b[0m\n",
      "\u001b[34mEpoch 1: 100%|██████████| 41/41 [00:08<00:00,  5.00it/s, loss=143, v_num=0, train_loss_step=98.70, val_loss=192.0, train_loss_epoch=145.0]\u001b[0m\n",
      "\u001b[34m2023-03-07 06:39:19,811 sagemaker-training-toolkit INFO     Waiting for the process to finish and give a return code.\u001b[0m\n",
      "\u001b[34m2023-03-07 06:39:19,811 sagemaker-training-toolkit INFO     Done waiting for a return code. Received 0 from exiting process.\u001b[0m\n",
      "\u001b[34m2023-03-07 06:39:19,811 sagemaker-training-toolkit INFO     Reporting training SUCCESS\u001b[0m\n",
      "\n",
      "2023-03-07 06:39:34 Uploading - Uploading generated training model\n",
      "2023-03-07 06:39:34 Completed - Training job completed\n",
      "Training seconds: 283\n",
      "Billable seconds: 283\n"
     ]
    }
   ],
   "source": [
    "estimator.logs()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "deb0cd08-bf35-4fb4-96cf-b89ae2f8f421",
   "metadata": {},
   "source": [
    "# 4. 모델 가중치 파일 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b6f718a0-c680-4570-8113-1a3c27521d27",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model artifact: \n",
      " s3://sagemaker-us-east-1-057716757052/pytorch-training-2023-03-07-06-33-33-202/output/model.tar.gz\n"
     ]
    }
   ],
   "source": [
    "print(\"model artifact: \\n\", estimator.model_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3564849c-9754-4a10-b417-86ec80a74d0f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
