{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4b27d946-a068-458a-9a19-a4e1f4abfcf0",
   "metadata": {
    "tags": []
   },
   "source": [
    "# [모듈 2.1] 세이지메이커에서 분산 훈련 하기\n",
    "\n",
    "이 노트북은 커널을 'conda_python3' 를 사용합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7444f2fd-db36-4489-a066-8bca587dc171",
   "metadata": {},
   "source": [
    "# 1. 환경 설정\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27b9bd08-1fe9-4306-ae4b-8cd0a99ac564",
   "metadata": {},
   "source": [
    "## 기본 세팅\n",
    "사용하는 패키지는 import 시점에 다시 재로딩 합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e5e68172-0ed2-448b-ad99-0659323e4848",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import sys\n",
    "sys.path.append('./scripts')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "772a23fa-eee6-4b29-945e-751105551487",
   "metadata": {},
   "source": [
    "## 파라미터 세팅"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f41139d8-0a16-4a4b-874e-895745acd082",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_gpus:  1\n",
      "epochs:  2\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import os\n",
    "\n",
    "epochs = 1\n",
    "num_gpus = torch.cuda.device_count()\n",
    "# model_dir = 'model'\n",
    "# num_gpus = 4\n",
    "# train_notebook = True\n",
    "\n",
    "print(\"num_gpus: \", num_gpus)\n",
    "print(\"epochs: \", epochs)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a2c0eafa-d780-44aa-89da-6934f675e993",
   "metadata": {},
   "outputs": [],
   "source": [
    "# if train_notebook:\n",
    "\n",
    "\n",
    "#     os.makedirs(model_dir, exist_ok=True)\n",
    "        \n",
    "#     src_dir = os.getcwd()\n",
    "#     os.environ['SM_MODEL_DIR'] = f'{src_dir}/{model_dir}'\n",
    "#     os.environ['SM_NUM_GPUS'] = str(num_gpus)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a28bcda9-79ef-4f79-9721-ee13e2618439",
   "metadata": {},
   "source": [
    "# 2. 세이지 메이크 로컬 모드 훈련\n",
    "#### 로컬의 GPU, CPU 여부로 instance_type 결정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "07a8e179-f1a7-40b3-a275-fcba9bf11ede",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sat Mar  4 09:17:34 2023       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 510.47.03    Driver Version: 510.47.03    CUDA Version: 11.6     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  Tesla V100-SXM2...  On   | 00000000:00:1E.0 Off |                    0 |\n",
      "| N/A   37C    P0    36W / 300W |   1251MiB / 16384MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "|    0   N/A  N/A      4081      C   ...3/envs/python3/bin/python     1249MiB |\n",
      "+-----------------------------------------------------------------------------+\n",
      "Instance type = local_gpu\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import subprocess\n",
    "\n",
    "\n",
    "try:\n",
    "    if subprocess.call(\"nvidia-smi\") == 0:\n",
    "        ## Set type to GPU if one is present\n",
    "        instance_type = \"local_gpu\"\n",
    "    else:\n",
    "        instance_type = \"local\"        \n",
    "except:\n",
    "    pass\n",
    "\n",
    "print(\"Instance type = \" + instance_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "97204ea5-68c0-4f1e-8dec-0318772d38a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sagemaker\n",
    "\n",
    "sagemaker.__version__\n",
    "\n",
    "# sagemaker_session = sagemaker.Session()\n",
    "role = sagemaker.get_execution_role()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e25ed9c-3f2a-40b1-ae9b-6ec7b8b8687f",
   "metadata": {},
   "source": [
    "## 2.4. 로컬 모드로 훈련 실행\n",
    "- 아래의 두 라인이 로컬모드로 훈련을 지시 합니다.\n",
    "```python\n",
    "    instance_type=instance_type, # local_gpu or local 지정\n",
    "    session = sagemaker.LocalSession(), # 로컬 세션을 사용합니다.\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bdb1893b-20a3-4867-bdac-eec5206e1867",
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperparameters = {'epochs': epochs, \n",
    "                   'n_gpus': num_gpus,\n",
    "                    }  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2e76fe8b-e1ed-4438-8d2d-3a662f37c558",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating cucd7st2m9-algo-1-w67e8 ... \n",
      "Creating cucd7st2m9-algo-1-w67e8 ... done\n",
      "Attaching to cucd7st2m9-algo-1-w67e8\n",
      "\u001b[36mcucd7st2m9-algo-1-w67e8 |\u001b[0m 2023-03-04 09:17:38,627 sagemaker-training-toolkit INFO     Imported framework sagemaker_pytorch_container.training\n",
      "\u001b[36mcucd7st2m9-algo-1-w67e8 |\u001b[0m 2023-03-04 09:17:38,650 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\n",
      "\u001b[36mcucd7st2m9-algo-1-w67e8 |\u001b[0m 2023-03-04 09:17:38,660 sagemaker-training-toolkit INFO     instance_groups entry not present in resource_config\n",
      "\u001b[36mcucd7st2m9-algo-1-w67e8 |\u001b[0m 2023-03-04 09:17:38,664 sagemaker_pytorch_container.training INFO     Block until all host DNS lookups succeed.\n",
      "\u001b[36mcucd7st2m9-algo-1-w67e8 |\u001b[0m 2023-03-04 09:17:38,667 sagemaker_pytorch_container.training INFO     Invoking user training script.\n",
      "\u001b[36mcucd7st2m9-algo-1-w67e8 |\u001b[0m 2023-03-04 09:17:38,709 botocore.credentials INFO     Found credentials from IAM Role: BaseNotebookInstanceEc2InstanceRole\n",
      "\u001b[36mcucd7st2m9-algo-1-w67e8 |\u001b[0m 2023-03-04 09:17:38,897 sagemaker-training-toolkit INFO     Installing dependencies from requirements.txt:\n",
      "\u001b[36mcucd7st2m9-algo-1-w67e8 |\u001b[0m /opt/conda/bin/python3.8 -m pip install -r requirements.txt\n",
      "\u001b[36mcucd7st2m9-algo-1-w67e8 |\u001b[0m Collecting pytorch-forecasting==0.10.3\n",
      "\u001b[36mcucd7st2m9-algo-1-w67e8 |\u001b[0m Downloading pytorch_forecasting-0.10.3-py3-none-any.whl (141 kB)\n",
      "\u001b[36mcucd7st2m9-algo-1-w67e8 |\u001b[0m ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 141.4/141.4 kB 13.9 MB/s eta 0:00:00\n",
      "\u001b[36mcucd7st2m9-algo-1-w67e8 |\u001b[0m \n",
      "\u001b[36mcucd7st2m9-algo-1-w67e8 |\u001b[0m Collecting pytorch_lightning==1.9.0\n",
      "\u001b[36mcucd7st2m9-algo-1-w67e8 |\u001b[0m Downloading pytorch_lightning-1.9.0-py3-none-any.whl (825 kB)\n",
      "\u001b[36mcucd7st2m9-algo-1-w67e8 |\u001b[0m ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 825.8/825.8 kB 54.6 MB/s eta 0:00:00\n",
      "\u001b[36mcucd7st2m9-algo-1-w67e8 |\u001b[0m \n",
      "\u001b[36mcucd7st2m9-algo-1-w67e8 |\u001b[0m Collecting pyarrow==11.0.0\n",
      "\u001b[36mcucd7st2m9-algo-1-w67e8 |\u001b[0m Downloading pyarrow-11.0.0-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (35.0 MB)\n",
      "\u001b[36mcucd7st2m9-algo-1-w67e8 |\u001b[0m ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 35.0/35.0 MB 33.4 MB/s eta 0:00:00\n",
      "\u001b[36mcucd7st2m9-algo-1-w67e8 |\u001b[0m \n",
      "\u001b[36mcucd7st2m9-algo-1-w67e8 |\u001b[0m Collecting tensorboard==2.12.0\n",
      "\u001b[36mcucd7st2m9-algo-1-w67e8 |\u001b[0m Downloading tensorboard-2.12.0-py3-none-any.whl (5.6 MB)\n",
      "\u001b[36mcucd7st2m9-algo-1-w67e8 |\u001b[0m ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 5.6/5.6 MB 69.0 MB/s eta 0:00:00\n",
      "\u001b[36mcucd7st2m9-algo-1-w67e8 |\u001b[0m \n",
      "\u001b[36mcucd7st2m9-algo-1-w67e8 |\u001b[0m Requirement already satisfied: torch<2.0,>=1.7 in /opt/conda/lib/python3.8/site-packages (from pytorch-forecasting==0.10.3->-r requirements.txt (line 2)) (1.12.1+cu113)\n",
      "\u001b[36mcucd7st2m9-algo-1-w67e8 |\u001b[0m Requirement already satisfied: pandas<2.0.0,>=1.3.0 in /opt/conda/lib/python3.8/site-packages (from pytorch-forecasting==0.10.3->-r requirements.txt (line 2)) (1.5.2)\n",
      "\u001b[36mcucd7st2m9-algo-1-w67e8 |\u001b[0m Requirement already satisfied: scipy<2.0,>=1.8 in /opt/conda/lib/python3.8/site-packages (from pytorch-forecasting==0.10.3->-r requirements.txt (line 2)) (1.10.0)\n",
      "\u001b[36mcucd7st2m9-algo-1-w67e8 |\u001b[0m Collecting scikit-learn<1.2,>=0.24\n",
      "\u001b[36mcucd7st2m9-algo-1-w67e8 |\u001b[0m Downloading scikit_learn-1.1.3-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (31.2 MB)\n",
      "\u001b[36mcucd7st2m9-algo-1-w67e8 |\u001b[0m ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 31.2/31.2 MB 40.6 MB/s eta 0:00:00\n",
      "\u001b[36mcucd7st2m9-algo-1-w67e8 |\u001b[0m \n",
      "\u001b[36mcucd7st2m9-algo-1-w67e8 |\u001b[0m Collecting optuna<3.0.0,>=2.3.0\n",
      "\u001b[36mcucd7st2m9-algo-1-w67e8 |\u001b[0m Downloading optuna-2.10.1-py3-none-any.whl (308 kB)\n",
      "\u001b[36mcucd7st2m9-algo-1-w67e8 |\u001b[0m ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 308.2/308.2 kB 37.8 MB/s eta 0:00:00\n",
      "\u001b[36mcucd7st2m9-algo-1-w67e8 |\u001b[0m \n",
      "\u001b[36mcucd7st2m9-algo-1-w67e8 |\u001b[0m Collecting statsmodels\n",
      "\u001b[36mcucd7st2m9-algo-1-w67e8 |\u001b[0m Downloading statsmodels-0.13.5-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (9.9 MB)\n",
      "\u001b[36mcucd7st2m9-algo-1-w67e8 |\u001b[0m ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 9.9/9.9 MB 47.9 MB/s eta 0:00:00\n",
      "\u001b[36mcucd7st2m9-algo-1-w67e8 |\u001b[0m \n",
      "\u001b[36mcucd7st2m9-algo-1-w67e8 |\u001b[0m Requirement already satisfied: matplotlib in /opt/conda/lib/python3.8/site-packages (from pytorch-forecasting==0.10.3->-r requirements.txt (line 2)) (3.6.2)\n",
      "\u001b[36mcucd7st2m9-algo-1-w67e8 |\u001b[0m Collecting torchmetrics>=0.7.0\n",
      "\u001b[36mcucd7st2m9-algo-1-w67e8 |\u001b[0m Downloading torchmetrics-0.11.3-py3-none-any.whl (518 kB)\n",
      "\u001b[36mcucd7st2m9-algo-1-w67e8 |\u001b[0m ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 518.6/518.6 kB 46.0 MB/s eta 0:00:00\n",
      "\u001b[36mcucd7st2m9-algo-1-w67e8 |\u001b[0m \n",
      "\u001b[36mcucd7st2m9-algo-1-w67e8 |\u001b[0m Collecting lightning-utilities>=0.4.2\n",
      "\u001b[36mcucd7st2m9-algo-1-w67e8 |\u001b[0m Downloading lightning_utilities-0.7.1-py3-none-any.whl (18 kB)\n",
      "\u001b[36mcucd7st2m9-algo-1-w67e8 |\u001b[0m Requirement already satisfied: typing-extensions>=4.0.0 in /opt/conda/lib/python3.8/site-packages (from pytorch_lightning==1.9.0->-r requirements.txt (line 3)) (4.4.0)\n",
      "\u001b[36mcucd7st2m9-algo-1-w67e8 |\u001b[0m Requirement already satisfied: PyYAML>=5.4 in /opt/conda/lib/python3.8/site-packages (from pytorch_lightning==1.9.0->-r requirements.txt (line 3)) (5.4.1)\n",
      "\u001b[36mcucd7st2m9-algo-1-w67e8 |\u001b[0m Requirement already satisfied: packaging>=17.1 in /opt/conda/lib/python3.8/site-packages (from pytorch_lightning==1.9.0->-r requirements.txt (line 3)) (22.0)\n",
      "\u001b[36mcucd7st2m9-algo-1-w67e8 |\u001b[0m Requirement already satisfied: tqdm>=4.57.0 in /opt/conda/lib/python3.8/site-packages (from pytorch_lightning==1.9.0->-r requirements.txt (line 3)) (4.64.1)\n",
      "\u001b[36mcucd7st2m9-algo-1-w67e8 |\u001b[0m Requirement already satisfied: numpy>=1.17.2 in /opt/conda/lib/python3.8/site-packages (from pytorch_lightning==1.9.0->-r requirements.txt (line 3)) (1.23.5)\n",
      "\u001b[36mcucd7st2m9-algo-1-w67e8 |\u001b[0m Requirement already satisfied: fsspec[http]>2021.06.0 in /opt/conda/lib/python3.8/site-packages (from pytorch_lightning==1.9.0->-r requirements.txt (line 3)) (2022.11.0)\n",
      "\u001b[36mcucd7st2m9-algo-1-w67e8 |\u001b[0m Collecting google-auth<3,>=1.6.3\n",
      "\u001b[36mcucd7st2m9-algo-1-w67e8 |\u001b[0m Downloading google_auth-2.16.2-py2.py3-none-any.whl (177 kB)\n",
      "\u001b[36mcucd7st2m9-algo-1-w67e8 |\u001b[0m ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 177.2/177.2 kB 24.4 MB/s eta 0:00:00\n",
      "\u001b[36mcucd7st2m9-algo-1-w67e8 |\u001b[0m \n",
      "\u001b[36mcucd7st2m9-algo-1-w67e8 |\u001b[0m Requirement already satisfied: setuptools>=41.0.0 in /opt/conda/lib/python3.8/site-packages (from tensorboard==2.12.0->-r requirements.txt (line 5)) (65.6.3)\n",
      "\u001b[36mcucd7st2m9-algo-1-w67e8 |\u001b[0m Collecting tensorboard-data-server<0.8.0,>=0.7.0\n",
      "\u001b[36mcucd7st2m9-algo-1-w67e8 |\u001b[0m Downloading tensorboard_data_server-0.7.0-py3-none-manylinux2014_x86_64.whl (6.6 MB)\n",
      "\u001b[36mcucd7st2m9-algo-1-w67e8 |\u001b[0m ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 6.6/6.6 MB 73.1 MB/s eta 0:00:00\n",
      "\u001b[36mcucd7st2m9-algo-1-w67e8 |\u001b[0m \n",
      "\u001b[36mcucd7st2m9-algo-1-w67e8 |\u001b[0m Requirement already satisfied: requests<3,>=2.21.0 in /opt/conda/lib/python3.8/site-packages (from tensorboard==2.12.0->-r requirements.txt (line 5)) (2.28.1)\n",
      "\u001b[36mcucd7st2m9-algo-1-w67e8 |\u001b[0m Collecting tensorboard-plugin-wit>=1.6.0\n",
      "\u001b[36mcucd7st2m9-algo-1-w67e8 |\u001b[0m Downloading tensorboard_plugin_wit-1.8.1-py3-none-any.whl (781 kB)\n",
      "\u001b[36mcucd7st2m9-algo-1-w67e8 |\u001b[0m ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 781.3/781.3 kB 55.2 MB/s eta 0:00:00\n",
      "\u001b[36mcucd7st2m9-algo-1-w67e8 |\u001b[0m \n",
      "\u001b[36mcucd7st2m9-algo-1-w67e8 |\u001b[0m Collecting grpcio>=1.48.2\n",
      "\u001b[36mcucd7st2m9-algo-1-w67e8 |\u001b[0m Downloading grpcio-1.51.3-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.8 MB)\n",
      "\u001b[36mcucd7st2m9-algo-1-w67e8 |\u001b[0m ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 4.8/4.8 MB 68.4 MB/s eta 0:00:00\n",
      "\u001b[36mcucd7st2m9-algo-1-w67e8 |\u001b[0m \n",
      "\u001b[36mcucd7st2m9-algo-1-w67e8 |\u001b[0m Collecting google-auth-oauthlib<0.5,>=0.4.1\n",
      "\u001b[36mcucd7st2m9-algo-1-w67e8 |\u001b[0m Downloading google_auth_oauthlib-0.4.6-py2.py3-none-any.whl (18 kB)\n",
      "\u001b[36mcucd7st2m9-algo-1-w67e8 |\u001b[0m Requirement already satisfied: wheel>=0.26 in /opt/conda/lib/python3.8/site-packages (from tensorboard==2.12.0->-r requirements.txt (line 5)) (0.38.4)\n",
      "\u001b[36mcucd7st2m9-algo-1-w67e8 |\u001b[0m Requirement already satisfied: protobuf>=3.19.6 in /opt/conda/lib/python3.8/site-packages (from tensorboard==2.12.0->-r requirements.txt (line 5)) (3.19.6)\n",
      "\u001b[36mcucd7st2m9-algo-1-w67e8 |\u001b[0m Requirement already satisfied: werkzeug>=1.0.1 in /opt/conda/lib/python3.8/site-packages (from tensorboard==2.12.0->-r requirements.txt (line 5)) (2.2.2)\n",
      "\u001b[36mcucd7st2m9-algo-1-w67e8 |\u001b[0m Collecting absl-py>=0.4\n",
      "\u001b[36mcucd7st2m9-algo-1-w67e8 |\u001b[0m Downloading absl_py-1.4.0-py3-none-any.whl (126 kB)\n",
      "\u001b[36mcucd7st2m9-algo-1-w67e8 |\u001b[0m ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 126.5/126.5 kB 19.9 MB/s eta 0:00:00\n",
      "\u001b[36mcucd7st2m9-algo-1-w67e8 |\u001b[0m \n",
      "\u001b[36mcucd7st2m9-algo-1-w67e8 |\u001b[0m Collecting markdown>=2.6.8\n",
      "\u001b[36mcucd7st2m9-algo-1-w67e8 |\u001b[0m Downloading Markdown-3.4.1-py3-none-any.whl (93 kB)\n",
      "\u001b[36mcucd7st2m9-algo-1-w67e8 |\u001b[0m ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 93.3/93.3 kB 15.1 MB/s eta 0:00:00\n",
      "\u001b[36mcucd7st2m9-algo-1-w67e8 |\u001b[0m \n",
      "\u001b[36mcucd7st2m9-algo-1-w67e8 |\u001b[0m Collecting aiohttp!=4.0.0a0,!=4.0.0a1\n",
      "\u001b[36mcucd7st2m9-algo-1-w67e8 |\u001b[0m Downloading aiohttp-3.8.4-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.0 MB)\n",
      "\u001b[36mcucd7st2m9-algo-1-w67e8 |\u001b[0m ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1.0/1.0 MB 58.2 MB/s eta 0:00:00\n",
      "\u001b[36mcucd7st2m9-algo-1-w67e8 |\u001b[0m \n",
      "\u001b[36mcucd7st2m9-algo-1-w67e8 |\u001b[0m Collecting pyasn1-modules>=0.2.1\n",
      "\u001b[36mcucd7st2m9-algo-1-w67e8 |\u001b[0m Downloading pyasn1_modules-0.2.8-py2.py3-none-any.whl (155 kB)\n",
      "\u001b[36mcucd7st2m9-algo-1-w67e8 |\u001b[0m ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 155.3/155.3 kB 23.9 MB/s eta 0:00:00\n",
      "\u001b[36mcucd7st2m9-algo-1-w67e8 |\u001b[0m \n",
      "\u001b[36mcucd7st2m9-algo-1-w67e8 |\u001b[0m Collecting cachetools<6.0,>=2.0.0\n",
      "\u001b[36mcucd7st2m9-algo-1-w67e8 |\u001b[0m Downloading cachetools-5.3.0-py3-none-any.whl (9.3 kB)\n",
      "\u001b[36mcucd7st2m9-algo-1-w67e8 |\u001b[0m Requirement already satisfied: rsa<5,>=3.1.4 in /opt/conda/lib/python3.8/site-packages (from google-auth<3,>=1.6.3->tensorboard==2.12.0->-r requirements.txt (line 5)) (4.7.2)\n",
      "\u001b[36mcucd7st2m9-algo-1-w67e8 |\u001b[0m Requirement already satisfied: six>=1.9.0 in /opt/conda/lib/python3.8/site-packages (from google-auth<3,>=1.6.3->tensorboard==2.12.0->-r requirements.txt (line 5)) (1.16.0)\n",
      "\u001b[36mcucd7st2m9-algo-1-w67e8 |\u001b[0m Collecting requests-oauthlib>=0.7.0\n",
      "\u001b[36mcucd7st2m9-algo-1-w67e8 |\u001b[0m Downloading requests_oauthlib-1.3.1-py2.py3-none-any.whl (23 kB)\n",
      "\u001b[36mcucd7st2m9-algo-1-w67e8 |\u001b[0m Requirement already satisfied: importlib-metadata>=4.4 in /opt/conda/lib/python3.8/site-packages (from markdown>=2.6.8->tensorboard==2.12.0->-r requirements.txt (line 5)) (4.13.0)\n",
      "\u001b[36mcucd7st2m9-algo-1-w67e8 |\u001b[0m Collecting cliff\n",
      "\u001b[36mcucd7st2m9-algo-1-w67e8 |\u001b[0m Downloading cliff-4.2.0-py3-none-any.whl (81 kB)\n",
      "\u001b[36mcucd7st2m9-algo-1-w67e8 |\u001b[0m ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 81.0/81.0 kB 13.1 MB/s eta 0:00:00\n",
      "\u001b[36mcucd7st2m9-algo-1-w67e8 |\u001b[0m \n",
      "\u001b[36mcucd7st2m9-algo-1-w67e8 |\u001b[0m Collecting cmaes>=0.8.2\n",
      "\u001b[36mcucd7st2m9-algo-1-w67e8 |\u001b[0m Downloading cmaes-0.9.1-py3-none-any.whl (21 kB)\n",
      "\u001b[36mcucd7st2m9-algo-1-w67e8 |\u001b[0m Collecting sqlalchemy>=1.1.0\n",
      "\u001b[36mcucd7st2m9-algo-1-w67e8 |\u001b[0m Downloading SQLAlchemy-2.0.4-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.8 MB)\n",
      "\u001b[36mcucd7st2m9-algo-1-w67e8 |\u001b[0m ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 2.8/2.8 MB 53.3 MB/s eta 0:00:00\n",
      "\u001b[36mcucd7st2m9-algo-1-w67e8 |\u001b[0m \n",
      "\u001b[36mcucd7st2m9-algo-1-w67e8 |\u001b[0m Collecting colorlog\n",
      "\u001b[36mcucd7st2m9-algo-1-w67e8 |\u001b[0m Downloading colorlog-6.7.0-py2.py3-none-any.whl (11 kB)\n",
      "\u001b[36mcucd7st2m9-algo-1-w67e8 |\u001b[0m Collecting alembic\n",
      "\u001b[36mcucd7st2m9-algo-1-w67e8 |\u001b[0m Downloading alembic-1.9.4-py3-none-any.whl (210 kB)\n",
      "\u001b[36mcucd7st2m9-algo-1-w67e8 |\u001b[0m ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 210.5/210.5 kB 31.6 MB/s eta 0:00:00\n",
      "\u001b[36mcucd7st2m9-algo-1-w67e8 |\u001b[0m \n",
      "\u001b[36mcucd7st2m9-algo-1-w67e8 |\u001b[0m Requirement already satisfied: python-dateutil>=2.8.1 in /opt/conda/lib/python3.8/site-packages (from pandas<2.0.0,>=1.3.0->pytorch-forecasting==0.10.3->-r requirements.txt (line 2)) (2.8.2)\n",
      "\u001b[36mcucd7st2m9-algo-1-w67e8 |\u001b[0m Requirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.8/site-packages (from pandas<2.0.0,>=1.3.0->pytorch-forecasting==0.10.3->-r requirements.txt (line 2)) (2022.7)\n",
      "\u001b[36mcucd7st2m9-algo-1-w67e8 |\u001b[0m Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.8/site-packages (from requests<3,>=2.21.0->tensorboard==2.12.0->-r requirements.txt (line 5)) (3.4)\n",
      "\u001b[36mcucd7st2m9-algo-1-w67e8 |\u001b[0m Requirement already satisfied: charset-normalizer<3,>=2 in /opt/conda/lib/python3.8/site-packages (from requests<3,>=2.21.0->tensorboard==2.12.0->-r requirements.txt (line 5)) (2.1.1)\n",
      "\u001b[36mcucd7st2m9-algo-1-w67e8 |\u001b[0m Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.8/site-packages (from requests<3,>=2.21.0->tensorboard==2.12.0->-r requirements.txt (line 5)) (1.26.13)\n",
      "\u001b[36mcucd7st2m9-algo-1-w67e8 |\u001b[0m Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.8/site-packages (from requests<3,>=2.21.0->tensorboard==2.12.0->-r requirements.txt (line 5)) (2022.12.7)\n",
      "\u001b[36mcucd7st2m9-algo-1-w67e8 |\u001b[0m Requirement already satisfied: joblib>=1.0.0 in /opt/conda/lib/python3.8/site-packages (from scikit-learn<1.2,>=0.24->pytorch-forecasting==0.10.3->-r requirements.txt (line 2)) (1.2.0)\n",
      "\u001b[36mcucd7st2m9-algo-1-w67e8 |\u001b[0m Requirement already satisfied: threadpoolctl>=2.0.0 in /opt/conda/lib/python3.8/site-packages (from scikit-learn<1.2,>=0.24->pytorch-forecasting==0.10.3->-r requirements.txt (line 2)) (3.1.0)\n",
      "\u001b[36mcucd7st2m9-algo-1-w67e8 |\u001b[0m Requirement already satisfied: MarkupSafe>=2.1.1 in /opt/conda/lib/python3.8/site-packages (from werkzeug>=1.0.1->tensorboard==2.12.0->-r requirements.txt (line 5)) (2.1.1)\n",
      "\u001b[36mcucd7st2m9-algo-1-w67e8 |\u001b[0m Requirement already satisfied: contourpy>=1.0.1 in /opt/conda/lib/python3.8/site-packages (from matplotlib->pytorch-forecasting==0.10.3->-r requirements.txt (line 2)) (1.0.6)\n",
      "\u001b[36mcucd7st2m9-algo-1-w67e8 |\u001b[0m Requirement already satisfied: fonttools>=4.22.0 in /opt/conda/lib/python3.8/site-packages (from matplotlib->pytorch-forecasting==0.10.3->-r requirements.txt (line 2)) (4.38.0)\n",
      "\u001b[36mcucd7st2m9-algo-1-w67e8 |\u001b[0m Requirement already satisfied: pyparsing>=2.2.1 in /opt/conda/lib/python3.8/site-packages (from matplotlib->pytorch-forecasting==0.10.3->-r requirements.txt (line 2)) (3.0.9)\n",
      "\u001b[36mcucd7st2m9-algo-1-w67e8 |\u001b[0m Requirement already satisfied: kiwisolver>=1.0.1 in /opt/conda/lib/python3.8/site-packages (from matplotlib->pytorch-forecasting==0.10.3->-r requirements.txt (line 2)) (1.4.4)\n",
      "\u001b[36mcucd7st2m9-algo-1-w67e8 |\u001b[0m Requirement already satisfied: cycler>=0.10 in /opt/conda/lib/python3.8/site-packages (from matplotlib->pytorch-forecasting==0.10.3->-r requirements.txt (line 2)) (0.11.0)\n",
      "\u001b[36mcucd7st2m9-algo-1-w67e8 |\u001b[0m Requirement already satisfied: pillow>=6.2.0 in /opt/conda/lib/python3.8/site-packages (from matplotlib->pytorch-forecasting==0.10.3->-r requirements.txt (line 2)) (9.4.0)\n",
      "\u001b[36mcucd7st2m9-algo-1-w67e8 |\u001b[0m Collecting patsy>=0.5.2\n",
      "\u001b[36mcucd7st2m9-algo-1-w67e8 |\u001b[0m Downloading patsy-0.5.3-py2.py3-none-any.whl (233 kB)\n",
      "\u001b[36mcucd7st2m9-algo-1-w67e8 |\u001b[0m ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 233.8/233.8 kB 34.2 MB/s eta 0:00:00\n",
      "\u001b[36mcucd7st2m9-algo-1-w67e8 |\u001b[0m \n",
      "\u001b[36mcucd7st2m9-algo-1-w67e8 |\u001b[0m Collecting async-timeout<5.0,>=4.0.0a3\n",
      "\u001b[36mcucd7st2m9-algo-1-w67e8 |\u001b[0m Downloading async_timeout-4.0.2-py3-none-any.whl (5.8 kB)\n",
      "\u001b[36mcucd7st2m9-algo-1-w67e8 |\u001b[0m Collecting frozenlist>=1.1.1\n",
      "\u001b[36mcucd7st2m9-algo-1-w67e8 |\u001b[0m Downloading frozenlist-1.3.3-cp38-cp38-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (161 kB)\n",
      "\u001b[36mcucd7st2m9-algo-1-w67e8 |\u001b[0m ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 161.3/161.3 kB 25.8 MB/s eta 0:00:00\n",
      "\u001b[36mcucd7st2m9-algo-1-w67e8 |\u001b[0m \n",
      "\u001b[36mcucd7st2m9-algo-1-w67e8 |\u001b[0m Collecting yarl<2.0,>=1.0\n",
      "\u001b[36mcucd7st2m9-algo-1-w67e8 |\u001b[0m Downloading yarl-1.8.2-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (262 kB)\n",
      "\u001b[36mcucd7st2m9-algo-1-w67e8 |\u001b[0m ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 262.1/262.1 kB 35.0 MB/s eta 0:00:00\n",
      "\u001b[36mcucd7st2m9-algo-1-w67e8 |\u001b[0m \n",
      "\u001b[36mcucd7st2m9-algo-1-w67e8 |\u001b[0m Collecting multidict<7.0,>=4.5\n",
      "\u001b[36mcucd7st2m9-algo-1-w67e8 |\u001b[0m Downloading multidict-6.0.4-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (121 kB)\n",
      "\u001b[36mcucd7st2m9-algo-1-w67e8 |\u001b[0m ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 121.3/121.3 kB 18.3 MB/s eta 0:00:00\n",
      "\u001b[36mcucd7st2m9-algo-1-w67e8 |\u001b[0m \n",
      "\u001b[36mcucd7st2m9-algo-1-w67e8 |\u001b[0m Requirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.8/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch_lightning==1.9.0->-r requirements.txt (line 3)) (22.2.0)\n",
      "\u001b[36mcucd7st2m9-algo-1-w67e8 |\u001b[0m Collecting aiosignal>=1.1.2\n",
      "\u001b[36mcucd7st2m9-algo-1-w67e8 |\u001b[0m Downloading aiosignal-1.3.1-py3-none-any.whl (7.6 kB)\n",
      "\u001b[36mcucd7st2m9-algo-1-w67e8 |\u001b[0m Requirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.8/site-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard==2.12.0->-r requirements.txt (line 5)) (3.11.0)\n",
      "\u001b[36mcucd7st2m9-algo-1-w67e8 |\u001b[0m Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /opt/conda/lib/python3.8/site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard==2.12.0->-r requirements.txt (line 5)) (0.4.8)\n",
      "\u001b[36mcucd7st2m9-algo-1-w67e8 |\u001b[0m Collecting oauthlib>=3.0.0\n",
      "\u001b[36mcucd7st2m9-algo-1-w67e8 |\u001b[0m Downloading oauthlib-3.2.2-py3-none-any.whl (151 kB)\n",
      "\u001b[36mcucd7st2m9-algo-1-w67e8 |\u001b[0m ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 151.7/151.7 kB 25.3 MB/s eta 0:00:00\n",
      "\u001b[36mcucd7st2m9-algo-1-w67e8 |\u001b[0m \n",
      "\u001b[36mcucd7st2m9-algo-1-w67e8 |\u001b[0m Requirement already satisfied: greenlet!=0.4.17 in /opt/conda/lib/python3.8/site-packages (from sqlalchemy>=1.1.0->optuna<3.0.0,>=2.3.0->pytorch-forecasting==0.10.3->-r requirements.txt (line 2)) (2.0.1)\n",
      "\u001b[36mcucd7st2m9-algo-1-w67e8 |\u001b[0m Collecting Mako\n",
      "\u001b[36mcucd7st2m9-algo-1-w67e8 |\u001b[0m Downloading Mako-1.2.4-py3-none-any.whl (78 kB)\n",
      "\u001b[36mcucd7st2m9-algo-1-w67e8 |\u001b[0m ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 78.7/78.7 kB 14.3 MB/s eta 0:00:00\n",
      "\u001b[36mcucd7st2m9-algo-1-w67e8 |\u001b[0m \n",
      "\u001b[36mcucd7st2m9-algo-1-w67e8 |\u001b[0m Collecting importlib-resources\n",
      "\u001b[36mcucd7st2m9-algo-1-w67e8 |\u001b[0m Downloading importlib_resources-5.12.0-py3-none-any.whl (36 kB)\n",
      "\u001b[36mcucd7st2m9-algo-1-w67e8 |\u001b[0m Collecting cmd2>=1.0.0\n",
      "\u001b[36mcucd7st2m9-algo-1-w67e8 |\u001b[0m Downloading cmd2-2.4.3-py3-none-any.whl (147 kB)\n",
      "\u001b[36mcucd7st2m9-algo-1-w67e8 |\u001b[0m ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 147.2/147.2 kB 23.7 MB/s eta 0:00:00\n",
      "\u001b[36mcucd7st2m9-algo-1-w67e8 |\u001b[0m \n",
      "\u001b[36mcucd7st2m9-algo-1-w67e8 |\u001b[0m Collecting PrettyTable>=0.7.2\n",
      "\u001b[36mcucd7st2m9-algo-1-w67e8 |\u001b[0m Downloading prettytable-3.6.0-py3-none-any.whl (27 kB)\n",
      "\u001b[36mcucd7st2m9-algo-1-w67e8 |\u001b[0m Collecting stevedore>=2.0.1\n",
      "\u001b[36mcucd7st2m9-algo-1-w67e8 |\u001b[0m Downloading stevedore-5.0.0-py3-none-any.whl (49 kB)\n",
      "\u001b[36mcucd7st2m9-algo-1-w67e8 |\u001b[0m ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 49.6/49.6 kB 8.8 MB/s eta 0:00:00\n",
      "\u001b[36mcucd7st2m9-algo-1-w67e8 |\u001b[0m \n",
      "\u001b[36mcucd7st2m9-algo-1-w67e8 |\u001b[0m Collecting autopage>=0.4.0\n",
      "\u001b[36mcucd7st2m9-algo-1-w67e8 |\u001b[0m Downloading autopage-0.5.1-py3-none-any.whl (29 kB)\n",
      "\u001b[36mcucd7st2m9-algo-1-w67e8 |\u001b[0m Requirement already satisfied: wcwidth>=0.1.7 in /opt/conda/lib/python3.8/site-packages (from cmd2>=1.0.0->cliff->optuna<3.0.0,>=2.3.0->pytorch-forecasting==0.10.3->-r requirements.txt (line 2)) (0.2.5)\n",
      "\u001b[36mcucd7st2m9-algo-1-w67e8 |\u001b[0m Collecting pyperclip>=1.6\n",
      "\u001b[36mcucd7st2m9-algo-1-w67e8 |\u001b[0m Downloading pyperclip-1.8.2.tar.gz (20 kB)\n",
      "\u001b[36mcucd7st2m9-algo-1-w67e8 |\u001b[0m Preparing metadata (setup.py): started\n",
      "\u001b[36mcucd7st2m9-algo-1-w67e8 |\u001b[0m Preparing metadata (setup.py): finished with status 'done'\n",
      "\u001b[36mcucd7st2m9-algo-1-w67e8 |\u001b[0m Collecting pbr!=2.1.0,>=2.0.0\n",
      "\u001b[36mcucd7st2m9-algo-1-w67e8 |\u001b[0m Downloading pbr-5.11.1-py2.py3-none-any.whl (112 kB)\n",
      "\u001b[36mcucd7st2m9-algo-1-w67e8 |\u001b[0m ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 112.7/112.7 kB 14.4 MB/s eta 0:00:00\n",
      "\u001b[36mcucd7st2m9-algo-1-w67e8 |\u001b[0m \n",
      "\u001b[36mcucd7st2m9-algo-1-w67e8 |\u001b[0m Building wheels for collected packages: pyperclip\n",
      "\u001b[36mcucd7st2m9-algo-1-w67e8 |\u001b[0m Building wheel for pyperclip (setup.py): started\n",
      "\u001b[36mcucd7st2m9-algo-1-w67e8 |\u001b[0m Building wheel for pyperclip (setup.py): finished with status 'done'\n",
      "\u001b[36mcucd7st2m9-algo-1-w67e8 |\u001b[0m Created wheel for pyperclip: filename=pyperclip-1.8.2-py3-none-any.whl size=11124 sha256=73975f6956f9d218151734e693fba6ad9b9511f0f0ce219cfa5108841c5b5de3\n",
      "\u001b[36mcucd7st2m9-algo-1-w67e8 |\u001b[0m Stored in directory: /root/.cache/pip/wheels/e9/08/ff/ce302129d1f57e10fd08d8a476ae601dedba9d08d47fe6be45\n",
      "\u001b[36mcucd7st2m9-algo-1-w67e8 |\u001b[0m Successfully built pyperclip\n",
      "\u001b[36mcucd7st2m9-algo-1-w67e8 |\u001b[0m Installing collected packages: tensorboard-plugin-wit, pyperclip, tensorboard-data-server, sqlalchemy, pyasn1-modules, pyarrow, PrettyTable, pbr, patsy, oauthlib, multidict, Mako, lightning-utilities, importlib-resources, grpcio, frozenlist, colorlog, cmd2, cmaes, cachetools, autopage, async-timeout, absl-py, yarl, torchmetrics, stevedore, scikit-learn, requests-oauthlib, markdown, google-auth, alembic, aiosignal, statsmodels, google-auth-oauthlib, cliff, aiohttp, tensorboard, optuna, pytorch_lightning, pytorch-forecasting\n",
      "\u001b[36mcucd7st2m9-algo-1-w67e8 |\u001b[0m Attempting uninstall: pyarrow\n",
      "\u001b[36mcucd7st2m9-algo-1-w67e8 |\u001b[0m Found existing installation: pyarrow 10.0.1\n",
      "\u001b[36mcucd7st2m9-algo-1-w67e8 |\u001b[0m Uninstalling pyarrow-10.0.1:\n",
      "\u001b[36mcucd7st2m9-algo-1-w67e8 |\u001b[0m Successfully uninstalled pyarrow-10.0.1\n",
      "\u001b[36mcucd7st2m9-algo-1-w67e8 |\u001b[0m Attempting uninstall: scikit-learn\n",
      "\u001b[36mcucd7st2m9-algo-1-w67e8 |\u001b[0m Found existing installation: scikit-learn 1.2.0\n",
      "\u001b[36mcucd7st2m9-algo-1-w67e8 |\u001b[0m Uninstalling scikit-learn-1.2.0:\n",
      "\u001b[36mcucd7st2m9-algo-1-w67e8 |\u001b[0m Successfully uninstalled scikit-learn-1.2.0\n",
      "\u001b[36mcucd7st2m9-algo-1-w67e8 |\u001b[0m Successfully installed Mako-1.2.4 PrettyTable-3.6.0 absl-py-1.4.0 aiohttp-3.8.4 aiosignal-1.3.1 alembic-1.9.4 async-timeout-4.0.2 autopage-0.5.1 cachetools-5.3.0 cliff-4.2.0 cmaes-0.9.1 cmd2-2.4.3 colorlog-6.7.0 frozenlist-1.3.3 google-auth-2.16.2 google-auth-oauthlib-0.4.6 grpcio-1.51.3 importlib-resources-5.12.0 lightning-utilities-0.7.1 markdown-3.4.1 multidict-6.0.4 oauthlib-3.2.2 optuna-2.10.1 patsy-0.5.3 pbr-5.11.1 pyarrow-11.0.0 pyasn1-modules-0.2.8 pyperclip-1.8.2 pytorch-forecasting-0.10.3 pytorch_lightning-1.9.0 requests-oauthlib-1.3.1 scikit-learn-1.1.3 sqlalchemy-2.0.4 statsmodels-0.13.5 stevedore-5.0.0 tensorboard-2.12.0 tensorboard-data-server-0.7.0 tensorboard-plugin-wit-1.8.1 torchmetrics-0.11.3 yarl-1.8.2\n",
      "\u001b[36mcucd7st2m9-algo-1-w67e8 |\u001b[0m WARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\n",
      "\u001b[36mcucd7st2m9-algo-1-w67e8 |\u001b[0m [notice] A new release of pip available: 22.3.1 -> 23.0.1\n",
      "\u001b[36mcucd7st2m9-algo-1-w67e8 |\u001b[0m [notice] To update, run: pip install --upgrade pip\n",
      "\u001b[36mcucd7st2m9-algo-1-w67e8 |\u001b[0m 2023-03-04 09:18:06,339 sagemaker-training-toolkit INFO     Waiting for the process to finish and give a return code.\n",
      "\u001b[36mcucd7st2m9-algo-1-w67e8 |\u001b[0m 2023-03-04 09:18:06,339 sagemaker-training-toolkit INFO     Done waiting for a return code. Received 0 from exiting process.\n",
      "\u001b[36mcucd7st2m9-algo-1-w67e8 |\u001b[0m 2023-03-04 09:18:06,365 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\n",
      "\u001b[36mcucd7st2m9-algo-1-w67e8 |\u001b[0m 2023-03-04 09:18:06,375 sagemaker-training-toolkit INFO     instance_groups entry not present in resource_config\n",
      "\u001b[36mcucd7st2m9-algo-1-w67e8 |\u001b[0m 2023-03-04 09:18:06,403 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\n",
      "\u001b[36mcucd7st2m9-algo-1-w67e8 |\u001b[0m 2023-03-04 09:18:06,414 sagemaker-training-toolkit INFO     instance_groups entry not present in resource_config\n",
      "\u001b[36mcucd7st2m9-algo-1-w67e8 |\u001b[0m 2023-03-04 09:18:06,441 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\n",
      "\u001b[36mcucd7st2m9-algo-1-w67e8 |\u001b[0m 2023-03-04 09:18:06,453 sagemaker-training-toolkit INFO     instance_groups entry not present in resource_config\n",
      "\u001b[36mcucd7st2m9-algo-1-w67e8 |\u001b[0m 2023-03-04 09:18:06,458 sagemaker-training-toolkit INFO     Invoking user script\n",
      "\u001b[36mcucd7st2m9-algo-1-w67e8 |\u001b[0m \n",
      "\u001b[36mcucd7st2m9-algo-1-w67e8 |\u001b[0m Training Env:\n",
      "\u001b[36mcucd7st2m9-algo-1-w67e8 |\u001b[0m \n",
      "\u001b[36mcucd7st2m9-algo-1-w67e8 |\u001b[0m {\n",
      "\u001b[36mcucd7st2m9-algo-1-w67e8 |\u001b[0m     \"additional_framework_parameters\": {},\n",
      "\u001b[36mcucd7st2m9-algo-1-w67e8 |\u001b[0m     \"channel_input_dirs\": {},\n",
      "\u001b[36mcucd7st2m9-algo-1-w67e8 |\u001b[0m     \"current_host\": \"algo-1-w67e8\",\n",
      "\u001b[36mcucd7st2m9-algo-1-w67e8 |\u001b[0m     \"current_instance_group\": \"homogeneousCluster\",\n",
      "\u001b[36mcucd7st2m9-algo-1-w67e8 |\u001b[0m     \"current_instance_group_hosts\": [],\n",
      "\u001b[36mcucd7st2m9-algo-1-w67e8 |\u001b[0m     \"current_instance_type\": \"local\",\n",
      "\u001b[36mcucd7st2m9-algo-1-w67e8 |\u001b[0m     \"distribution_hosts\": [\n",
      "\u001b[36mcucd7st2m9-algo-1-w67e8 |\u001b[0m         \"algo-1-w67e8\"\n",
      "\u001b[36mcucd7st2m9-algo-1-w67e8 |\u001b[0m     ],\n",
      "\u001b[36mcucd7st2m9-algo-1-w67e8 |\u001b[0m     \"distribution_instance_groups\": [],\n",
      "\u001b[36mcucd7st2m9-algo-1-w67e8 |\u001b[0m     \"framework_module\": \"sagemaker_pytorch_container.training:main\",\n",
      "\u001b[36mcucd7st2m9-algo-1-w67e8 |\u001b[0m     \"hosts\": [\n",
      "\u001b[36mcucd7st2m9-algo-1-w67e8 |\u001b[0m         \"algo-1-w67e8\"\n",
      "\u001b[36mcucd7st2m9-algo-1-w67e8 |\u001b[0m     ],\n",
      "\u001b[36mcucd7st2m9-algo-1-w67e8 |\u001b[0m     \"hyperparameters\": {\n",
      "\u001b[36mcucd7st2m9-algo-1-w67e8 |\u001b[0m         \"epochs\": 2,\n",
      "\u001b[36mcucd7st2m9-algo-1-w67e8 |\u001b[0m         \"n_gpus\": 1\n",
      "\u001b[36mcucd7st2m9-algo-1-w67e8 |\u001b[0m     },\n",
      "\u001b[36mcucd7st2m9-algo-1-w67e8 |\u001b[0m     \"input_config_dir\": \"/opt/ml/input/config\",\n",
      "\u001b[36mcucd7st2m9-algo-1-w67e8 |\u001b[0m     \"input_data_config\": {},\n",
      "\u001b[36mcucd7st2m9-algo-1-w67e8 |\u001b[0m     \"input_dir\": \"/opt/ml/input\",\n",
      "\u001b[36mcucd7st2m9-algo-1-w67e8 |\u001b[0m     \"instance_groups\": [],\n",
      "\u001b[36mcucd7st2m9-algo-1-w67e8 |\u001b[0m     \"instance_groups_dict\": {},\n",
      "\u001b[36mcucd7st2m9-algo-1-w67e8 |\u001b[0m     \"is_hetero\": false,\n",
      "\u001b[36mcucd7st2m9-algo-1-w67e8 |\u001b[0m     \"is_master\": true,\n",
      "\u001b[36mcucd7st2m9-algo-1-w67e8 |\u001b[0m     \"is_modelparallel_enabled\": null,\n",
      "\u001b[36mcucd7st2m9-algo-1-w67e8 |\u001b[0m     \"is_smddpmprun_installed\": true,\n",
      "\u001b[36mcucd7st2m9-algo-1-w67e8 |\u001b[0m     \"job_name\": \"pytorch-training-2023-03-04-09-17-36-229\",\n",
      "\u001b[36mcucd7st2m9-algo-1-w67e8 |\u001b[0m     \"log_level\": 20,\n",
      "\u001b[36mcucd7st2m9-algo-1-w67e8 |\u001b[0m     \"master_hostname\": \"algo-1-w67e8\",\n",
      "\u001b[36mcucd7st2m9-algo-1-w67e8 |\u001b[0m     \"model_dir\": \"/opt/ml/model\",\n",
      "\u001b[36mcucd7st2m9-algo-1-w67e8 |\u001b[0m     \"module_dir\": \"s3://sagemaker-ap-northeast-2-057716757052/pytorch-training-2023-03-04-09-17-36-229/source/sourcedir.tar.gz\",\n",
      "\u001b[36mcucd7st2m9-algo-1-w67e8 |\u001b[0m     \"module_name\": \"TFT_Train\",\n",
      "\u001b[36mcucd7st2m9-algo-1-w67e8 |\u001b[0m     \"network_interface_name\": \"eth0\",\n",
      "\u001b[36mcucd7st2m9-algo-1-w67e8 |\u001b[0m     \"num_cpus\": 8,\n",
      "\u001b[36mcucd7st2m9-algo-1-w67e8 |\u001b[0m     \"num_gpus\": 1,\n",
      "\u001b[36mcucd7st2m9-algo-1-w67e8 |\u001b[0m     \"num_neurons\": 0,\n",
      "\u001b[36mcucd7st2m9-algo-1-w67e8 |\u001b[0m     \"output_data_dir\": \"/opt/ml/output/data\",\n",
      "\u001b[36mcucd7st2m9-algo-1-w67e8 |\u001b[0m     \"output_dir\": \"/opt/ml/output\",\n",
      "\u001b[36mcucd7st2m9-algo-1-w67e8 |\u001b[0m     \"output_intermediate_dir\": \"/opt/ml/output/intermediate\",\n",
      "\u001b[36mcucd7st2m9-algo-1-w67e8 |\u001b[0m     \"resource_config\": {\n",
      "\u001b[36mcucd7st2m9-algo-1-w67e8 |\u001b[0m         \"current_host\": \"algo-1-w67e8\",\n",
      "\u001b[36mcucd7st2m9-algo-1-w67e8 |\u001b[0m         \"hosts\": [\n",
      "\u001b[36mcucd7st2m9-algo-1-w67e8 |\u001b[0m             \"algo-1-w67e8\"\n",
      "\u001b[36mcucd7st2m9-algo-1-w67e8 |\u001b[0m         ]\n",
      "\u001b[36mcucd7st2m9-algo-1-w67e8 |\u001b[0m     },\n",
      "\u001b[36mcucd7st2m9-algo-1-w67e8 |\u001b[0m     \"user_entry_point\": \"TFT_Train.py\"\n",
      "\u001b[36mcucd7st2m9-algo-1-w67e8 |\u001b[0m }\n",
      "\u001b[36mcucd7st2m9-algo-1-w67e8 |\u001b[0m \n",
      "\u001b[36mcucd7st2m9-algo-1-w67e8 |\u001b[0m Environment variables:\n",
      "\u001b[36mcucd7st2m9-algo-1-w67e8 |\u001b[0m \n",
      "\u001b[36mcucd7st2m9-algo-1-w67e8 |\u001b[0m SM_HOSTS=[\"algo-1-w67e8\"]\n",
      "\u001b[36mcucd7st2m9-algo-1-w67e8 |\u001b[0m SM_NETWORK_INTERFACE_NAME=eth0\n",
      "\u001b[36mcucd7st2m9-algo-1-w67e8 |\u001b[0m SM_HPS={\"epochs\":2,\"n_gpus\":1}\n",
      "\u001b[36mcucd7st2m9-algo-1-w67e8 |\u001b[0m SM_USER_ENTRY_POINT=TFT_Train.py\n",
      "\u001b[36mcucd7st2m9-algo-1-w67e8 |\u001b[0m SM_FRAMEWORK_PARAMS={}\n",
      "\u001b[36mcucd7st2m9-algo-1-w67e8 |\u001b[0m SM_RESOURCE_CONFIG={\"current_host\":\"algo-1-w67e8\",\"hosts\":[\"algo-1-w67e8\"]}\n",
      "\u001b[36mcucd7st2m9-algo-1-w67e8 |\u001b[0m SM_INPUT_DATA_CONFIG={}\n",
      "\u001b[36mcucd7st2m9-algo-1-w67e8 |\u001b[0m SM_OUTPUT_DATA_DIR=/opt/ml/output/data\n",
      "\u001b[36mcucd7st2m9-algo-1-w67e8 |\u001b[0m SM_CHANNELS=[]\n",
      "\u001b[36mcucd7st2m9-algo-1-w67e8 |\u001b[0m SM_CURRENT_HOST=algo-1-w67e8\n",
      "\u001b[36mcucd7st2m9-algo-1-w67e8 |\u001b[0m SM_CURRENT_INSTANCE_TYPE=local\n",
      "\u001b[36mcucd7st2m9-algo-1-w67e8 |\u001b[0m SM_CURRENT_INSTANCE_GROUP=homogeneousCluster\n",
      "\u001b[36mcucd7st2m9-algo-1-w67e8 |\u001b[0m SM_CURRENT_INSTANCE_GROUP_HOSTS=[]\n",
      "\u001b[36mcucd7st2m9-algo-1-w67e8 |\u001b[0m SM_INSTANCE_GROUPS=[]\n",
      "\u001b[36mcucd7st2m9-algo-1-w67e8 |\u001b[0m SM_INSTANCE_GROUPS_DICT={}\n",
      "\u001b[36mcucd7st2m9-algo-1-w67e8 |\u001b[0m SM_DISTRIBUTION_INSTANCE_GROUPS=[]\n",
      "\u001b[36mcucd7st2m9-algo-1-w67e8 |\u001b[0m SM_IS_HETERO=false\n",
      "\u001b[36mcucd7st2m9-algo-1-w67e8 |\u001b[0m SM_MODULE_NAME=TFT_Train\n",
      "\u001b[36mcucd7st2m9-algo-1-w67e8 |\u001b[0m SM_LOG_LEVEL=20\n",
      "\u001b[36mcucd7st2m9-algo-1-w67e8 |\u001b[0m SM_FRAMEWORK_MODULE=sagemaker_pytorch_container.training:main\n",
      "\u001b[36mcucd7st2m9-algo-1-w67e8 |\u001b[0m SM_INPUT_DIR=/opt/ml/input\n",
      "\u001b[36mcucd7st2m9-algo-1-w67e8 |\u001b[0m SM_INPUT_CONFIG_DIR=/opt/ml/input/config\n",
      "\u001b[36mcucd7st2m9-algo-1-w67e8 |\u001b[0m SM_OUTPUT_DIR=/opt/ml/output\n",
      "\u001b[36mcucd7st2m9-algo-1-w67e8 |\u001b[0m SM_NUM_CPUS=8\n",
      "\u001b[36mcucd7st2m9-algo-1-w67e8 |\u001b[0m SM_NUM_GPUS=1\n",
      "\u001b[36mcucd7st2m9-algo-1-w67e8 |\u001b[0m SM_NUM_NEURONS=0\n",
      "\u001b[36mcucd7st2m9-algo-1-w67e8 |\u001b[0m SM_MODEL_DIR=/opt/ml/model\n",
      "\u001b[36mcucd7st2m9-algo-1-w67e8 |\u001b[0m SM_MODULE_DIR=s3://sagemaker-ap-northeast-2-057716757052/pytorch-training-2023-03-04-09-17-36-229/source/sourcedir.tar.gz\n",
      "\u001b[36mcucd7st2m9-algo-1-w67e8 |\u001b[0m SM_TRAINING_ENV={\"additional_framework_parameters\":{},\"channel_input_dirs\":{},\"current_host\":\"algo-1-w67e8\",\"current_instance_group\":\"homogeneousCluster\",\"current_instance_group_hosts\":[],\"current_instance_type\":\"local\",\"distribution_hosts\":[\"algo-1-w67e8\"],\"distribution_instance_groups\":[],\"framework_module\":\"sagemaker_pytorch_container.training:main\",\"hosts\":[\"algo-1-w67e8\"],\"hyperparameters\":{\"epochs\":2,\"n_gpus\":1},\"input_config_dir\":\"/opt/ml/input/config\",\"input_data_config\":{},\"input_dir\":\"/opt/ml/input\",\"instance_groups\":[],\"instance_groups_dict\":{},\"is_hetero\":false,\"is_master\":true,\"is_modelparallel_enabled\":null,\"is_smddpmprun_installed\":true,\"job_name\":\"pytorch-training-2023-03-04-09-17-36-229\",\"log_level\":20,\"master_hostname\":\"algo-1-w67e8\",\"model_dir\":\"/opt/ml/model\",\"module_dir\":\"s3://sagemaker-ap-northeast-2-057716757052/pytorch-training-2023-03-04-09-17-36-229/source/sourcedir.tar.gz\",\"module_name\":\"TFT_Train\",\"network_interface_name\":\"eth0\",\"num_cpus\":8,\"num_gpus\":1,\"num_neurons\":0,\"output_data_dir\":\"/opt/ml/output/data\",\"output_dir\":\"/opt/ml/output\",\"output_intermediate_dir\":\"/opt/ml/output/intermediate\",\"resource_config\":{\"current_host\":\"algo-1-w67e8\",\"hosts\":[\"algo-1-w67e8\"]},\"user_entry_point\":\"TFT_Train.py\"}\n",
      "\u001b[36mcucd7st2m9-algo-1-w67e8 |\u001b[0m SM_USER_ARGS=[\"--epochs\",\"2\",\"--n_gpus\",\"1\"]\n",
      "\u001b[36mcucd7st2m9-algo-1-w67e8 |\u001b[0m SM_OUTPUT_INTERMEDIATE_DIR=/opt/ml/output/intermediate\n",
      "\u001b[36mcucd7st2m9-algo-1-w67e8 |\u001b[0m SM_HP_EPOCHS=2\n",
      "\u001b[36mcucd7st2m9-algo-1-w67e8 |\u001b[0m SM_HP_N_GPUS=1\n",
      "\u001b[36mcucd7st2m9-algo-1-w67e8 |\u001b[0m PYTHONPATH=/opt/ml/code:/opt/conda/bin:/opt/conda/lib/python38.zip:/opt/conda/lib/python3.8:/opt/conda/lib/python3.8/lib-dynload:/opt/conda/lib/python3.8/site-packages:/opt/conda/lib/python3.8/site-packages/smdebug-1.0.24b20230106-py3.8.egg:/opt/conda/lib/python3.8/site-packages/pyinstrument-3.4.2-py3.8.egg:/opt/conda/lib/python3.8/site-packages/pyinstrument_cext-0.2.4-py3.8-linux-x86_64.egg:/opt/conda/lib/python3.8/site-packages/flash_attn-0.1-py3.8-linux-x86_64.egg:/opt/conda/lib/python3.8/site-packages/einops-0.6.0-py3.8.egg\n",
      "\u001b[36mcucd7st2m9-algo-1-w67e8 |\u001b[0m \n",
      "\u001b[36mcucd7st2m9-algo-1-w67e8 |\u001b[0m Invoking script with the following command:\n",
      "\u001b[36mcucd7st2m9-algo-1-w67e8 |\u001b[0m \n",
      "\u001b[36mcucd7st2m9-algo-1-w67e8 |\u001b[0m /opt/conda/bin/python3.8 TFT_Train.py --epochs 2 --n_gpus 1\n",
      "\u001b[36mcucd7st2m9-algo-1-w67e8 |\u001b[0m \n",
      "\u001b[36mcucd7st2m9-algo-1-w67e8 |\u001b[0m \n",
      "\u001b[36mcucd7st2m9-algo-1-w67e8 |\u001b[0m 2023-03-04 09:18:09,421 sagemaker-training-toolkit INFO     Exceptions not imported for SageMaker TF as Tensorflow is not installed.\n",
      "\u001b[36mcucd7st2m9-algo-1-w67e8 |\u001b[0m Not running on notebook\n",
      "\u001b[36mcucd7st2m9-algo-1-w67e8 |\u001b[0m ***** Arguments *****\n",
      "\u001b[36mcucd7st2m9-algo-1-w67e8 |\u001b[0m epochs=2\n",
      "\u001b[36mcucd7st2m9-algo-1-w67e8 |\u001b[0m seed=100\n",
      "\u001b[36mcucd7st2m9-algo-1-w67e8 |\u001b[0m train_batch_size=64\n",
      "\u001b[36mcucd7st2m9-algo-1-w67e8 |\u001b[0m model_dir=/opt/ml/model\n",
      "\u001b[36mcucd7st2m9-algo-1-w67e8 |\u001b[0m n_gpus=1\n",
      "\u001b[36mcucd7st2m9-algo-1-w67e8 |\u001b[0m /opt/conda/lib/python3.8/site-packages/pytorch_lightning/trainer/connectors/accelerator_connector.py:467: LightningDeprecationWarning: Setting `Trainer(gpus='1')` is deprecated in v1.7 and will be removed in v2.0. Please use `Trainer(accelerator='gpu', devices='1')` instead.\n",
      "\u001b[36mcucd7st2m9-algo-1-w67e8 |\u001b[0m   rank_zero_deprecation(\n",
      "\u001b[36mcucd7st2m9-algo-1-w67e8 |\u001b[0m GPU available: True (cuda), used: True\n",
      "\u001b[36mcucd7st2m9-algo-1-w67e8 |\u001b[0m TPU available: False, using: 0 TPU cores\n",
      "\u001b[36mcucd7st2m9-algo-1-w67e8 |\u001b[0m IPU available: False, using: 0 IPUs\n",
      "\u001b[36mcucd7st2m9-algo-1-w67e8 |\u001b[0m HPU available: False, using: 0 HPUs\n",
      "\u001b[36mcucd7st2m9-algo-1-w67e8 |\u001b[0m Number of parameters in network: 29.7k\n",
      "\u001b[36mcucd7st2m9-algo-1-w67e8 |\u001b[0m Missing logger folder: lightning_logs/lightning_logs\n",
      "\u001b[36mcucd7st2m9-algo-1-w67e8 |\u001b[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\u001b[36mcucd7st2m9-algo-1-w67e8 |\u001b[0m | Name                               | Type                            | Params\n",
      "\u001b[36mcucd7st2m9-algo-1-w67e8 |\u001b[0m ----------------------------------------------------------------------------------------\n",
      "\u001b[36mcucd7st2m9-algo-1-w67e8 |\u001b[0m 0  | loss                               | QuantileLoss                    | 0     \n",
      "\u001b[36mcucd7st2m9-algo-1-w67e8 |\u001b[0m 1  | logging_metrics                    | ModuleList                      | 0     \n",
      "\u001b[36mcucd7st2m9-algo-1-w67e8 |\u001b[0m 2  | input_embeddings                   | MultiEmbedding                  | 1.3 K \n",
      "\u001b[36mcucd7st2m9-algo-1-w67e8 |\u001b[0m 3  | prescalers                         | ModuleDict                      | 256   \n",
      "\u001b[36mcucd7st2m9-algo-1-w67e8 |\u001b[0m 4  | static_variable_selection          | VariableSelectionNetwork        | 3.4 K \n",
      "\u001b[36mcucd7st2m9-algo-1-w67e8 |\u001b[0m 5  | encoder_variable_selection         | VariableSelectionNetwork        | 8.0 K \n",
      "\u001b[36mcucd7st2m9-algo-1-w67e8 |\u001b[0m 6  | decoder_variable_selection         | VariableSelectionNetwork        | 2.7 K \n",
      "\u001b[36mcucd7st2m9-algo-1-w67e8 |\u001b[0m 7  | static_context_variable_selection  | GatedResidualNetwork            | 1.1 K \n",
      "\u001b[36mcucd7st2m9-algo-1-w67e8 |\u001b[0m 8  | static_context_initial_hidden_lstm | GatedResidualNetwork            | 1.1 K \n",
      "\u001b[36mcucd7st2m9-algo-1-w67e8 |\u001b[0m 9  | static_context_initial_cell_lstm   | GatedResidualNetwork            | 1.1 K \n",
      "\u001b[36mcucd7st2m9-algo-1-w67e8 |\u001b[0m 10 | static_context_enrichment          | GatedResidualNetwork            | 1.1 K \n",
      "\u001b[36mcucd7st2m9-algo-1-w67e8 |\u001b[0m 11 | lstm_encoder                       | LSTM                            | 2.2 K \n",
      "\u001b[36mcucd7st2m9-algo-1-w67e8 |\u001b[0m 12 | lstm_decoder                       | LSTM                            | 2.2 K \n",
      "\u001b[36mcucd7st2m9-algo-1-w67e8 |\u001b[0m 13 | post_lstm_gate_encoder             | GatedLinearUnit                 | 544   \n",
      "\u001b[36mcucd7st2m9-algo-1-w67e8 |\u001b[0m 14 | post_lstm_add_norm_encoder         | AddNorm                         | 32    \n",
      "\u001b[36mcucd7st2m9-algo-1-w67e8 |\u001b[0m 15 | static_enrichment                  | GatedResidualNetwork            | 1.4 K \n",
      "\u001b[36mcucd7st2m9-algo-1-w67e8 |\u001b[0m 16 | multihead_attn                     | InterpretableMultiHeadAttention | 1.1 K \n",
      "\u001b[36mcucd7st2m9-algo-1-w67e8 |\u001b[0m 17 | post_attn_gate_norm                | GateAddNorm                     | 576   \n",
      "\u001b[36mcucd7st2m9-algo-1-w67e8 |\u001b[0m 18 | pos_wise_ff                        | GatedResidualNetwork            | 1.1 K \n",
      "\u001b[36mcucd7st2m9-algo-1-w67e8 |\u001b[0m 19 | pre_output_gate_norm               | GateAddNorm                     | 576   \n",
      "\u001b[36mcucd7st2m9-algo-1-w67e8 |\u001b[0m 20 | output_layer                       | Linear                          | 119   \n",
      "\u001b[36mcucd7st2m9-algo-1-w67e8 |\u001b[0m ----------------------------------------------------------------------------------------\n",
      "\u001b[36mcucd7st2m9-algo-1-w67e8 |\u001b[0m 29.7 K    Trainable params\n",
      "\u001b[36mcucd7st2m9-algo-1-w67e8 |\u001b[0m 0         Non-trainable params\n",
      "\u001b[36mcucd7st2m9-algo-1-w67e8 |\u001b[0m 29.7 K    Total params\n",
      "\u001b[36mcucd7st2m9-algo-1-w67e8 |\u001b[0m 0.119     Total estimated model params size (MB)\n",
      "\u001b[36mcucd7st2m9-algo-1-w67e8 |\u001b[0m Sanity Checking: 0it [00:00, ?it/s]\n",
      "\u001b[36mcucd7st2m9-algo-1-w67e8 |\u001b[0m [2023-03-04 09:18:22.314 algo-1-w67e8:88 INFO utils.py:28] RULE_JOB_STOP_SIGNAL_FILENAME: None\n",
      "\u001b[36mcucd7st2m9-algo-1-w67e8 |\u001b[0m [2023-03-04 09:18:22.484 algo-1-w67e8:88 INFO profiler_config_parser.py:111] Unable to find config at /opt/ml/input/config/profilerconfig.json. Profiler is disabled.\n",
      "\u001b[36mcucd7st2m9-algo-1-w67e8 |\u001b[0m Sanity Checking:   0%|          | 0/1 [00:00<?, ?it/s]\n",
      "\u001b[36mcucd7st2m9-algo-1-w67e8 |\u001b[0m Sanity Checking DataLoader 0:   0%|          | 0/1 [00:00<?, ?it/s]\n",
      "\u001b[36mcucd7st2m9-algo-1-w67e8 |\u001b[0m Sanity Checking DataLoader 0: 100%|██████████| 1/1 [00:01<00:00,  1.05s/it]\n",
      "\u001b[36mcucd7st2m9-algo-1-w67e8 |\u001b[0m \n",
      "\u001b[36mcucd7st2m9-algo-1-w67e8 |\u001b[0m Training: 0it [00:00, ?it/s]\n",
      "\u001b[36mcucd7st2m9-algo-1-w67e8 |\u001b[0m Training:   0%|          | 0/162 [00:00<?, ?it/s]\n",
      "\u001b[36mcucd7st2m9-algo-1-w67e8 |\u001b[0m Epoch 0:   0%|          | 0/162 [00:00<?, ?it/s]\n",
      "\u001b[36mcucd7st2m9-algo-1-w67e8 |\u001b[0m Epoch 0:   1%|          | 1/162 [00:00<01:58,  1.36it/s]\n",
      "\u001b[36mcucd7st2m9-algo-1-w67e8 |\u001b[0m Epoch 0:   1%|          | 1/162 [00:00<01:58,  1.36it/s, loss=368, v_num=0, train_loss_step=368.0]\n",
      "\u001b[36mcucd7st2m9-algo-1-w67e8 |\u001b[0m Epoch 0:   1%|          | 2/162 [00:00<01:17,  2.08it/s, loss=368, v_num=0, train_loss_step=368.0]\n",
      "\u001b[36mcucd7st2m9-algo-1-w67e8 |\u001b[0m Epoch 0:   1%|          | 2/162 [00:00<01:17,  2.08it/s, loss=348, v_num=0, train_loss_step=327.0]\n",
      "\u001b[36mcucd7st2m9-algo-1-w67e8 |\u001b[0m Epoch 0:   2%|▏         | 3/162 [00:01<01:05,  2.44it/s, loss=348, v_num=0, train_loss_step=327.0]\n",
      "\u001b[36mcucd7st2m9-algo-1-w67e8 |\u001b[0m Epoch 0:   2%|▏         | 3/162 [00:01<01:05,  2.43it/s, loss=348, v_num=0, train_loss_step=349.0]\n",
      "\u001b[36mcucd7st2m9-algo-1-w67e8 |\u001b[0m Epoch 0:   2%|▏         | 4/162 [00:01<00:58,  2.71it/s, loss=348, v_num=0, train_loss_step=349.0]\n",
      "\u001b[36mcucd7st2m9-algo-1-w67e8 |\u001b[0m Epoch 0:   2%|▏         | 4/162 [00:01<00:58,  2.71it/s, loss=333, v_num=0, train_loss_step=287.0]\n",
      "\u001b[36mcucd7st2m9-algo-1-w67e8 |\u001b[0m Epoch 0:   3%|▎         | 5/162 [00:01<00:53,  2.93it/s, loss=333, v_num=0, train_loss_step=287.0]\n",
      "\u001b[36mcucd7st2m9-algo-1-w67e8 |\u001b[0m Epoch 0:   3%|▎         | 5/162 [00:01<00:53,  2.93it/s, loss=332, v_num=0, train_loss_step=326.0]\n",
      "\u001b[36mcucd7st2m9-algo-1-w67e8 |\u001b[0m Epoch 0:   4%|▎         | 6/162 [00:01<00:51,  3.02it/s, loss=332, v_num=0, train_loss_step=326.0]\n",
      "\u001b[36mcucd7st2m9-algo-1-w67e8 |\u001b[0m Epoch 0:   4%|▎         | 6/162 [00:01<00:51,  3.02it/s, loss=322, v_num=0, train_loss_step=275.0]\n",
      "\u001b[36mcucd7st2m9-algo-1-w67e8 |\u001b[0m Epoch 0:   4%|▍         | 7/162 [00:02<00:49,  3.13it/s, loss=322, v_num=0, train_loss_step=275.0]\n",
      "\u001b[36mcucd7st2m9-algo-1-w67e8 |\u001b[0m Epoch 0:   4%|▍         | 7/162 [00:02<00:49,  3.13it/s, loss=336, v_num=0, train_loss_step=418.0]\n",
      "\u001b[36mcucd7st2m9-algo-1-w67e8 |\u001b[0m Epoch 0:   5%|▍         | 8/162 [00:02<00:47,  3.21it/s, loss=336, v_num=0, train_loss_step=418.0]\n",
      "\u001b[36mcucd7st2m9-algo-1-w67e8 |\u001b[0m Epoch 0:   5%|▍         | 8/162 [00:02<00:47,  3.21it/s, loss=332, v_num=0, train_loss_step=306.0]\n",
      "\u001b[36mcucd7st2m9-algo-1-w67e8 |\u001b[0m Epoch 0:   6%|▌         | 9/162 [00:02<00:46,  3.30it/s, loss=332, v_num=0, train_loss_step=306.0]\n",
      "\u001b[36mcucd7st2m9-algo-1-w67e8 |\u001b[0m Epoch 0:   6%|▌         | 9/162 [00:02<00:46,  3.30it/s, loss=322, v_num=0, train_loss_step=236.0]\n",
      "\u001b[36mcucd7st2m9-algo-1-w67e8 |\u001b[0m Epoch 0:   6%|▌         | 10/162 [00:02<00:45,  3.35it/s, loss=322, v_num=0, train_loss_step=236.0]\n",
      "\u001b[36mcucd7st2m9-algo-1-w67e8 |\u001b[0m Epoch 0:   6%|▌         | 10/162 [00:02<00:45,  3.35it/s, loss=322, v_num=0, train_loss_step=326.0]\n",
      "\u001b[36mcucd7st2m9-algo-1-w67e8 |\u001b[0m Epoch 0:   7%|▋         | 11/162 [00:03<00:44,  3.39it/s, loss=322, v_num=0, train_loss_step=326.0]\n",
      "\u001b[36mcucd7st2m9-algo-1-w67e8 |\u001b[0m Epoch 0:   7%|▋         | 11/162 [00:03<00:44,  3.38it/s, loss=312, v_num=0, train_loss_step=213.0]\n",
      "\u001b[36mcucd7st2m9-algo-1-w67e8 |\u001b[0m Epoch 0:   7%|▋         | 12/162 [00:03<00:44,  3.40it/s, loss=312, v_num=0, train_loss_step=213.0]\n",
      "\u001b[36mcucd7st2m9-algo-1-w67e8 |\u001b[0m Epoch 0:   7%|▋         | 12/162 [00:03<00:44,  3.40it/s, loss=308, v_num=0, train_loss_step=263.0]\n",
      "\u001b[36mcucd7st2m9-algo-1-w67e8 |\u001b[0m Epoch 0:   8%|▊         | 13/162 [00:03<00:43,  3.45it/s, loss=308, v_num=0, train_loss_step=263.0]\n",
      "\u001b[36mcucd7st2m9-algo-1-w67e8 |\u001b[0m Epoch 0:   8%|▊         | 13/162 [00:03<00:43,  3.45it/s, loss=308, v_num=0, train_loss_step=307.0]\n",
      "\u001b[36mcucd7st2m9-algo-1-w67e8 |\u001b[0m Epoch 0:   9%|▊         | 14/162 [00:04<00:42,  3.48it/s, loss=308, v_num=0, train_loss_step=307.0]\n",
      "\u001b[36mcucd7st2m9-algo-1-w67e8 |\u001b[0m Epoch 0:   9%|▊         | 14/162 [00:04<00:42,  3.48it/s, loss=302, v_num=0, train_loss_step=229.0]\n",
      "\u001b[36mcucd7st2m9-algo-1-w67e8 |\u001b[0m Epoch 0:   9%|▉         | 15/162 [00:04<00:41,  3.51it/s, loss=302, v_num=0, train_loss_step=229.0]\n",
      "\u001b[36mcucd7st2m9-algo-1-w67e8 |\u001b[0m Epoch 0:   9%|▉         | 15/162 [00:04<00:41,  3.51it/s, loss=293, v_num=0, train_loss_step=166.0]\n",
      "\u001b[36mcucd7st2m9-algo-1-w67e8 |\u001b[0m Epoch 0:  10%|▉         | 16/162 [00:04<00:41,  3.53it/s, loss=293, v_num=0, train_loss_step=166.0]\n",
      "\u001b[36mcucd7st2m9-algo-1-w67e8 |\u001b[0m Epoch 0:  10%|▉         | 16/162 [00:04<00:41,  3.53it/s, loss=287, v_num=0, train_loss_step=196.0]\n",
      "\u001b[36mcucd7st2m9-algo-1-w67e8 |\u001b[0m Epoch 0:  10%|█         | 17/162 [00:04<00:40,  3.56it/s, loss=287, v_num=0, train_loss_step=196.0]\n",
      "\u001b[36mcucd7st2m9-algo-1-w67e8 |\u001b[0m Epoch 0:  10%|█         | 17/162 [00:04<00:40,  3.56it/s, loss=281, v_num=0, train_loss_step=190.0]\n",
      "\u001b[36mcucd7st2m9-algo-1-w67e8 |\u001b[0m Epoch 0:  11%|█         | 18/162 [00:05<00:40,  3.55it/s, loss=281, v_num=0, train_loss_step=190.0]\n",
      "\u001b[36mcucd7st2m9-algo-1-w67e8 |\u001b[0m Epoch 0:  11%|█         | 18/162 [00:05<00:40,  3.55it/s, loss=278, v_num=0, train_loss_step=224.0]\n",
      "\u001b[36mcucd7st2m9-algo-1-w67e8 |\u001b[0m Epoch 0:  12%|█▏        | 19/162 [00:05<00:40,  3.56it/s, loss=278, v_num=0, train_loss_step=224.0]\n",
      "\u001b[36mcucd7st2m9-algo-1-w67e8 |\u001b[0m Epoch 0:  12%|█▏        | 19/162 [00:05<00:40,  3.56it/s, loss=273, v_num=0, train_loss_step=186.0]\n",
      "\u001b[36mcucd7st2m9-algo-1-w67e8 |\u001b[0m Epoch 0:  12%|█▏        | 20/162 [00:05<00:39,  3.58it/s, loss=273, v_num=0, train_loss_step=186.0]\n",
      "\u001b[36mcucd7st2m9-algo-1-w67e8 |\u001b[0m Epoch 0:  12%|█▏        | 20/162 [00:05<00:39,  3.57it/s, loss=271, v_num=0, train_loss_step=232.0]\n",
      "\u001b[36mcucd7st2m9-algo-1-w67e8 |\u001b[0m Epoch 0:  13%|█▎        | 21/162 [00:05<00:39,  3.59it/s, loss=271, v_num=0, train_loss_step=232.0]\n",
      "\u001b[36mcucd7st2m9-algo-1-w67e8 |\u001b[0m Epoch 0:  13%|█▎        | 21/162 [00:05<00:39,  3.59it/s, loss=263, v_num=0, train_loss_step=197.0]\n",
      "\u001b[36mcucd7st2m9-algo-1-w67e8 |\u001b[0m Epoch 0:  14%|█▎        | 22/162 [00:06<00:38,  3.61it/s, loss=263, v_num=0, train_loss_step=197.0]\n",
      "\u001b[36mcucd7st2m9-algo-1-w67e8 |\u001b[0m Epoch 0:  14%|█▎        | 22/162 [00:06<00:38,  3.61it/s, loss=259, v_num=0, train_loss_step=247.0]\n",
      "\u001b[36mcucd7st2m9-algo-1-w67e8 |\u001b[0m Epoch 0:  14%|█▍        | 23/162 [00:06<00:38,  3.62it/s, loss=259, v_num=0, train_loss_step=247.0]\n",
      "\u001b[36mcucd7st2m9-algo-1-w67e8 |\u001b[0m Epoch 0:  14%|█▍        | 23/162 [00:06<00:38,  3.62it/s, loss=249, v_num=0, train_loss_step=165.0]\n",
      "\u001b[36mcucd7st2m9-algo-1-w67e8 |\u001b[0m Epoch 0:  15%|█▍        | 24/162 [00:06<00:38,  3.62it/s, loss=249, v_num=0, train_loss_step=165.0]\n",
      "\u001b[36mcucd7st2m9-algo-1-w67e8 |\u001b[0m Epoch 0:  15%|█▍        | 24/162 [00:06<00:38,  3.62it/s, loss=245, v_num=0, train_loss_step=194.0]\n",
      "\u001b[36mcucd7st2m9-algo-1-w67e8 |\u001b[0m Epoch 0:  15%|█▌        | 25/162 [00:06<00:37,  3.63it/s, loss=245, v_num=0, train_loss_step=194.0]\n",
      "\u001b[36mcucd7st2m9-algo-1-w67e8 |\u001b[0m Epoch 0:  15%|█▌        | 25/162 [00:06<00:37,  3.63it/s, loss=236, v_num=0, train_loss_step=152.0]\n",
      "\u001b[36mcucd7st2m9-algo-1-w67e8 |\u001b[0m Epoch 0:  16%|█▌        | 26/162 [00:07<00:37,  3.65it/s, loss=236, v_num=0, train_loss_step=152.0]\n",
      "\u001b[36mcucd7st2m9-algo-1-w67e8 |\u001b[0m Epoch 0:  16%|█▌        | 26/162 [00:07<00:37,  3.65it/s, loss=232, v_num=0, train_loss_step=196.0]\n",
      "\u001b[36mcucd7st2m9-algo-1-w67e8 |\u001b[0m Epoch 0:  17%|█▋        | 27/162 [00:07<00:36,  3.66it/s, loss=232, v_num=0, train_loss_step=196.0]\n",
      "\u001b[36mcucd7st2m9-algo-1-w67e8 |\u001b[0m Epoch 0:  17%|█▋        | 27/162 [00:07<00:36,  3.66it/s, loss=223, v_num=0, train_loss_step=231.0]\n",
      "\u001b[36mcucd7st2m9-algo-1-w67e8 |\u001b[0m Epoch 0:  17%|█▋        | 28/162 [00:07<00:36,  3.67it/s, loss=223, v_num=0, train_loss_step=231.0]\n",
      "\u001b[36mcucd7st2m9-algo-1-w67e8 |\u001b[0m Epoch 0:  17%|█▋        | 28/162 [00:07<00:36,  3.67it/s, loss=214, v_num=0, train_loss_step=138.0]\n",
      "\u001b[36mcucd7st2m9-algo-1-w67e8 |\u001b[0m Epoch 0:  18%|█▊        | 29/162 [00:07<00:36,  3.68it/s, loss=214, v_num=0, train_loss_step=138.0]\n",
      "\u001b[36mcucd7st2m9-algo-1-w67e8 |\u001b[0m Epoch 0:  18%|█▊        | 29/162 [00:07<00:36,  3.68it/s, loss=211, v_num=0, train_loss_step=176.0]\n",
      "\u001b[36mcucd7st2m9-algo-1-w67e8 |\u001b[0m Epoch 0:  19%|█▊        | 30/162 [00:08<00:35,  3.68it/s, loss=211, v_num=0, train_loss_step=176.0]\n",
      "\u001b[36mcucd7st2m9-algo-1-w67e8 |\u001b[0m Epoch 0:  19%|█▊        | 30/162 [00:08<00:35,  3.68it/s, loss=203, v_num=0, train_loss_step=164.0]\n",
      "\u001b[36mcucd7st2m9-algo-1-w67e8 |\u001b[0m Epoch 0:  19%|█▉        | 31/162 [00:08<00:35,  3.70it/s, loss=203, v_num=0, train_loss_step=164.0]\n",
      "\u001b[36mcucd7st2m9-algo-1-w67e8 |\u001b[0m Epoch 0:  19%|█▉        | 31/162 [00:08<00:35,  3.70it/s, loss=203, v_num=0, train_loss_step=200.0]\n",
      "\u001b[36mcucd7st2m9-algo-1-w67e8 |\u001b[0m Epoch 0:  20%|█▉        | 32/162 [00:08<00:35,  3.71it/s, loss=203, v_num=0, train_loss_step=200.0]\n",
      "\u001b[36mcucd7st2m9-algo-1-w67e8 |\u001b[0m Epoch 0:  20%|█▉        | 32/162 [00:08<00:35,  3.71it/s, loss=197, v_num=0, train_loss_step=149.0]\n",
      "\u001b[36mcucd7st2m9-algo-1-w67e8 |\u001b[0m Epoch 0:  20%|██        | 33/162 [00:08<00:34,  3.71it/s, loss=197, v_num=0, train_loss_step=149.0]\n",
      "\u001b[36mcucd7st2m9-algo-1-w67e8 |\u001b[0m Epoch 0:  20%|██        | 33/162 [00:08<00:34,  3.71it/s, loss=189, v_num=0, train_loss_step=149.0]\n",
      "\u001b[36mcucd7st2m9-algo-1-w67e8 |\u001b[0m Epoch 0:  21%|██        | 34/162 [00:09<00:34,  3.72it/s, loss=189, v_num=0, train_loss_step=149.0]\n",
      "\u001b[36mcucd7st2m9-algo-1-w67e8 |\u001b[0m Epoch 0:  21%|██        | 34/162 [00:09<00:34,  3.72it/s, loss=184, v_num=0, train_loss_step=131.0]\n",
      "\u001b[36mcucd7st2m9-algo-1-w67e8 |\u001b[0m Epoch 0:  22%|██▏       | 35/162 [00:09<00:34,  3.73it/s, loss=184, v_num=0, train_loss_step=131.0]\n",
      "\u001b[36mcucd7st2m9-algo-1-w67e8 |\u001b[0m Epoch 0:  22%|██▏       | 35/162 [00:09<00:34,  3.73it/s, loss=184, v_num=0, train_loss_step=173.0]\n",
      "\u001b[36mcucd7st2m9-algo-1-w67e8 |\u001b[0m Epoch 0:  22%|██▏       | 36/162 [00:09<00:33,  3.73it/s, loss=184, v_num=0, train_loss_step=173.0]\n",
      "\u001b[36mcucd7st2m9-algo-1-w67e8 |\u001b[0m Epoch 0:  22%|██▏       | 36/162 [00:09<00:33,  3.73it/s, loss=182, v_num=0, train_loss_step=152.0]\n",
      "\u001b[36mcucd7st2m9-algo-1-w67e8 |\u001b[0m Epoch 0:  23%|██▎       | 37/162 [00:09<00:33,  3.74it/s, loss=182, v_num=0, train_loss_step=152.0]\n",
      "\u001b[36mcucd7st2m9-algo-1-w67e8 |\u001b[0m Epoch 0:  23%|██▎       | 37/162 [00:09<00:33,  3.74it/s, loss=182, v_num=0, train_loss_step=190.0]\n",
      "\u001b[36mcucd7st2m9-algo-1-w67e8 |\u001b[0m Epoch 0:  23%|██▎       | 38/162 [00:10<00:33,  3.75it/s, loss=182, v_num=0, train_loss_step=190.0]\n",
      "\u001b[36mcucd7st2m9-algo-1-w67e8 |\u001b[0m Epoch 0:  23%|██▎       | 38/162 [00:10<00:33,  3.75it/s, loss=180, v_num=0, train_loss_step=172.0]\n",
      "\u001b[36mcucd7st2m9-algo-1-w67e8 |\u001b[0m Epoch 0:  24%|██▍       | 39/162 [00:10<00:32,  3.76it/s, loss=180, v_num=0, train_loss_step=172.0]\n",
      "\u001b[36mcucd7st2m9-algo-1-w67e8 |\u001b[0m Epoch 0:  24%|██▍       | 39/162 [00:10<00:32,  3.76it/s, loss=177, v_num=0, train_loss_step=132.0]\n",
      "\u001b[36mcucd7st2m9-algo-1-w67e8 |\u001b[0m Epoch 0:  25%|██▍       | 40/162 [00:10<00:32,  3.77it/s, loss=177, v_num=0, train_loss_step=132.0]\n",
      "\u001b[36mcucd7st2m9-algo-1-w67e8 |\u001b[0m Epoch 0:  25%|██▍       | 40/162 [00:10<00:32,  3.77it/s, loss=173, v_num=0, train_loss_step=143.0]\n",
      "\u001b[36mcucd7st2m9-algo-1-w67e8 |\u001b[0m Epoch 0:  25%|██▌       | 41/162 [00:10<00:32,  3.77it/s, loss=173, v_num=0, train_loss_step=143.0]\n",
      "\u001b[36mcucd7st2m9-algo-1-w67e8 |\u001b[0m Epoch 0:  25%|██▌       | 41/162 [00:10<00:32,  3.77it/s, loss=172, v_num=0, train_loss_step=177.0]\n",
      "\u001b[36mcucd7st2m9-algo-1-w67e8 |\u001b[0m Epoch 0:  26%|██▌       | 42/162 [00:11<00:31,  3.77it/s, loss=172, v_num=0, train_loss_step=177.0]\n",
      "\u001b[36mcucd7st2m9-algo-1-w67e8 |\u001b[0m Epoch 0:  26%|██▌       | 42/162 [00:11<00:31,  3.77it/s, loss=167, v_num=0, train_loss_step=156.0]\n",
      "\u001b[36mcucd7st2m9-algo-1-w67e8 |\u001b[0m Epoch 0:  27%|██▋       | 43/162 [00:11<00:31,  3.78it/s, loss=167, v_num=0, train_loss_step=156.0]\n",
      "\u001b[36mcucd7st2m9-algo-1-w67e8 |\u001b[0m Epoch 0:  27%|██▋       | 43/162 [00:11<00:31,  3.78it/s, loss=166, v_num=0, train_loss_step=141.0]\n",
      "\u001b[36mcucd7st2m9-algo-1-w67e8 |\u001b[0m Epoch 0:  27%|██▋       | 44/162 [00:11<00:31,  3.79it/s, loss=166, v_num=0, train_loss_step=141.0]\n",
      "\u001b[36mcucd7st2m9-algo-1-w67e8 |\u001b[0m Epoch 0:  27%|██▋       | 44/162 [00:11<00:31,  3.79it/s, loss=164, v_num=0, train_loss_step=157.0]\n",
      "\u001b[36mcucd7st2m9-algo-1-w67e8 |\u001b[0m Epoch 0:  28%|██▊       | 45/162 [00:11<00:30,  3.79it/s, loss=164, v_num=0, train_loss_step=157.0]\n",
      "\u001b[36mcucd7st2m9-algo-1-w67e8 |\u001b[0m Epoch 0:  28%|██▊       | 45/162 [00:11<00:30,  3.79it/s, loss=162, v_num=0, train_loss_step=110.0]\n",
      "\u001b[36mcucd7st2m9-algo-1-w67e8 |\u001b[0m Epoch 0:  28%|██▊       | 46/162 [00:12<00:30,  3.79it/s, loss=162, v_num=0, train_loss_step=110.0]\n",
      "\u001b[36mcucd7st2m9-algo-1-w67e8 |\u001b[0m Epoch 0:  28%|██▊       | 46/162 [00:12<00:30,  3.79it/s, loss=159, v_num=0, train_loss_step=138.0]\n",
      "\u001b[36mcucd7st2m9-algo-1-w67e8 |\u001b[0m Epoch 0:  29%|██▉       | 47/162 [00:12<00:30,  3.80it/s, loss=159, v_num=0, train_loss_step=138.0]\n",
      "\u001b[36mcucd7st2m9-algo-1-w67e8 |\u001b[0m Epoch 0:  29%|██▉       | 47/162 [00:12<00:30,  3.80it/s, loss=156, v_num=0, train_loss_step=168.0]\n",
      "\u001b[36mcucd7st2m9-algo-1-w67e8 |\u001b[0m Epoch 0:  30%|██▉       | 48/162 [00:12<00:30,  3.79it/s, loss=156, v_num=0, train_loss_step=168.0]\n",
      "\u001b[36mcucd7st2m9-algo-1-w67e8 |\u001b[0m Epoch 0:  30%|██▉       | 48/162 [00:12<00:30,  3.79it/s, loss=156, v_num=0, train_loss_step=139.0]\n",
      "\u001b[36mcucd7st2m9-algo-1-w67e8 |\u001b[0m Epoch 0:  30%|███       | 49/162 [00:12<00:29,  3.79it/s, loss=156, v_num=0, train_loss_step=139.0]\n",
      "\u001b[36mcucd7st2m9-algo-1-w67e8 |\u001b[0m Epoch 0:  30%|███       | 49/162 [00:12<00:29,  3.79it/s, loss=154, v_num=0, train_loss_step=143.0]\n",
      "\u001b[36mcucd7st2m9-algo-1-w67e8 |\u001b[0m Epoch 0:  31%|███       | 50/162 [00:13<00:29,  3.80it/s, loss=154, v_num=0, train_loss_step=143.0]\n",
      "\u001b[36mcucd7st2m9-algo-1-w67e8 |\u001b[0m Epoch 0:  31%|███       | 50/162 [00:13<00:29,  3.80it/s, loss=156, v_num=0, train_loss_step=196.0]\n",
      "\u001b[36mcucd7st2m9-algo-1-w67e8 |\u001b[0m Epoch 0:  31%|███▏      | 51/162 [00:13<00:29,  3.80it/s, loss=156, v_num=0, train_loss_step=196.0]\n",
      "\u001b[36mcucd7st2m9-algo-1-w67e8 |\u001b[0m Epoch 0:  31%|███▏      | 51/162 [00:13<00:29,  3.80it/s, loss=153, v_num=0, train_loss_step=138.0]\n",
      "\u001b[36mcucd7st2m9-algo-1-w67e8 |\u001b[0m Epoch 0:  32%|███▏      | 52/162 [00:13<00:28,  3.80it/s, loss=153, v_num=0, train_loss_step=138.0]\n",
      "\u001b[36mcucd7st2m9-algo-1-w67e8 |\u001b[0m Epoch 0:  32%|███▏      | 52/162 [00:13<00:28,  3.80it/s, loss=152, v_num=0, train_loss_step=133.0]\n",
      "\u001b[36mcucd7st2m9-algo-1-w67e8 |\u001b[0m Epoch 0:  33%|███▎      | 53/162 [00:13<00:28,  3.81it/s, loss=152, v_num=0, train_loss_step=133.0]\n",
      "\u001b[36mcucd7st2m9-algo-1-w67e8 |\u001b[0m Epoch 0:  33%|███▎      | 53/162 [00:13<00:28,  3.81it/s, loss=153, v_num=0, train_loss_step=173.0]\n",
      "\u001b[36mcucd7st2m9-algo-1-w67e8 |\u001b[0m Epoch 0:  33%|███▎      | 54/162 [00:14<00:28,  3.81it/s, loss=153, v_num=0, train_loss_step=173.0]\n",
      "\u001b[36mcucd7st2m9-algo-1-w67e8 |\u001b[0m Epoch 0:  33%|███▎      | 54/162 [00:14<00:28,  3.81it/s, loss=157, v_num=0, train_loss_step=198.0]\n",
      "\u001b[36mcucd7st2m9-algo-1-w67e8 |\u001b[0m Epoch 0:  34%|███▍      | 55/162 [00:14<00:28,  3.81it/s, loss=157, v_num=0, train_loss_step=198.0]\n",
      "\u001b[36mcucd7st2m9-algo-1-w67e8 |\u001b[0m Epoch 0:  34%|███▍      | 55/162 [00:14<00:28,  3.81it/s, loss=156, v_num=0, train_loss_step=170.0]\n",
      "\u001b[36mcucd7st2m9-algo-1-w67e8 |\u001b[0m Epoch 0:  35%|███▍      | 56/162 [00:14<00:27,  3.82it/s, loss=156, v_num=0, train_loss_step=170.0]\n",
      "\u001b[36mcucd7st2m9-algo-1-w67e8 |\u001b[0m Epoch 0:  35%|███▍      | 56/162 [00:14<00:27,  3.82it/s, loss=155, v_num=0, train_loss_step=133.0]\n",
      "\u001b[36mcucd7st2m9-algo-1-w67e8 |\u001b[0m Epoch 0:  35%|███▌      | 57/162 [00:14<00:27,  3.82it/s, loss=155, v_num=0, train_loss_step=133.0]\n",
      "\u001b[36mcucd7st2m9-algo-1-w67e8 |\u001b[0m Epoch 0:  35%|███▌      | 57/162 [00:14<00:27,  3.82it/s, loss=153, v_num=0, train_loss_step=145.0]\n",
      "\u001b[36mcucd7st2m9-algo-1-w67e8 |\u001b[0m Epoch 0:  36%|███▌      | 58/162 [00:15<00:27,  3.82it/s, loss=153, v_num=0, train_loss_step=145.0]\n",
      "\u001b[36mcucd7st2m9-algo-1-w67e8 |\u001b[0m Epoch 0:  36%|███▌      | 58/162 [00:15<00:27,  3.82it/s, loss=152, v_num=0, train_loss_step=143.0]\n",
      "\u001b[36mcucd7st2m9-algo-1-w67e8 |\u001b[0m Epoch 0:  36%|███▋      | 59/162 [00:15<00:26,  3.82it/s, loss=152, v_num=0, train_loss_step=143.0]\n",
      "\u001b[36mcucd7st2m9-algo-1-w67e8 |\u001b[0m Epoch 0:  36%|███▋      | 59/162 [00:15<00:26,  3.82it/s, loss=153, v_num=0, train_loss_step=148.0]\n",
      "\u001b[36mcucd7st2m9-algo-1-w67e8 |\u001b[0m Epoch 0:  37%|███▋      | 60/162 [00:15<00:26,  3.82it/s, loss=153, v_num=0, train_loss_step=148.0]\n",
      "\u001b[36mcucd7st2m9-algo-1-w67e8 |\u001b[0m Epoch 0:  37%|███▋      | 60/162 [00:15<00:26,  3.82it/s, loss=152, v_num=0, train_loss_step=134.0]\n",
      "\u001b[36mcucd7st2m9-algo-1-w67e8 |\u001b[0m Epoch 0:  38%|███▊      | 61/162 [00:15<00:26,  3.83it/s, loss=152, v_num=0, train_loss_step=134.0]\n",
      "\u001b[36mcucd7st2m9-algo-1-w67e8 |\u001b[0m Epoch 0:  38%|███▊      | 61/162 [00:15<00:26,  3.83it/s, loss=150, v_num=0, train_loss_step=140.0]\n",
      "\u001b[36mcucd7st2m9-algo-1-w67e8 |\u001b[0m Epoch 0:  38%|███▊      | 62/162 [00:16<00:26,  3.83it/s, loss=150, v_num=0, train_loss_step=140.0]\n",
      "\u001b[36mcucd7st2m9-algo-1-w67e8 |\u001b[0m Epoch 0:  38%|███▊      | 62/162 [00:16<00:26,  3.83it/s, loss=147, v_num=0, train_loss_step=101.0]\n",
      "\u001b[36mcucd7st2m9-algo-1-w67e8 |\u001b[0m Epoch 0:  39%|███▉      | 63/162 [00:16<00:25,  3.83it/s, loss=147, v_num=0, train_loss_step=101.0]\n",
      "\u001b[36mcucd7st2m9-algo-1-w67e8 |\u001b[0m Epoch 0:  39%|███▉      | 63/162 [00:16<00:25,  3.83it/s, loss=147, v_num=0, train_loss_step=138.0]\n",
      "\u001b[36mcucd7st2m9-algo-1-w67e8 |\u001b[0m Epoch 0:  40%|███▉      | 64/162 [00:16<00:25,  3.83it/s, loss=147, v_num=0, train_loss_step=138.0]\n",
      "\u001b[36mcucd7st2m9-algo-1-w67e8 |\u001b[0m Epoch 0:  40%|███▉      | 64/162 [00:16<00:25,  3.83it/s, loss=147, v_num=0, train_loss_step=147.0]\n",
      "\u001b[36mcucd7st2m9-algo-1-w67e8 |\u001b[0m Epoch 0:  40%|████      | 65/162 [00:16<00:25,  3.83it/s, loss=147, v_num=0, train_loss_step=147.0]\n",
      "\u001b[36mcucd7st2m9-algo-1-w67e8 |\u001b[0m Epoch 0:  40%|████      | 65/162 [00:16<00:25,  3.83it/s, loss=147, v_num=0, train_loss_step=115.0]\n",
      "\u001b[36mcucd7st2m9-algo-1-w67e8 |\u001b[0m Epoch 0:  41%|████      | 66/162 [00:17<00:25,  3.83it/s, loss=147, v_num=0, train_loss_step=115.0]\n",
      "\u001b[36mcucd7st2m9-algo-1-w67e8 |\u001b[0m Epoch 0:  41%|████      | 66/162 [00:17<00:25,  3.83it/s, loss=149, v_num=0, train_loss_step=173.0]\n",
      "\u001b[36mcucd7st2m9-algo-1-w67e8 |\u001b[0m Epoch 0:  41%|████▏     | 67/162 [00:17<00:24,  3.83it/s, loss=149, v_num=0, train_loss_step=173.0]\n",
      "\u001b[36mcucd7st2m9-algo-1-w67e8 |\u001b[0m Epoch 0:  41%|████▏     | 67/162 [00:17<00:24,  3.83it/s, loss=149, v_num=0, train_loss_step=178.0]\n",
      "\u001b[36mcucd7st2m9-algo-1-w67e8 |\u001b[0m Epoch 0:  42%|████▏     | 68/162 [00:17<00:24,  3.83it/s, loss=149, v_num=0, train_loss_step=178.0]\n",
      "\u001b[36mcucd7st2m9-algo-1-w67e8 |\u001b[0m Epoch 0:  42%|████▏     | 68/162 [00:17<00:24,  3.83it/s, loss=150, v_num=0, train_loss_step=147.0]\n",
      "\u001b[36mcucd7st2m9-algo-1-w67e8 |\u001b[0m Epoch 0:  43%|████▎     | 69/162 [00:17<00:24,  3.84it/s, loss=150, v_num=0, train_loss_step=147.0]\n",
      "\u001b[36mcucd7st2m9-algo-1-w67e8 |\u001b[0m Epoch 0:  43%|████▎     | 69/162 [00:17<00:24,  3.84it/s, loss=151, v_num=0, train_loss_step=172.0]\n",
      "\u001b[36mcucd7st2m9-algo-1-w67e8 |\u001b[0m Epoch 0:  43%|████▎     | 70/162 [00:18<00:24,  3.83it/s, loss=151, v_num=0, train_loss_step=172.0]\n",
      "\u001b[36mcucd7st2m9-algo-1-w67e8 |\u001b[0m Epoch 0:  43%|████▎     | 70/162 [00:18<00:24,  3.83it/s, loss=148, v_num=0, train_loss_step=139.0]\n",
      "\u001b[36mcucd7st2m9-algo-1-w67e8 |\u001b[0m Epoch 0:  44%|████▍     | 71/162 [00:18<00:23,  3.83it/s, loss=148, v_num=0, train_loss_step=139.0]\n",
      "\u001b[36mcucd7st2m9-algo-1-w67e8 |\u001b[0m Epoch 0:  44%|████▍     | 71/162 [00:18<00:23,  3.83it/s, loss=148, v_num=0, train_loss_step=128.0]\n",
      "\u001b[36mcucd7st2m9-algo-1-w67e8 |\u001b[0m Epoch 0:  44%|████▍     | 72/162 [00:18<00:23,  3.83it/s, loss=148, v_num=0, train_loss_step=128.0]\n",
      "\u001b[36mcucd7st2m9-algo-1-w67e8 |\u001b[0m Epoch 0:  44%|████▍     | 72/162 [00:18<00:23,  3.83it/s, loss=148, v_num=0, train_loss_step=136.0]\n",
      "\u001b[36mcucd7st2m9-algo-1-w67e8 |\u001b[0m Epoch 0:  45%|████▌     | 73/162 [00:19<00:23,  3.83it/s, loss=148, v_num=0, train_loss_step=136.0]\n",
      "\u001b[36mcucd7st2m9-algo-1-w67e8 |\u001b[0m Epoch 0:  45%|████▌     | 73/162 [00:19<00:23,  3.83it/s, loss=147, v_num=0, train_loss_step=151.0]\n",
      "\u001b[36mcucd7st2m9-algo-1-w67e8 |\u001b[0m Epoch 0:  46%|████▌     | 74/162 [00:19<00:22,  3.83it/s, loss=147, v_num=0, train_loss_step=151.0]\n",
      "\u001b[36mcucd7st2m9-algo-1-w67e8 |\u001b[0m Epoch 0:  46%|████▌     | 74/162 [00:19<00:22,  3.83it/s, loss=143, v_num=0, train_loss_step=121.0]\n",
      "\u001b[36mcucd7st2m9-algo-1-w67e8 |\u001b[0m Epoch 0:  46%|████▋     | 75/162 [00:19<00:22,  3.84it/s, loss=143, v_num=0, train_loss_step=121.0]\n",
      "\u001b[36mcucd7st2m9-algo-1-w67e8 |\u001b[0m Epoch 0:  46%|████▋     | 75/162 [00:19<00:22,  3.84it/s, loss=142, v_num=0, train_loss_step=151.0]\n",
      "\u001b[36mcucd7st2m9-algo-1-w67e8 |\u001b[0m Epoch 0:  47%|████▋     | 76/162 [00:19<00:22,  3.84it/s, loss=142, v_num=0, train_loss_step=151.0]\n",
      "\u001b[36mcucd7st2m9-algo-1-w67e8 |\u001b[0m Epoch 0:  47%|████▋     | 76/162 [00:19<00:22,  3.84it/s, loss=145, v_num=0, train_loss_step=195.0]\n",
      "\u001b[36mcucd7st2m9-algo-1-w67e8 |\u001b[0m Epoch 0:  48%|████▊     | 77/162 [00:20<00:22,  3.83it/s, loss=145, v_num=0, train_loss_step=195.0]\n",
      "\u001b[36mcucd7st2m9-algo-1-w67e8 |\u001b[0m Epoch 0:  48%|████▊     | 77/162 [00:20<00:22,  3.83it/s, loss=145, v_num=0, train_loss_step=132.0]\n",
      "\u001b[36mcucd7st2m9-algo-1-w67e8 |\u001b[0m Epoch 0:  48%|████▊     | 78/162 [00:20<00:21,  3.83it/s, loss=145, v_num=0, train_loss_step=132.0]\n",
      "\u001b[36mcucd7st2m9-algo-1-w67e8 |\u001b[0m Epoch 0:  48%|████▊     | 78/162 [00:20<00:21,  3.83it/s, loss=145, v_num=0, train_loss_step=143.0]\n",
      "\u001b[36mcucd7st2m9-algo-1-w67e8 |\u001b[0m Epoch 0:  49%|████▉     | 79/162 [00:20<00:21,  3.80it/s, loss=145, v_num=0, train_loss_step=143.0]\n",
      "\u001b[36mcucd7st2m9-algo-1-w67e8 |\u001b[0m Epoch 0:  49%|████▉     | 79/162 [00:20<00:21,  3.80it/s, loss=145, v_num=0, train_loss_step=149.0]\n",
      "\u001b[36mcucd7st2m9-algo-1-w67e8 |\u001b[0m Epoch 0:  49%|████▉     | 80/162 [00:21<00:21,  3.80it/s, loss=145, v_num=0, train_loss_step=149.0]\n",
      "\u001b[36mcucd7st2m9-algo-1-w67e8 |\u001b[0m Epoch 0:  49%|████▉     | 80/162 [00:21<00:21,  3.80it/s, loss=145, v_num=0, train_loss_step=136.0]\n",
      "\u001b[36mcucd7st2m9-algo-1-w67e8 |\u001b[0m Epoch 0:  50%|█████     | 81/162 [00:21<00:21,  3.80it/s, loss=145, v_num=0, train_loss_step=136.0]\n",
      "\u001b[36mcucd7st2m9-algo-1-w67e8 |\u001b[0m Epoch 0:  50%|█████     | 81/162 [00:21<00:21,  3.80it/s, loss=144, v_num=0, train_loss_step=120.0]\n",
      "\u001b[36mcucd7st2m9-algo-1-w67e8 |\u001b[0m Epoch 0:  51%|█████     | 82/162 [00:21<00:21,  3.80it/s, loss=144, v_num=0, train_loss_step=120.0]\n",
      "\u001b[36mcucd7st2m9-algo-1-w67e8 |\u001b[0m Epoch 0:  51%|█████     | 82/162 [00:21<00:21,  3.80it/s, loss=146, v_num=0, train_loss_step=142.0]\n",
      "\u001b[36mcucd7st2m9-algo-1-w67e8 |\u001b[0m Epoch 0:  51%|█████     | 83/162 [00:21<00:20,  3.80it/s, loss=146, v_num=0, train_loss_step=142.0]\n",
      "\u001b[36mcucd7st2m9-algo-1-w67e8 |\u001b[0m Epoch 0:  51%|█████     | 83/162 [00:21<00:20,  3.80it/s, loss=145, v_num=0, train_loss_step=126.0]\n",
      "\u001b[36mcucd7st2m9-algo-1-w67e8 |\u001b[0m Epoch 0:  52%|█████▏    | 84/162 [00:22<00:20,  3.80it/s, loss=145, v_num=0, train_loss_step=126.0]\n",
      "\u001b[36mcucd7st2m9-algo-1-w67e8 |\u001b[0m Epoch 0:  52%|█████▏    | 84/162 [00:22<00:20,  3.80it/s, loss=145, v_num=0, train_loss_step=136.0]\n",
      "\u001b[36mcucd7st2m9-algo-1-w67e8 |\u001b[0m Epoch 0:  52%|█████▏    | 85/162 [00:22<00:20,  3.81it/s, loss=145, v_num=0, train_loss_step=136.0]\n",
      "\u001b[36mcucd7st2m9-algo-1-w67e8 |\u001b[0m Epoch 0:  52%|█████▏    | 85/162 [00:22<00:20,  3.80it/s, loss=146, v_num=0, train_loss_step=139.0]\n",
      "\u001b[36mcucd7st2m9-algo-1-w67e8 |\u001b[0m Epoch 0:  53%|█████▎    | 86/162 [00:22<00:19,  3.81it/s, loss=146, v_num=0, train_loss_step=139.0]\n",
      "\u001b[36mcucd7st2m9-algo-1-w67e8 |\u001b[0m Epoch 0:  53%|█████▎    | 86/162 [00:22<00:19,  3.81it/s, loss=144, v_num=0, train_loss_step=133.0]\n",
      "\u001b[36mcucd7st2m9-algo-1-w67e8 |\u001b[0m Epoch 0:  54%|█████▎    | 87/162 [00:22<00:19,  3.81it/s, loss=144, v_num=0, train_loss_step=133.0]\n",
      "\u001b[36mcucd7st2m9-algo-1-w67e8 |\u001b[0m Epoch 0:  54%|█████▎    | 87/162 [00:22<00:19,  3.81it/s, loss=142, v_num=0, train_loss_step=134.0]\n",
      "\u001b[36mcucd7st2m9-algo-1-w67e8 |\u001b[0m Epoch 0:  54%|█████▍    | 88/162 [00:23<00:19,  3.81it/s, loss=142, v_num=0, train_loss_step=134.0]\n",
      "\u001b[36mcucd7st2m9-algo-1-w67e8 |\u001b[0m Epoch 0:  54%|█████▍    | 88/162 [00:23<00:19,  3.81it/s, loss=141, v_num=0, train_loss_step=126.0]\n",
      "\u001b[36mcucd7st2m9-algo-1-w67e8 |\u001b[0m Epoch 0:  55%|█████▍    | 89/162 [00:23<00:19,  3.81it/s, loss=141, v_num=0, train_loss_step=126.0]\n",
      "\u001b[36mcucd7st2m9-algo-1-w67e8 |\u001b[0m Epoch 0:  55%|█████▍    | 89/162 [00:23<00:19,  3.81it/s, loss=139, v_num=0, train_loss_step=132.0]\n",
      "\u001b[36mcucd7st2m9-algo-1-w67e8 |\u001b[0m Epoch 0:  56%|█████▌    | 90/162 [00:23<00:18,  3.81it/s, loss=139, v_num=0, train_loss_step=132.0]\n",
      "\u001b[36mcucd7st2m9-algo-1-w67e8 |\u001b[0m Epoch 0:  56%|█████▌    | 90/162 [00:23<00:18,  3.81it/s, loss=140, v_num=0, train_loss_step=170.0]\n",
      "\u001b[36mcucd7st2m9-algo-1-w67e8 |\u001b[0m Epoch 0:  56%|█████▌    | 91/162 [00:23<00:18,  3.81it/s, loss=140, v_num=0, train_loss_step=170.0]\n",
      "\u001b[36mcucd7st2m9-algo-1-w67e8 |\u001b[0m Epoch 0:  56%|█████▌    | 91/162 [00:23<00:18,  3.81it/s, loss=140, v_num=0, train_loss_step=120.0]\n",
      "\u001b[36mcucd7st2m9-algo-1-w67e8 |\u001b[0m Epoch 0:  57%|█████▋    | 92/162 [00:24<00:18,  3.81it/s, loss=140, v_num=0, train_loss_step=120.0]\n",
      "\u001b[36mcucd7st2m9-algo-1-w67e8 |\u001b[0m Epoch 0:  57%|█████▋    | 92/162 [00:24<00:18,  3.81it/s, loss=141, v_num=0, train_loss_step=153.0]\n",
      "\u001b[36mcucd7st2m9-algo-1-w67e8 |\u001b[0m Epoch 0:  57%|█████▋    | 93/162 [00:24<00:18,  3.81it/s, loss=141, v_num=0, train_loss_step=153.0]\n",
      "\u001b[36mcucd7st2m9-algo-1-w67e8 |\u001b[0m Epoch 0:  57%|█████▋    | 93/162 [00:24<00:18,  3.81it/s, loss=140, v_num=0, train_loss_step=141.0]\n",
      "\u001b[36mcucd7st2m9-algo-1-w67e8 |\u001b[0m Epoch 0:  58%|█████▊    | 94/162 [00:24<00:17,  3.81it/s, loss=140, v_num=0, train_loss_step=141.0]\n",
      "\u001b[36mcucd7st2m9-algo-1-w67e8 |\u001b[0m Epoch 0:  58%|█████▊    | 94/162 [00:24<00:17,  3.81it/s, loss=141, v_num=0, train_loss_step=142.0]\n",
      "\u001b[36mcucd7st2m9-algo-1-w67e8 |\u001b[0m Epoch 0:  59%|█████▊    | 95/162 [00:24<00:17,  3.81it/s, loss=141, v_num=0, train_loss_step=142.0]\n",
      "\u001b[36mcucd7st2m9-algo-1-w67e8 |\u001b[0m Epoch 0:  59%|█████▊    | 95/162 [00:24<00:17,  3.81it/s, loss=140, v_num=0, train_loss_step=127.0]\n",
      "\u001b[36mcucd7st2m9-algo-1-w67e8 |\u001b[0m Epoch 0:  59%|█████▉    | 96/162 [00:25<00:17,  3.81it/s, loss=140, v_num=0, train_loss_step=127.0]\n",
      "\u001b[36mcucd7st2m9-algo-1-w67e8 |\u001b[0m Epoch 0:  59%|█████▉    | 96/162 [00:25<00:17,  3.80it/s, loss=137, v_num=0, train_loss_step=140.0]\n",
      "\u001b[36mcucd7st2m9-algo-1-w67e8 |\u001b[0m Epoch 0:  60%|█████▉    | 97/162 [00:25<00:17,  3.81it/s, loss=137, v_num=0, train_loss_step=140.0]\n",
      "\u001b[36mcucd7st2m9-algo-1-w67e8 |\u001b[0m Epoch 0:  60%|█████▉    | 97/162 [00:25<00:17,  3.81it/s, loss=138, v_num=0, train_loss_step=147.0]\n",
      "\u001b[36mcucd7st2m9-algo-1-w67e8 |\u001b[0m Epoch 0:  60%|██████    | 98/162 [00:25<00:16,  3.80it/s, loss=138, v_num=0, train_loss_step=147.0]\n",
      "\u001b[36mcucd7st2m9-algo-1-w67e8 |\u001b[0m Epoch 0:  60%|██████    | 98/162 [00:25<00:16,  3.80it/s, loss=137, v_num=0, train_loss_step=127.0]\n",
      "\u001b[36mcucd7st2m9-algo-1-w67e8 |\u001b[0m Epoch 0:  61%|██████    | 99/162 [00:26<00:16,  3.80it/s, loss=137, v_num=0, train_loss_step=127.0]\n",
      "\u001b[36mcucd7st2m9-algo-1-w67e8 |\u001b[0m Epoch 0:  61%|██████    | 99/162 [00:26<00:16,  3.80it/s, loss=137, v_num=0, train_loss_step=144.0]\n",
      "\u001b[36mcucd7st2m9-algo-1-w67e8 |\u001b[0m Epoch 0:  62%|██████▏   | 100/162 [00:26<00:16,  3.80it/s, loss=137, v_num=0, train_loss_step=144.0]\n",
      "\u001b[36mcucd7st2m9-algo-1-w67e8 |\u001b[0m Epoch 0:  62%|██████▏   | 100/162 [00:26<00:16,  3.80it/s, loss=136, v_num=0, train_loss_step=126.0]\n",
      "\u001b[36mcucd7st2m9-algo-1-w67e8 |\u001b[0m Epoch 0:  62%|██████▏   | 101/162 [00:26<00:16,  3.80it/s, loss=136, v_num=0, train_loss_step=126.0]\n",
      "\u001b[36mcucd7st2m9-algo-1-w67e8 |\u001b[0m Epoch 0:  62%|██████▏   | 101/162 [00:26<00:16,  3.80it/s, loss=136, v_num=0, train_loss_step=120.0]\n",
      "\u001b[36mcucd7st2m9-algo-1-w67e8 |\u001b[0m Epoch 0:  63%|██████▎   | 102/162 [00:26<00:15,  3.79it/s, loss=136, v_num=0, train_loss_step=120.0]\n",
      "\u001b[36mcucd7st2m9-algo-1-w67e8 |\u001b[0m Epoch 0:  63%|██████▎   | 102/162 [00:26<00:15,  3.79it/s, loss=135, v_num=0, train_loss_step=127.0]\n",
      "\u001b[36mcucd7st2m9-algo-1-w67e8 |\u001b[0m Epoch 0:  64%|██████▎   | 103/162 [00:27<00:15,  3.80it/s, loss=135, v_num=0, train_loss_step=127.0]\n",
      "\u001b[36mcucd7st2m9-algo-1-w67e8 |\u001b[0m Epoch 0:  64%|██████▎   | 103/162 [00:27<00:15,  3.80it/s, loss=138, v_num=0, train_loss_step=171.0]\n",
      "\u001b[36mcucd7st2m9-algo-1-w67e8 |\u001b[0m Epoch 0:  64%|██████▍   | 104/162 [00:27<00:15,  3.80it/s, loss=138, v_num=0, train_loss_step=171.0]\n",
      "\u001b[36mcucd7st2m9-algo-1-w67e8 |\u001b[0m Epoch 0:  64%|██████▍   | 104/162 [00:27<00:15,  3.80it/s, loss=138, v_num=0, train_loss_step=139.0]\n",
      "\u001b[36mcucd7st2m9-algo-1-w67e8 |\u001b[0m Epoch 0:  65%|██████▍   | 105/162 [00:27<00:15,  3.79it/s, loss=138, v_num=0, train_loss_step=139.0]\n",
      "\u001b[36mcucd7st2m9-algo-1-w67e8 |\u001b[0m Epoch 0:  65%|██████▍   | 105/162 [00:27<00:15,  3.79it/s, loss=137, v_num=0, train_loss_step=129.0]\n",
      "\u001b[36mcucd7st2m9-algo-1-w67e8 |\u001b[0m Epoch 0:  65%|██████▌   | 106/162 [00:27<00:14,  3.79it/s, loss=137, v_num=0, train_loss_step=129.0]\n",
      "\u001b[36mcucd7st2m9-algo-1-w67e8 |\u001b[0m Epoch 0:  65%|██████▌   | 106/162 [00:27<00:14,  3.79it/s, loss=141, v_num=0, train_loss_step=207.0]\n",
      "\u001b[36mcucd7st2m9-algo-1-w67e8 |\u001b[0m Epoch 0:  66%|██████▌   | 107/162 [00:28<00:14,  3.79it/s, loss=141, v_num=0, train_loss_step=207.0]\n",
      "\u001b[36mcucd7st2m9-algo-1-w67e8 |\u001b[0m Epoch 0:  66%|██████▌   | 107/162 [00:28<00:14,  3.79it/s, loss=142, v_num=0, train_loss_step=154.0]\n",
      "\u001b[36mcucd7st2m9-algo-1-w67e8 |\u001b[0m Epoch 0:  67%|██████▋   | 108/162 [00:28<00:14,  3.79it/s, loss=142, v_num=0, train_loss_step=154.0]\n",
      "\u001b[36mcucd7st2m9-algo-1-w67e8 |\u001b[0m Epoch 0:  67%|██████▋   | 108/162 [00:28<00:14,  3.79it/s, loss=144, v_num=0, train_loss_step=161.0]\n",
      "\u001b[36mcucd7st2m9-algo-1-w67e8 |\u001b[0m Epoch 0:  67%|██████▋   | 109/162 [00:28<00:13,  3.80it/s, loss=144, v_num=0, train_loss_step=161.0]\n",
      "\u001b[36mcucd7st2m9-algo-1-w67e8 |\u001b[0m Epoch 0:  67%|██████▋   | 109/162 [00:28<00:13,  3.80it/s, loss=143, v_num=0, train_loss_step=121.0]\n",
      "\u001b[36mcucd7st2m9-algo-1-w67e8 |\u001b[0m Epoch 0:  68%|██████▊   | 110/162 [00:28<00:13,  3.80it/s, loss=143, v_num=0, train_loss_step=121.0]\n",
      "\u001b[36mcucd7st2m9-algo-1-w67e8 |\u001b[0m Epoch 0:  68%|██████▊   | 110/162 [00:28<00:13,  3.80it/s, loss=142, v_num=0, train_loss_step=150.0]\n",
      "\u001b[36mcucd7st2m9-algo-1-w67e8 |\u001b[0m Epoch 0:  69%|██████▊   | 111/162 [00:29<00:13,  3.80it/s, loss=142, v_num=0, train_loss_step=150.0]\n",
      "\u001b[36mcucd7st2m9-algo-1-w67e8 |\u001b[0m Epoch 0:  69%|██████▊   | 111/162 [00:29<00:13,  3.80it/s, loss=144, v_num=0, train_loss_step=150.0]\n",
      "\u001b[36mcucd7st2m9-algo-1-w67e8 |\u001b[0m Epoch 0:  69%|██████▉   | 112/162 [00:29<00:13,  3.80it/s, loss=144, v_num=0, train_loss_step=150.0]\n",
      "\u001b[36mcucd7st2m9-algo-1-w67e8 |\u001b[0m Epoch 0:  69%|██████▉   | 112/162 [00:29<00:13,  3.80it/s, loss=143, v_num=0, train_loss_step=128.0]\n",
      "\u001b[36mcucd7st2m9-algo-1-w67e8 |\u001b[0m Epoch 0:  70%|██████▉   | 113/162 [00:29<00:12,  3.80it/s, loss=143, v_num=0, train_loss_step=128.0]\n",
      "\u001b[36mcucd7st2m9-algo-1-w67e8 |\u001b[0m Epoch 0:  70%|██████▉   | 113/162 [00:29<00:12,  3.80it/s, loss=143, v_num=0, train_loss_step=159.0]\n",
      "\u001b[36mcucd7st2m9-algo-1-w67e8 |\u001b[0m Epoch 0:  70%|███████   | 114/162 [00:30<00:12,  3.80it/s, loss=143, v_num=0, train_loss_step=159.0]\n",
      "\u001b[36mcucd7st2m9-algo-1-w67e8 |\u001b[0m Epoch 0:  70%|███████   | 114/162 [00:30<00:12,  3.80it/s, loss=143, v_num=0, train_loss_step=130.0]\n",
      "\u001b[36mcucd7st2m9-algo-1-w67e8 |\u001b[0m Epoch 0:  71%|███████   | 115/162 [00:30<00:12,  3.80it/s, loss=143, v_num=0, train_loss_step=130.0]\n",
      "\u001b[36mcucd7st2m9-algo-1-w67e8 |\u001b[0m Epoch 0:  71%|███████   | 115/162 [00:30<00:12,  3.80it/s, loss=144, v_num=0, train_loss_step=152.0]\n",
      "\u001b[36mcucd7st2m9-algo-1-w67e8 |\u001b[0m Epoch 0:  72%|███████▏  | 116/162 [00:30<00:12,  3.80it/s, loss=144, v_num=0, train_loss_step=152.0]\n",
      "\u001b[36mcucd7st2m9-algo-1-w67e8 |\u001b[0m Epoch 0:  72%|███████▏  | 116/162 [00:30<00:12,  3.80it/s, loss=143, v_num=0, train_loss_step=115.0]\n",
      "\u001b[36mcucd7st2m9-algo-1-w67e8 |\u001b[0m Epoch 0:  72%|███████▏  | 117/162 [00:30<00:11,  3.80it/s, loss=143, v_num=0, train_loss_step=115.0]\n",
      "\u001b[36mcucd7st2m9-algo-1-w67e8 |\u001b[0m Epoch 0:  72%|███████▏  | 117/162 [00:30<00:11,  3.80it/s, loss=142, v_num=0, train_loss_step=134.0]\n",
      "\u001b[36mcucd7st2m9-algo-1-w67e8 |\u001b[0m Epoch 0:  73%|███████▎  | 118/162 [00:31<00:11,  3.80it/s, loss=142, v_num=0, train_loss_step=134.0]\n",
      "\u001b[36mcucd7st2m9-algo-1-w67e8 |\u001b[0m Epoch 0:  73%|███████▎  | 118/162 [00:31<00:11,  3.80it/s, loss=143, v_num=0, train_loss_step=140.0]\n",
      "\u001b[36mcucd7st2m9-algo-1-w67e8 |\u001b[0m Epoch 0:  73%|███████▎  | 119/162 [00:31<00:11,  3.80it/s, loss=143, v_num=0, train_loss_step=140.0]\n",
      "\u001b[36mcucd7st2m9-algo-1-w67e8 |\u001b[0m Epoch 0:  73%|███████▎  | 119/162 [00:31<00:11,  3.80it/s, loss=142, v_num=0, train_loss_step=131.0]\n",
      "\u001b[36mcucd7st2m9-algo-1-w67e8 |\u001b[0m Epoch 0:  74%|███████▍  | 120/162 [00:31<00:11,  3.80it/s, loss=142, v_num=0, train_loss_step=131.0]\n",
      "\u001b[36mcucd7st2m9-algo-1-w67e8 |\u001b[0m Epoch 0:  74%|███████▍  | 120/162 [00:31<00:11,  3.80it/s, loss=141, v_num=0, train_loss_step=97.90]\n",
      "\u001b[36mcucd7st2m9-algo-1-w67e8 |\u001b[0m Epoch 0:  75%|███████▍  | 121/162 [00:31<00:10,  3.80it/s, loss=141, v_num=0, train_loss_step=97.90]\n",
      "\u001b[36mcucd7st2m9-algo-1-w67e8 |\u001b[0m Epoch 0:  75%|███████▍  | 121/162 [00:31<00:10,  3.80it/s, loss=140, v_num=0, train_loss_step=97.20]\n",
      "\u001b[36mcucd7st2m9-algo-1-w67e8 |\u001b[0m Epoch 0:  75%|███████▌  | 122/162 [00:32<00:10,  3.80it/s, loss=140, v_num=0, train_loss_step=97.20]\n",
      "\u001b[36mcucd7st2m9-algo-1-w67e8 |\u001b[0m Epoch 0:  75%|███████▌  | 122/162 [00:32<00:10,  3.80it/s, loss=141, v_num=0, train_loss_step=153.0]\n",
      "\u001b[36mcucd7st2m9-algo-1-w67e8 |\u001b[0m Epoch 0:  76%|███████▌  | 123/162 [00:32<00:10,  3.80it/s, loss=141, v_num=0, train_loss_step=153.0]\n",
      "\u001b[36mcucd7st2m9-algo-1-w67e8 |\u001b[0m Epoch 0:  76%|███████▌  | 123/162 [00:32<00:10,  3.80it/s, loss=141, v_num=0, train_loss_step=177.0]\n",
      "\u001b[36mcucd7st2m9-algo-1-w67e8 |\u001b[0m Epoch 0:  77%|███████▋  | 124/162 [00:32<00:10,  3.80it/s, loss=141, v_num=0, train_loss_step=177.0]\n",
      "\u001b[36mcucd7st2m9-algo-1-w67e8 |\u001b[0m Epoch 0:  77%|███████▋  | 124/162 [00:32<00:10,  3.80it/s, loss=140, v_num=0, train_loss_step=115.0]\n",
      "\u001b[36mcucd7st2m9-algo-1-w67e8 |\u001b[0m Epoch 0:  77%|███████▋  | 125/162 [00:32<00:09,  3.80it/s, loss=140, v_num=0, train_loss_step=115.0]\n",
      "\u001b[36mcucd7st2m9-algo-1-w67e8 |\u001b[0m Epoch 0:  77%|███████▋  | 125/162 [00:32<00:09,  3.80it/s, loss=141, v_num=0, train_loss_step=153.0]\n",
      "\u001b[36mcucd7st2m9-algo-1-w67e8 |\u001b[0m Epoch 0:  78%|███████▊  | 126/162 [00:33<00:09,  3.80it/s, loss=141, v_num=0, train_loss_step=153.0]\n",
      "\u001b[36mcucd7st2m9-algo-1-w67e8 |\u001b[0m Epoch 0:  78%|███████▊  | 126/162 [00:33<00:09,  3.80it/s, loss=139, v_num=0, train_loss_step=159.0]\n",
      "\u001b[36mcucd7st2m9-algo-1-w67e8 |\u001b[0m Epoch 0:  78%|███████▊  | 127/162 [00:33<00:09,  3.80it/s, loss=139, v_num=0, train_loss_step=159.0]\n",
      "\u001b[36mcucd7st2m9-algo-1-w67e8 |\u001b[0m Epoch 0:  78%|███████▊  | 127/162 [00:33<00:09,  3.80it/s, loss=138, v_num=0, train_loss_step=129.0]\n",
      "\u001b[36mcucd7st2m9-algo-1-w67e8 |\u001b[0m Epoch 0:  79%|███████▉  | 128/162 [00:33<00:08,  3.80it/s, loss=138, v_num=0, train_loss_step=129.0]\n",
      "\u001b[36mcucd7st2m9-algo-1-w67e8 |\u001b[0m Epoch 0:  79%|███████▉  | 128/162 [00:33<00:08,  3.80it/s, loss=139, v_num=0, train_loss_step=183.0]\n",
      "\u001b[36mcucd7st2m9-algo-1-w67e8 |\u001b[0m Epoch 0:  80%|███████▉  | 129/162 [00:33<00:08,  3.80it/s, loss=139, v_num=0, train_loss_step=183.0]\n",
      "\u001b[36mcucd7st2m9-algo-1-w67e8 |\u001b[0m Epoch 0:  80%|███████▉  | 129/162 [00:33<00:08,  3.80it/s, loss=141, v_num=0, train_loss_step=171.0]\n",
      "\u001b[36mcucd7st2m9-algo-1-w67e8 |\u001b[0m Epoch 0:  80%|████████  | 130/162 [00:34<00:08,  3.80it/s, loss=141, v_num=0, train_loss_step=171.0]\n",
      "\u001b[36mcucd7st2m9-algo-1-w67e8 |\u001b[0m Epoch 0:  80%|████████  | 130/162 [00:34<00:08,  3.80it/s, loss=142, v_num=0, train_loss_step=160.0]\n",
      "\u001b[36mcucd7st2m9-algo-1-w67e8 |\u001b[0m Epoch 0:  81%|████████  | 131/162 [00:34<00:08,  3.80it/s, loss=142, v_num=0, train_loss_step=160.0]\n",
      "\u001b[36mcucd7st2m9-algo-1-w67e8 |\u001b[0m Epoch 0:  81%|████████  | 131/162 [00:34<00:08,  3.80it/s, loss=141, v_num=0, train_loss_step=138.0]\n",
      "\u001b[36mcucd7st2m9-algo-1-w67e8 |\u001b[0m Epoch 0:  81%|████████▏ | 132/162 [00:34<00:07,  3.80it/s, loss=141, v_num=0, train_loss_step=138.0]\n",
      "\u001b[36mcucd7st2m9-algo-1-w67e8 |\u001b[0m Epoch 0:  81%|████████▏ | 132/162 [00:34<00:07,  3.80it/s, loss=142, v_num=0, train_loss_step=135.0]\n",
      "\u001b[36mcucd7st2m9-algo-1-w67e8 |\u001b[0m Epoch 0:  82%|████████▏ | 133/162 [00:34<00:07,  3.80it/s, loss=142, v_num=0, train_loss_step=135.0]\n",
      "\u001b[36mcucd7st2m9-algo-1-w67e8 |\u001b[0m Epoch 0:  82%|████████▏ | 133/162 [00:34<00:07,  3.80it/s, loss=140, v_num=0, train_loss_step=122.0]\n",
      "\u001b[36mcucd7st2m9-algo-1-w67e8 |\u001b[0m Epoch 0:  83%|████████▎ | 134/162 [00:35<00:07,  3.81it/s, loss=140, v_num=0, train_loss_step=122.0]\n",
      "\u001b[36mcucd7st2m9-algo-1-w67e8 |\u001b[0m Epoch 0:  83%|████████▎ | 134/162 [00:35<00:07,  3.81it/s, loss=139, v_num=0, train_loss_step=110.0]\n",
      "\u001b[36mcucd7st2m9-algo-1-w67e8 |\u001b[0m Epoch 0:  83%|████████▎ | 135/162 [00:35<00:07,  3.80it/s, loss=139, v_num=0, train_loss_step=110.0]\n",
      "\u001b[36mcucd7st2m9-algo-1-w67e8 |\u001b[0m Epoch 0:  83%|████████▎ | 135/162 [00:35<00:07,  3.80it/s, loss=138, v_num=0, train_loss_step=144.0]\n",
      "\u001b[36mcucd7st2m9-algo-1-w67e8 |\u001b[0m Epoch 0:  84%|████████▍ | 136/162 [00:35<00:06,  3.80it/s, loss=138, v_num=0, train_loss_step=144.0]\n",
      "\u001b[36mcucd7st2m9-algo-1-w67e8 |\u001b[0m Epoch 0:  84%|████████▍ | 136/162 [00:35<00:06,  3.80it/s, loss=140, v_num=0, train_loss_step=154.0]\n",
      "\u001b[36mcucd7st2m9-algo-1-w67e8 |\u001b[0m Epoch 0:  85%|████████▍ | 137/162 [00:36<00:06,  3.80it/s, loss=140, v_num=0, train_loss_step=154.0]\n",
      "\u001b[36mcucd7st2m9-algo-1-w67e8 |\u001b[0m Epoch 0:  85%|████████▍ | 137/162 [00:36<00:06,  3.80it/s, loss=141, v_num=0, train_loss_step=147.0]\n",
      "\u001b[36mcucd7st2m9-algo-1-w67e8 |\u001b[0m Epoch 0:  85%|████████▌ | 138/162 [00:36<00:06,  3.80it/s, loss=141, v_num=0, train_loss_step=147.0]\n",
      "\u001b[36mcucd7st2m9-algo-1-w67e8 |\u001b[0m Epoch 0:  85%|████████▌ | 138/162 [00:36<00:06,  3.80it/s, loss=140, v_num=0, train_loss_step=131.0]\n",
      "\u001b[36mcucd7st2m9-algo-1-w67e8 |\u001b[0m Epoch 0:  86%|████████▌ | 139/162 [00:36<00:06,  3.80it/s, loss=140, v_num=0, train_loss_step=131.0]\n",
      "\u001b[36mcucd7st2m9-algo-1-w67e8 |\u001b[0m Epoch 0:  86%|████████▌ | 139/162 [00:36<00:06,  3.80it/s, loss=140, v_num=0, train_loss_step=114.0]\n",
      "\u001b[36mcucd7st2m9-algo-1-w67e8 |\u001b[0m Epoch 0:  86%|████████▋ | 140/162 [00:36<00:05,  3.80it/s, loss=140, v_num=0, train_loss_step=114.0]\n",
      "\u001b[36mcucd7st2m9-algo-1-w67e8 |\u001b[0m Epoch 0:  86%|████████▋ | 140/162 [00:36<00:05,  3.80it/s, loss=142, v_num=0, train_loss_step=142.0]\n",
      "\u001b[36mcucd7st2m9-algo-1-w67e8 |\u001b[0m Epoch 0:  87%|████████▋ | 141/162 [00:37<00:05,  3.80it/s, loss=142, v_num=0, train_loss_step=142.0]\n",
      "\u001b[36mcucd7st2m9-algo-1-w67e8 |\u001b[0m Epoch 0:  87%|████████▋ | 141/162 [00:37<00:05,  3.80it/s, loss=144, v_num=0, train_loss_step=148.0]\n",
      "\u001b[36mcucd7st2m9-algo-1-w67e8 |\u001b[0m Epoch 0:  88%|████████▊ | 142/162 [00:37<00:05,  3.80it/s, loss=144, v_num=0, train_loss_step=148.0]\n",
      "\u001b[36mcucd7st2m9-algo-1-w67e8 |\u001b[0m Epoch 0:  88%|████████▊ | 142/162 [00:37<00:05,  3.80it/s, loss=142, v_num=0, train_loss_step=115.0]\n",
      "\u001b[36mcucd7st2m9-algo-1-w67e8 |\u001b[0m Epoch 0:  88%|████████▊ | 143/162 [00:37<00:05,  3.80it/s, loss=142, v_num=0, train_loss_step=115.0]\n",
      "\u001b[36mcucd7st2m9-algo-1-w67e8 |\u001b[0m Epoch 0:  88%|████████▊ | 143/162 [00:37<00:05,  3.80it/s, loss=141, v_num=0, train_loss_step=156.0]\n",
      "\u001b[36mcucd7st2m9-algo-1-w67e8 |\u001b[0m Epoch 0:  89%|████████▉ | 144/162 [00:37<00:04,  3.79it/s, loss=141, v_num=0, train_loss_step=156.0]\n",
      "\u001b[36mcucd7st2m9-algo-1-w67e8 |\u001b[0m Epoch 0:  89%|████████▉ | 144/162 [00:37<00:04,  3.79it/s, loss=142, v_num=0, train_loss_step=134.0]\n",
      "\u001b[36mcucd7st2m9-algo-1-w67e8 |\u001b[0m Epoch 0:  90%|████████▉ | 145/162 [00:38<00:04,  3.79it/s, loss=142, v_num=0, train_loss_step=134.0]\n",
      "\u001b[36mcucd7st2m9-algo-1-w67e8 |\u001b[0m Epoch 0:  90%|████████▉ | 145/162 [00:38<00:04,  3.79it/s, loss=141, v_num=0, train_loss_step=120.0]\n",
      "\u001b[36mcucd7st2m9-algo-1-w67e8 |\u001b[0m Epoch 0:  90%|█████████ | 146/162 [00:38<00:04,  3.80it/s, loss=141, v_num=0, train_loss_step=120.0]\n",
      "\u001b[36mcucd7st2m9-algo-1-w67e8 |\u001b[0m Epoch 0:  90%|█████████ | 146/162 [00:38<00:04,  3.80it/s, loss=138, v_num=0, train_loss_step=108.0]\n",
      "\u001b[36mcucd7st2m9-algo-1-w67e8 |\u001b[0m Epoch 0:  91%|█████████ | 147/162 [00:38<00:03,  3.80it/s, loss=138, v_num=0, train_loss_step=108.0]\n",
      "\u001b[36mcucd7st2m9-algo-1-w67e8 |\u001b[0m Epoch 0:  91%|█████████ | 147/162 [00:38<00:03,  3.80it/s, loss=138, v_num=0, train_loss_step=118.0]\n",
      "\u001b[36mcucd7st2m9-algo-1-w67e8 |\u001b[0m Epoch 0:  91%|█████████▏| 148/162 [00:38<00:03,  3.80it/s, loss=138, v_num=0, train_loss_step=118.0]\n",
      "\u001b[36mcucd7st2m9-algo-1-w67e8 |\u001b[0m Epoch 0:  91%|█████████▏| 148/162 [00:38<00:03,  3.80it/s, loss=137, v_num=0, train_loss_step=170.0]\n",
      "\u001b[36mcucd7st2m9-algo-1-w67e8 |\u001b[0m Epoch 0:  92%|█████████▏| 149/162 [00:39<00:03,  3.80it/s, loss=137, v_num=0, train_loss_step=170.0]\n",
      "\u001b[36mcucd7st2m9-algo-1-w67e8 |\u001b[0m Epoch 0:  92%|█████████▏| 149/162 [00:39<00:03,  3.80it/s, loss=136, v_num=0, train_loss_step=155.0]\n",
      "\u001b[36mcucd7st2m9-algo-1-w67e8 |\u001b[0m Epoch 0:  93%|█████████▎| 150/162 [00:39<00:03,  3.80it/s, loss=136, v_num=0, train_loss_step=155.0]\n",
      "\u001b[36mcucd7st2m9-algo-1-w67e8 |\u001b[0m Epoch 0:  93%|█████████▎| 150/162 [00:39<00:03,  3.80it/s, loss=134, v_num=0, train_loss_step=127.0]\n",
      "\u001b[36mcucd7st2m9-algo-1-w67e8 |\u001b[0m Epoch 0:  93%|█████████▎| 151/162 [00:39<00:02,  3.80it/s, loss=134, v_num=0, train_loss_step=127.0]\n",
      "\u001b[36mcucd7st2m9-algo-1-w67e8 |\u001b[0m Epoch 0:  93%|█████████▎| 151/162 [00:39<00:02,  3.80it/s, loss=133, v_num=0, train_loss_step=101.0]\n",
      "\u001b[36mcucd7st2m9-algo-1-w67e8 |\u001b[0m Epoch 0:  94%|█████████▍| 152/162 [00:40<00:02,  3.80it/s, loss=133, v_num=0, train_loss_step=101.0]\n",
      "\u001b[36mcucd7st2m9-algo-1-w67e8 |\u001b[0m Epoch 0:  94%|█████████▍| 152/162 [00:40<00:02,  3.80it/s, loss=132, v_num=0, train_loss_step=118.0]\n",
      "\u001b[36mcucd7st2m9-algo-1-w67e8 |\u001b[0m Epoch 0:  94%|█████████▍| 153/162 [00:40<00:02,  3.80it/s, loss=132, v_num=0, train_loss_step=118.0]\n",
      "\u001b[36mcucd7st2m9-algo-1-w67e8 |\u001b[0m Epoch 0:  94%|█████████▍| 153/162 [00:40<00:02,  3.80it/s, loss=132, v_num=0, train_loss_step=117.0]\n",
      "\u001b[36mcucd7st2m9-algo-1-w67e8 |\u001b[0m Epoch 0:  95%|█████████▌| 154/162 [00:40<00:02,  3.80it/s, loss=132, v_num=0, train_loss_step=117.0]\n",
      "\u001b[36mcucd7st2m9-algo-1-w67e8 |\u001b[0m Epoch 0:  95%|█████████▌| 154/162 [00:40<00:02,  3.79it/s, loss=131, v_num=0, train_loss_step=108.0]\n",
      "\u001b[36mcucd7st2m9-algo-1-w67e8 |\u001b[0m Epoch 0:  96%|█████████▌| 155/162 [00:40<00:01,  3.79it/s, loss=131, v_num=0, train_loss_step=108.0]\n",
      "\u001b[36mcucd7st2m9-algo-1-w67e8 |\u001b[0m Epoch 0:  96%|█████████▌| 155/162 [00:40<00:01,  3.79it/s, loss=131, v_num=0, train_loss_step=136.0]\n",
      "\u001b[36mcucd7st2m9-algo-1-w67e8 |\u001b[0m Epoch 0:  96%|█████████▋| 156/162 [00:41<00:01,  3.79it/s, loss=131, v_num=0, train_loss_step=136.0]\n",
      "\u001b[36mcucd7st2m9-algo-1-w67e8 |\u001b[0m Epoch 0:  96%|█████████▋| 156/162 [00:41<00:01,  3.79it/s, loss=129, v_num=0, train_loss_step=118.0]\n",
      "\u001b[36mcucd7st2m9-algo-1-w67e8 |\u001b[0m Epoch 0:  97%|█████████▋| 157/162 [00:41<00:01,  3.79it/s, loss=129, v_num=0, train_loss_step=118.0]\n",
      "\u001b[36mcucd7st2m9-algo-1-w67e8 |\u001b[0m Epoch 0:  97%|█████████▋| 157/162 [00:41<00:01,  3.79it/s, loss=129, v_num=0, train_loss_step=147.0]\n",
      "\u001b[36mcucd7st2m9-algo-1-w67e8 |\u001b[0m Epoch 0:  98%|█████████▊| 158/162 [00:41<00:01,  3.79it/s, loss=129, v_num=0, train_loss_step=147.0]\n",
      "\u001b[36mcucd7st2m9-algo-1-w67e8 |\u001b[0m Epoch 0:  98%|█████████▊| 158/162 [00:41<00:01,  3.79it/s, loss=129, v_num=0, train_loss_step=130.0]\n",
      "\u001b[36mcucd7st2m9-algo-1-w67e8 |\u001b[0m Epoch 0:  98%|█████████▊| 159/162 [00:41<00:00,  3.79it/s, loss=129, v_num=0, train_loss_step=130.0]\n",
      "\u001b[36mcucd7st2m9-algo-1-w67e8 |\u001b[0m Epoch 0:  98%|█████████▊| 159/162 [00:41<00:00,  3.79it/s, loss=128, v_num=0, train_loss_step=93.90]\n",
      "\u001b[36mcucd7st2m9-algo-1-w67e8 |\u001b[0m Epoch 0:  99%|█████████▉| 160/162 [00:42<00:00,  3.79it/s, loss=128, v_num=0, train_loss_step=93.90]\n",
      "\u001b[36mcucd7st2m9-algo-1-w67e8 |\u001b[0m Epoch 0:  99%|█████████▉| 160/162 [00:42<00:00,  3.79it/s, loss=128, v_num=0, train_loss_step=134.0]\n",
      "\u001b[36mcucd7st2m9-algo-1-w67e8 |\u001b[0m Epoch 0:  99%|█████████▉| 161/162 [00:42<00:00,  3.80it/s, loss=128, v_num=0, train_loss_step=134.0]\n",
      "\u001b[36mcucd7st2m9-algo-1-w67e8 |\u001b[0m Epoch 0:  99%|█████████▉| 161/162 [00:42<00:00,  3.80it/s, loss=127, v_num=0, train_loss_step=123.0]\n",
      "\u001b[36mcucd7st2m9-algo-1-w67e8 |\u001b[0m \n",
      "\u001b[36mcucd7st2m9-algo-1-w67e8 |\u001b[0m Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "\u001b[36mcucd7st2m9-algo-1-w67e8 |\u001b[0m \n",
      "\u001b[36mcucd7st2m9-algo-1-w67e8 |\u001b[0m Validation:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "\u001b[36mcucd7st2m9-algo-1-w67e8 |\u001b[0m \n",
      "\u001b[36mcucd7st2m9-algo-1-w67e8 |\u001b[0m Validation DataLoader 0:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "\u001b[36mcucd7st2m9-algo-1-w67e8 |\u001b[0m \n",
      "\u001b[36mcucd7st2m9-algo-1-w67e8 |\u001b[0m Validation DataLoader 0: 100%|██████████| 1/1 [00:00<00:00,  3.25it/s]\u001b[A\n",
      "\u001b[36mcucd7st2m9-algo-1-w67e8 |\u001b[0m Epoch 0: 100%|██████████| 162/162 [00:43<00:00,  3.77it/s, loss=127, v_num=0, train_loss_step=123.0]\n",
      "\u001b[36mcucd7st2m9-algo-1-w67e8 |\u001b[0m Epoch 0: 100%|██████████| 162/162 [00:43<00:00,  3.70it/s, loss=127, v_num=0, train_loss_step=123.0, val_loss=199.0]\n",
      "\u001b[36mcucd7st2m9-algo-1-w67e8 |\u001b[0m \u001b[A\n",
      "\u001b[36mcucd7st2m9-algo-1-w67e8 |\u001b[0m Epoch 0: 100%|██████████| 162/162 [00:43<00:00,  3.70it/s, loss=127, v_num=0, train_loss_step=123.0, val_loss=199.0, train_loss_epoch=161.0]\n",
      "\u001b[36mcucd7st2m9-algo-1-w67e8 |\u001b[0m Epoch 0:   0%|          | 0/162 [00:00<?, ?it/s, loss=127, v_num=0, train_loss_step=123.0, val_loss=199.0, train_loss_epoch=161.0]\n",
      "\u001b[36mcucd7st2m9-algo-1-w67e8 |\u001b[0m Epoch 1:   0%|          | 0/162 [00:00<?, ?it/s, loss=127, v_num=0, train_loss_step=123.0, val_loss=199.0, train_loss_epoch=161.0]\n",
      "\u001b[36mcucd7st2m9-algo-1-w67e8 |\u001b[0m Epoch 1:   1%|          | 1/162 [00:00<00:48,  3.30it/s, loss=127, v_num=0, train_loss_step=123.0, val_loss=199.0, train_loss_epoch=161.0]\n",
      "\u001b[36mcucd7st2m9-algo-1-w67e8 |\u001b[0m Epoch 1:   1%|          | 1/162 [00:00<00:48,  3.29it/s, loss=128, v_num=0, train_loss_step=137.0, val_loss=199.0, train_loss_epoch=161.0]\n",
      "\u001b[36mcucd7st2m9-algo-1-w67e8 |\u001b[0m Epoch 1:   1%|          | 2/162 [00:00<00:43,  3.66it/s, loss=128, v_num=0, train_loss_step=137.0, val_loss=199.0, train_loss_epoch=161.0]\n",
      "\u001b[36mcucd7st2m9-algo-1-w67e8 |\u001b[0m Epoch 1:   1%|          | 2/162 [00:00<00:43,  3.66it/s, loss=124, v_num=0, train_loss_step=93.30, val_loss=199.0, train_loss_epoch=161.0]\n",
      "\u001b[36mcucd7st2m9-algo-1-w67e8 |\u001b[0m Epoch 1:   2%|▏         | 3/162 [00:00<00:42,  3.76it/s, loss=124, v_num=0, train_loss_step=93.30, val_loss=199.0, train_loss_epoch=161.0]\n",
      "\u001b[36mcucd7st2m9-algo-1-w67e8 |\u001b[0m Epoch 1:   2%|▏         | 3/162 [00:00<00:42,  3.75it/s, loss=124, v_num=0, train_loss_step=132.0, val_loss=199.0, train_loss_epoch=161.0]\n",
      "\u001b[36mcucd7st2m9-algo-1-w67e8 |\u001b[0m Epoch 1:   2%|▏         | 4/162 [00:01<00:41,  3.81it/s, loss=124, v_num=0, train_loss_step=132.0, val_loss=199.0, train_loss_epoch=161.0]\n",
      "\u001b[36mcucd7st2m9-algo-1-w67e8 |\u001b[0m Epoch 1:   2%|▏         | 4/162 [00:01<00:41,  3.80it/s, loss=123, v_num=0, train_loss_step=99.40, val_loss=199.0, train_loss_epoch=161.0]\n",
      "\u001b[36mcucd7st2m9-algo-1-w67e8 |\u001b[0m Epoch 1:   3%|▎         | 5/162 [00:01<00:40,  3.84it/s, loss=123, v_num=0, train_loss_step=99.40, val_loss=199.0, train_loss_epoch=161.0]\n",
      "\u001b[36mcucd7st2m9-algo-1-w67e8 |\u001b[0m Epoch 1:   3%|▎         | 5/162 [00:01<00:40,  3.83it/s, loss=123, v_num=0, train_loss_step=99.90, val_loss=199.0, train_loss_epoch=161.0]\n",
      "\u001b[36mcucd7st2m9-algo-1-w67e8 |\u001b[0m Epoch 1:   4%|▎         | 6/162 [00:01<00:40,  3.85it/s, loss=123, v_num=0, train_loss_step=99.90, val_loss=199.0, train_loss_epoch=161.0]\n",
      "\u001b[36mcucd7st2m9-algo-1-w67e8 |\u001b[0m Epoch 1:   4%|▎         | 6/162 [00:01<00:40,  3.84it/s, loss=121, v_num=0, train_loss_step=89.90, val_loss=199.0, train_loss_epoch=161.0]\n",
      "\u001b[36mcucd7st2m9-algo-1-w67e8 |\u001b[0m Epoch 1:   4%|▍         | 7/162 [00:01<00:40,  3.84it/s, loss=121, v_num=0, train_loss_step=89.90, val_loss=199.0, train_loss_epoch=161.0]\n",
      "\u001b[36mcucd7st2m9-algo-1-w67e8 |\u001b[0m Epoch 1:   4%|▍         | 7/162 [00:01<00:40,  3.84it/s, loss=118, v_num=0, train_loss_step=98.70, val_loss=199.0, train_loss_epoch=161.0]\n",
      "\u001b[36mcucd7st2m9-algo-1-w67e8 |\u001b[0m Epoch 1:   5%|▍         | 8/162 [00:02<00:40,  3.83it/s, loss=118, v_num=0, train_loss_step=98.70, val_loss=199.0, train_loss_epoch=161.0]\n",
      "\u001b[36mcucd7st2m9-algo-1-w67e8 |\u001b[0m Epoch 1:   5%|▍         | 8/162 [00:02<00:40,  3.83it/s, loss=116, v_num=0, train_loss_step=126.0, val_loss=199.0, train_loss_epoch=161.0]\n",
      "\u001b[36mcucd7st2m9-algo-1-w67e8 |\u001b[0m Epoch 1:   6%|▌         | 9/162 [00:02<00:39,  3.84it/s, loss=116, v_num=0, train_loss_step=126.0, val_loss=199.0, train_loss_epoch=161.0]\n",
      "\u001b[36mcucd7st2m9-algo-1-w67e8 |\u001b[0m Epoch 1:   6%|▌         | 9/162 [00:02<00:39,  3.84it/s, loss=116, v_num=0, train_loss_step=109.0, val_loss=199.0, train_loss_epoch=161.0]\n",
      "\u001b[36mcucd7st2m9-algo-1-w67e8 |\u001b[0m Epoch 1:   6%|▌         | 10/162 [00:02<00:39,  3.83it/s, loss=116, v_num=0, train_loss_step=109.0, val_loss=199.0, train_loss_epoch=161.0]\n",
      "\u001b[36mcucd7st2m9-algo-1-w67e8 |\u001b[0m Epoch 1:   6%|▌         | 10/162 [00:02<00:39,  3.83it/s, loss=115, v_num=0, train_loss_step=90.30, val_loss=199.0, train_loss_epoch=161.0]\n",
      "\u001b[36mcucd7st2m9-algo-1-w67e8 |\u001b[0m Epoch 1:   7%|▋         | 11/162 [00:02<00:39,  3.84it/s, loss=115, v_num=0, train_loss_step=90.30, val_loss=199.0, train_loss_epoch=161.0]\n",
      "\u001b[36mcucd7st2m9-algo-1-w67e8 |\u001b[0m Epoch 1:   7%|▋         | 11/162 [00:02<00:39,  3.84it/s, loss=116, v_num=0, train_loss_step=130.0, val_loss=199.0, train_loss_epoch=161.0]\n",
      "\u001b[36mcucd7st2m9-algo-1-w67e8 |\u001b[0m Epoch 1:   7%|▋         | 12/162 [00:03<00:38,  3.86it/s, loss=116, v_num=0, train_loss_step=130.0, val_loss=199.0, train_loss_epoch=161.0]\n",
      "\u001b[36mcucd7st2m9-algo-1-w67e8 |\u001b[0m Epoch 1:   7%|▋         | 12/162 [00:03<00:38,  3.85it/s, loss=116, v_num=0, train_loss_step=131.0, val_loss=199.0, train_loss_epoch=161.0]\n",
      "\u001b[36mcucd7st2m9-algo-1-w67e8 |\u001b[0m Epoch 1:   8%|▊         | 13/162 [00:03<00:38,  3.85it/s, loss=116, v_num=0, train_loss_step=131.0, val_loss=199.0, train_loss_epoch=161.0]\n",
      "\u001b[36mcucd7st2m9-algo-1-w67e8 |\u001b[0m Epoch 1:   8%|▊         | 13/162 [00:03<00:38,  3.85it/s, loss=117, v_num=0, train_loss_step=127.0, val_loss=199.0, train_loss_epoch=161.0]\n",
      "\u001b[36mcucd7st2m9-algo-1-w67e8 |\u001b[0m Epoch 1:   9%|▊         | 14/162 [00:03<00:38,  3.83it/s, loss=117, v_num=0, train_loss_step=127.0, val_loss=199.0, train_loss_epoch=161.0]\n",
      "\u001b[36mcucd7st2m9-algo-1-w67e8 |\u001b[0m Epoch 1:   9%|▊         | 14/162 [00:03<00:38,  3.83it/s, loss=115, v_num=0, train_loss_step=87.00, val_loss=199.0, train_loss_epoch=161.0]\n",
      "\u001b[36mcucd7st2m9-algo-1-w67e8 |\u001b[0m Epoch 1:   9%|▉         | 15/162 [00:03<00:38,  3.84it/s, loss=115, v_num=0, train_loss_step=87.00, val_loss=199.0, train_loss_epoch=161.0]\n",
      "\u001b[36mcucd7st2m9-algo-1-w67e8 |\u001b[0m Epoch 1:   9%|▉         | 15/162 [00:03<00:38,  3.84it/s, loss=116, v_num=0, train_loss_step=139.0, val_loss=199.0, train_loss_epoch=161.0]\n",
      "\u001b[36mcucd7st2m9-algo-1-w67e8 |\u001b[0m Epoch 1:  10%|▉         | 16/162 [00:04<00:37,  3.84it/s, loss=116, v_num=0, train_loss_step=139.0, val_loss=199.0, train_loss_epoch=161.0]\n",
      "\u001b[36mcucd7st2m9-algo-1-w67e8 |\u001b[0m Epoch 1:  10%|▉         | 16/162 [00:04<00:37,  3.84it/s, loss=115, v_num=0, train_loss_step=138.0, val_loss=199.0, train_loss_epoch=161.0]\n",
      "\u001b[36mcucd7st2m9-algo-1-w67e8 |\u001b[0m Epoch 1:  10%|█         | 17/162 [00:04<00:37,  3.86it/s, loss=115, v_num=0, train_loss_step=138.0, val_loss=199.0, train_loss_epoch=161.0]\n",
      "\u001b[36mcucd7st2m9-algo-1-w67e8 |\u001b[0m Epoch 1:  10%|█         | 17/162 [00:04<00:37,  3.85it/s, loss=114, v_num=0, train_loss_step=111.0, val_loss=199.0, train_loss_epoch=161.0]\n",
      "\u001b[36mcucd7st2m9-algo-1-w67e8 |\u001b[0m Epoch 1:  11%|█         | 18/162 [00:04<00:37,  3.87it/s, loss=114, v_num=0, train_loss_step=111.0, val_loss=199.0, train_loss_epoch=161.0]\n",
      "\u001b[36mcucd7st2m9-algo-1-w67e8 |\u001b[0m Epoch 1:  11%|█         | 18/162 [00:04<00:37,  3.87it/s, loss=114, v_num=0, train_loss_step=93.00, val_loss=199.0, train_loss_epoch=161.0]\n",
      "\u001b[36mcucd7st2m9-algo-1-w67e8 |\u001b[0m Epoch 1:  12%|█▏        | 19/162 [00:04<00:37,  3.85it/s, loss=114, v_num=0, train_loss_step=93.00, val_loss=199.0, train_loss_epoch=161.0]\n",
      "\u001b[36mcucd7st2m9-algo-1-w67e8 |\u001b[0m Epoch 1:  12%|█▏        | 19/162 [00:04<00:37,  3.85it/s, loss=116, v_num=0, train_loss_step=164.0, val_loss=199.0, train_loss_epoch=161.0]\n",
      "\u001b[36mcucd7st2m9-algo-1-w67e8 |\u001b[0m Epoch 1:  12%|█▏        | 20/162 [00:05<00:36,  3.86it/s, loss=116, v_num=0, train_loss_step=164.0, val_loss=199.0, train_loss_epoch=161.0]\n",
      "\u001b[36mcucd7st2m9-algo-1-w67e8 |\u001b[0m Epoch 1:  12%|█▏        | 20/162 [00:05<00:36,  3.86it/s, loss=116, v_num=0, train_loss_step=129.0, val_loss=199.0, train_loss_epoch=161.0]\n",
      "\u001b[36mcucd7st2m9-algo-1-w67e8 |\u001b[0m Epoch 1:  13%|█▎        | 21/162 [00:05<00:36,  3.86it/s, loss=116, v_num=0, train_loss_step=129.0, val_loss=199.0, train_loss_epoch=161.0]\n",
      "\u001b[36mcucd7st2m9-algo-1-w67e8 |\u001b[0m Epoch 1:  13%|█▎        | 21/162 [00:05<00:36,  3.86it/s, loss=114, v_num=0, train_loss_step=93.40, val_loss=199.0, train_loss_epoch=161.0]\n",
      "\u001b[36mcucd7st2m9-algo-1-w67e8 |\u001b[0m Epoch 1:  14%|█▎        | 22/162 [00:05<00:36,  3.85it/s, loss=114, v_num=0, train_loss_step=93.40, val_loss=199.0, train_loss_epoch=161.0]\n",
      "\u001b[36mcucd7st2m9-algo-1-w67e8 |\u001b[0m Epoch 1:  14%|█▎        | 22/162 [00:05<00:36,  3.85it/s, loss=115, v_num=0, train_loss_step=112.0, val_loss=199.0, train_loss_epoch=161.0]\n",
      "\u001b[36mcucd7st2m9-algo-1-w67e8 |\u001b[0m Epoch 1:  14%|█▍        | 23/162 [00:05<00:36,  3.86it/s, loss=115, v_num=0, train_loss_step=112.0, val_loss=199.0, train_loss_epoch=161.0]\n",
      "\u001b[36mcucd7st2m9-algo-1-w67e8 |\u001b[0m Epoch 1:  14%|█▍        | 23/162 [00:05<00:36,  3.85it/s, loss=114, v_num=0, train_loss_step=119.0, val_loss=199.0, train_loss_epoch=161.0]\n",
      "\u001b[36mcucd7st2m9-algo-1-w67e8 |\u001b[0m Epoch 1:  15%|█▍        | 24/162 [00:06<00:35,  3.86it/s, loss=114, v_num=0, train_loss_step=119.0, val_loss=199.0, train_loss_epoch=161.0]\n",
      "\u001b[36mcucd7st2m9-algo-1-w67e8 |\u001b[0m Epoch 1:  15%|█▍        | 24/162 [00:06<00:35,  3.86it/s, loss=114, v_num=0, train_loss_step=97.10, val_loss=199.0, train_loss_epoch=161.0]\n",
      "\u001b[36mcucd7st2m9-algo-1-w67e8 |\u001b[0m Epoch 1:  15%|█▌        | 25/162 [00:06<00:35,  3.85it/s, loss=114, v_num=0, train_loss_step=97.10, val_loss=199.0, train_loss_epoch=161.0]\n",
      "\u001b[36mcucd7st2m9-algo-1-w67e8 |\u001b[0m Epoch 1:  15%|█▌        | 25/162 [00:06<00:35,  3.84it/s, loss=116, v_num=0, train_loss_step=128.0, val_loss=199.0, train_loss_epoch=161.0]\n",
      "\u001b[36mcucd7st2m9-algo-1-w67e8 |\u001b[0m Epoch 1:  16%|█▌        | 26/162 [00:06<00:35,  3.84it/s, loss=116, v_num=0, train_loss_step=128.0, val_loss=199.0, train_loss_epoch=161.0]\n",
      "\u001b[36mcucd7st2m9-algo-1-w67e8 |\u001b[0m Epoch 1:  16%|█▌        | 26/162 [00:06<00:35,  3.84it/s, loss=118, v_num=0, train_loss_step=130.0, val_loss=199.0, train_loss_epoch=161.0]\n",
      "\u001b[36mcucd7st2m9-algo-1-w67e8 |\u001b[0m Epoch 1:  17%|█▋        | 27/162 [00:07<00:35,  3.85it/s, loss=118, v_num=0, train_loss_step=130.0, val_loss=199.0, train_loss_epoch=161.0]\n",
      "\u001b[36mcucd7st2m9-algo-1-w67e8 |\u001b[0m Epoch 1:  17%|█▋        | 27/162 [00:07<00:35,  3.85it/s, loss=118, v_num=0, train_loss_step=103.0, val_loss=199.0, train_loss_epoch=161.0]\n",
      "\u001b[36mcucd7st2m9-algo-1-w67e8 |\u001b[0m Epoch 1:  17%|█▋        | 28/162 [00:07<00:34,  3.86it/s, loss=118, v_num=0, train_loss_step=103.0, val_loss=199.0, train_loss_epoch=161.0]\n",
      "\u001b[36mcucd7st2m9-algo-1-w67e8 |\u001b[0m Epoch 1:  17%|█▋        | 28/162 [00:07<00:34,  3.86it/s, loss=118, v_num=0, train_loss_step=120.0, val_loss=199.0, train_loss_epoch=161.0]\n",
      "\u001b[36mcucd7st2m9-algo-1-w67e8 |\u001b[0m Epoch 1:  18%|█▊        | 29/162 [00:07<00:34,  3.86it/s, loss=118, v_num=0, train_loss_step=120.0, val_loss=199.0, train_loss_epoch=161.0]\n",
      "\u001b[36mcucd7st2m9-algo-1-w67e8 |\u001b[0m Epoch 1:  18%|█▊        | 29/162 [00:07<00:34,  3.86it/s, loss=118, v_num=0, train_loss_step=115.0, val_loss=199.0, train_loss_epoch=161.0]\n",
      "\u001b[36mcucd7st2m9-algo-1-w67e8 |\u001b[0m Epoch 1:  19%|█▊        | 30/162 [00:07<00:34,  3.85it/s, loss=118, v_num=0, train_loss_step=115.0, val_loss=199.0, train_loss_epoch=161.0]\n",
      "\u001b[36mcucd7st2m9-algo-1-w67e8 |\u001b[0m Epoch 1:  19%|█▊        | 30/162 [00:07<00:34,  3.85it/s, loss=119, v_num=0, train_loss_step=106.0, val_loss=199.0, train_loss_epoch=161.0]\n",
      "\u001b[36mcucd7st2m9-algo-1-w67e8 |\u001b[0m Epoch 1:  19%|█▉        | 31/162 [00:08<00:34,  3.85it/s, loss=119, v_num=0, train_loss_step=106.0, val_loss=199.0, train_loss_epoch=161.0]\n",
      "\u001b[36mcucd7st2m9-algo-1-w67e8 |\u001b[0m Epoch 1:  19%|█▉        | 31/162 [00:08<00:34,  3.85it/s, loss=119, v_num=0, train_loss_step=131.0, val_loss=199.0, train_loss_epoch=161.0]\n",
      "\u001b[36mcucd7st2m9-algo-1-w67e8 |\u001b[0m Epoch 1:  20%|█▉        | 32/162 [00:08<00:33,  3.86it/s, loss=119, v_num=0, train_loss_step=131.0, val_loss=199.0, train_loss_epoch=161.0]\n",
      "\u001b[36mcucd7st2m9-algo-1-w67e8 |\u001b[0m Epoch 1:  20%|█▉        | 32/162 [00:08<00:33,  3.86it/s, loss=118, v_num=0, train_loss_step=119.0, val_loss=199.0, train_loss_epoch=161.0]\n",
      "\u001b[36mcucd7st2m9-algo-1-w67e8 |\u001b[0m Epoch 1:  20%|██        | 33/162 [00:08<00:33,  3.86it/s, loss=118, v_num=0, train_loss_step=119.0, val_loss=199.0, train_loss_epoch=161.0]\n",
      "\u001b[36mcucd7st2m9-algo-1-w67e8 |\u001b[0m Epoch 1:  20%|██        | 33/162 [00:08<00:33,  3.86it/s, loss=117, v_num=0, train_loss_step=110.0, val_loss=199.0, train_loss_epoch=161.0]\n",
      "\u001b[36mcucd7st2m9-algo-1-w67e8 |\u001b[0m Epoch 1:  21%|██        | 34/162 [00:08<00:33,  3.87it/s, loss=117, v_num=0, train_loss_step=110.0, val_loss=199.0, train_loss_epoch=161.0]\n",
      "\u001b[36mcucd7st2m9-algo-1-w67e8 |\u001b[0m Epoch 1:  21%|██        | 34/162 [00:08<00:33,  3.86it/s, loss=119, v_num=0, train_loss_step=116.0, val_loss=199.0, train_loss_epoch=161.0]\n",
      "\u001b[36mcucd7st2m9-algo-1-w67e8 |\u001b[0m Epoch 1:  22%|██▏       | 35/162 [00:09<00:32,  3.86it/s, loss=119, v_num=0, train_loss_step=116.0, val_loss=199.0, train_loss_epoch=161.0]\n",
      "\u001b[36mcucd7st2m9-algo-1-w67e8 |\u001b[0m Epoch 1:  22%|██▏       | 35/162 [00:09<00:32,  3.86it/s, loss=119, v_num=0, train_loss_step=150.0, val_loss=199.0, train_loss_epoch=161.0]\n",
      "\u001b[36mcucd7st2m9-algo-1-w67e8 |\u001b[0m Epoch 1:  22%|██▏       | 36/162 [00:09<00:32,  3.87it/s, loss=119, v_num=0, train_loss_step=150.0, val_loss=199.0, train_loss_epoch=161.0]\n",
      "\u001b[36mcucd7st2m9-algo-1-w67e8 |\u001b[0m Epoch 1:  22%|██▏       | 36/162 [00:09<00:32,  3.87it/s, loss=118, v_num=0, train_loss_step=109.0, val_loss=199.0, train_loss_epoch=161.0]\n",
      "\u001b[36mcucd7st2m9-algo-1-w67e8 |\u001b[0m Epoch 1:  23%|██▎       | 37/162 [00:09<00:32,  3.87it/s, loss=118, v_num=0, train_loss_step=109.0, val_loss=199.0, train_loss_epoch=161.0]\n",
      "\u001b[36mcucd7st2m9-algo-1-w67e8 |\u001b[0m Epoch 1:  23%|██▎       | 37/162 [00:09<00:32,  3.87it/s, loss=118, v_num=0, train_loss_step=109.0, val_loss=199.0, train_loss_epoch=161.0]\n",
      "\u001b[36mcucd7st2m9-algo-1-w67e8 |\u001b[0m Epoch 1:  23%|██▎       | 38/162 [00:09<00:32,  3.87it/s, loss=118, v_num=0, train_loss_step=109.0, val_loss=199.0, train_loss_epoch=161.0]\n",
      "\u001b[36mcucd7st2m9-algo-1-w67e8 |\u001b[0m Epoch 1:  23%|██▎       | 38/162 [00:09<00:32,  3.87it/s, loss=119, v_num=0, train_loss_step=114.0, val_loss=199.0, train_loss_epoch=161.0]\n",
      "\u001b[36mcucd7st2m9-algo-1-w67e8 |\u001b[0m Epoch 1:  24%|██▍       | 39/162 [00:10<00:31,  3.88it/s, loss=119, v_num=0, train_loss_step=114.0, val_loss=199.0, train_loss_epoch=161.0]\n",
      "\u001b[36mcucd7st2m9-algo-1-w67e8 |\u001b[0m Epoch 1:  24%|██▍       | 39/162 [00:10<00:31,  3.88it/s, loss=117, v_num=0, train_loss_step=122.0, val_loss=199.0, train_loss_epoch=161.0]\n",
      "\u001b[36mcucd7st2m9-algo-1-w67e8 |\u001b[0m Epoch 1:  25%|██▍       | 40/162 [00:10<00:31,  3.88it/s, loss=117, v_num=0, train_loss_step=122.0, val_loss=199.0, train_loss_epoch=161.0]\n",
      "\u001b[36mcucd7st2m9-algo-1-w67e8 |\u001b[0m Epoch 1:  25%|██▍       | 40/162 [00:10<00:31,  3.88it/s, loss=118, v_num=0, train_loss_step=147.0, val_loss=199.0, train_loss_epoch=161.0]\n",
      "\u001b[36mcucd7st2m9-algo-1-w67e8 |\u001b[0m Epoch 1:  25%|██▌       | 41/162 [00:10<00:31,  3.88it/s, loss=118, v_num=0, train_loss_step=147.0, val_loss=199.0, train_loss_epoch=161.0]\n",
      "\u001b[36mcucd7st2m9-algo-1-w67e8 |\u001b[0m Epoch 1:  25%|██▌       | 41/162 [00:10<00:31,  3.88it/s, loss=120, v_num=0, train_loss_step=132.0, val_loss=199.0, train_loss_epoch=161.0]\n",
      "\u001b[36mcucd7st2m9-algo-1-w67e8 |\u001b[0m Epoch 1:  26%|██▌       | 42/162 [00:10<00:30,  3.89it/s, loss=120, v_num=0, train_loss_step=132.0, val_loss=199.0, train_loss_epoch=161.0]\n",
      "\u001b[36mcucd7st2m9-algo-1-w67e8 |\u001b[0m Epoch 1:  26%|██▌       | 42/162 [00:10<00:30,  3.89it/s, loss=119, v_num=0, train_loss_step=93.70, val_loss=199.0, train_loss_epoch=161.0]\n",
      "\u001b[36mcucd7st2m9-algo-1-w67e8 |\u001b[0m Epoch 1:  27%|██▋       | 43/162 [00:11<00:30,  3.87it/s, loss=119, v_num=0, train_loss_step=93.70, val_loss=199.0, train_loss_epoch=161.0]\n",
      "\u001b[36mcucd7st2m9-algo-1-w67e8 |\u001b[0m Epoch 1:  27%|██▋       | 43/162 [00:11<00:30,  3.87it/s, loss=118, v_num=0, train_loss_step=102.0, val_loss=199.0, train_loss_epoch=161.0]\n",
      "\u001b[36mcucd7st2m9-algo-1-w67e8 |\u001b[0m Epoch 1:  27%|██▋       | 44/162 [00:11<00:30,  3.87it/s, loss=118, v_num=0, train_loss_step=102.0, val_loss=199.0, train_loss_epoch=161.0]\n",
      "\u001b[36mcucd7st2m9-algo-1-w67e8 |\u001b[0m Epoch 1:  27%|██▋       | 44/162 [00:11<00:30,  3.87it/s, loss=121, v_num=0, train_loss_step=161.0, val_loss=199.0, train_loss_epoch=161.0]\n",
      "\u001b[36mcucd7st2m9-algo-1-w67e8 |\u001b[0m Epoch 1:  28%|██▊       | 45/162 [00:11<00:30,  3.87it/s, loss=121, v_num=0, train_loss_step=161.0, val_loss=199.0, train_loss_epoch=161.0]\n",
      "\u001b[36mcucd7st2m9-algo-1-w67e8 |\u001b[0m Epoch 1:  28%|██▊       | 45/162 [00:11<00:30,  3.87it/s, loss=121, v_num=0, train_loss_step=131.0, val_loss=199.0, train_loss_epoch=161.0]\n",
      "\u001b[36mcucd7st2m9-algo-1-w67e8 |\u001b[0m Epoch 1:  28%|██▊       | 46/162 [00:11<00:30,  3.86it/s, loss=121, v_num=0, train_loss_step=131.0, val_loss=199.0, train_loss_epoch=161.0]\n",
      "\u001b[36mcucd7st2m9-algo-1-w67e8 |\u001b[0m Epoch 1:  28%|██▊       | 46/162 [00:11<00:30,  3.86it/s, loss=121, v_num=0, train_loss_step=133.0, val_loss=199.0, train_loss_epoch=161.0]\n",
      "\u001b[36mcucd7st2m9-algo-1-w67e8 |\u001b[0m Epoch 1:  29%|██▉       | 47/162 [00:12<00:29,  3.87it/s, loss=121, v_num=0, train_loss_step=133.0, val_loss=199.0, train_loss_epoch=161.0]\n",
      "\u001b[36mcucd7st2m9-algo-1-w67e8 |\u001b[0m Epoch 1:  29%|██▉       | 47/162 [00:12<00:29,  3.87it/s, loss=122, v_num=0, train_loss_step=108.0, val_loss=199.0, train_loss_epoch=161.0]\n",
      "\u001b[36mcucd7st2m9-algo-1-w67e8 |\u001b[0m Epoch 1:  30%|██▉       | 48/162 [00:12<00:29,  3.87it/s, loss=122, v_num=0, train_loss_step=108.0, val_loss=199.0, train_loss_epoch=161.0]\n",
      "\u001b[36mcucd7st2m9-algo-1-w67e8 |\u001b[0m Epoch 1:  30%|██▉       | 48/162 [00:12<00:29,  3.87it/s, loss=120, v_num=0, train_loss_step=97.20, val_loss=199.0, train_loss_epoch=161.0]\n",
      "\u001b[36mcucd7st2m9-algo-1-w67e8 |\u001b[0m Epoch 1:  30%|███       | 49/162 [00:12<00:29,  3.86it/s, loss=120, v_num=0, train_loss_step=97.20, val_loss=199.0, train_loss_epoch=161.0]\n",
      "\u001b[36mcucd7st2m9-algo-1-w67e8 |\u001b[0m Epoch 1:  30%|███       | 49/162 [00:12<00:29,  3.86it/s, loss=121, v_num=0, train_loss_step=134.0, val_loss=199.0, train_loss_epoch=161.0]\n",
      "\u001b[36mcucd7st2m9-algo-1-w67e8 |\u001b[0m Epoch 1:  31%|███       | 50/162 [00:12<00:29,  3.86it/s, loss=121, v_num=0, train_loss_step=134.0, val_loss=199.0, train_loss_epoch=161.0]\n",
      "\u001b[36mcucd7st2m9-algo-1-w67e8 |\u001b[0m Epoch 1:  31%|███       | 50/162 [00:12<00:29,  3.86it/s, loss=121, v_num=0, train_loss_step=91.40, val_loss=199.0, train_loss_epoch=161.0]\n",
      "\u001b[36mcucd7st2m9-algo-1-w67e8 |\u001b[0m Epoch 1:  31%|███▏      | 51/162 [00:13<00:28,  3.86it/s, loss=121, v_num=0, train_loss_step=91.40, val_loss=199.0, train_loss_epoch=161.0]\n",
      "\u001b[36mcucd7st2m9-algo-1-w67e8 |\u001b[0m Epoch 1:  31%|███▏      | 51/162 [00:13<00:28,  3.86it/s, loss=119, v_num=0, train_loss_step=93.90, val_loss=199.0, train_loss_epoch=161.0]\n",
      "\u001b[36mcucd7st2m9-algo-1-w67e8 |\u001b[0m Epoch 1:  32%|███▏      | 52/162 [00:13<00:28,  3.86it/s, loss=119, v_num=0, train_loss_step=93.90, val_loss=199.0, train_loss_epoch=161.0]\n",
      "\u001b[36mcucd7st2m9-algo-1-w67e8 |\u001b[0m Epoch 1:  32%|███▏      | 52/162 [00:13<00:28,  3.86it/s, loss=119, v_num=0, train_loss_step=117.0, val_loss=199.0, train_loss_epoch=161.0]\n",
      "\u001b[36mcucd7st2m9-algo-1-w67e8 |\u001b[0m Epoch 1:  33%|███▎      | 53/162 [00:13<00:28,  3.87it/s, loss=119, v_num=0, train_loss_step=117.0, val_loss=199.0, train_loss_epoch=161.0]\n",
      "\u001b[36mcucd7st2m9-algo-1-w67e8 |\u001b[0m Epoch 1:  33%|███▎      | 53/162 [00:13<00:28,  3.87it/s, loss=119, v_num=0, train_loss_step=111.0, val_loss=199.0, train_loss_epoch=161.0]\n",
      "\u001b[36mcucd7st2m9-algo-1-w67e8 |\u001b[0m Epoch 1:  33%|███▎      | 54/162 [00:13<00:27,  3.87it/s, loss=119, v_num=0, train_loss_step=111.0, val_loss=199.0, train_loss_epoch=161.0]\n",
      "\u001b[36mcucd7st2m9-algo-1-w67e8 |\u001b[0m Epoch 1:  33%|███▎      | 54/162 [00:13<00:27,  3.87it/s, loss=118, v_num=0, train_loss_step=95.80, val_loss=199.0, train_loss_epoch=161.0]\n",
      "\u001b[36mcucd7st2m9-algo-1-w67e8 |\u001b[0m Epoch 1:  34%|███▍      | 55/162 [00:14<00:27,  3.87it/s, loss=118, v_num=0, train_loss_step=95.80, val_loss=199.0, train_loss_epoch=161.0]\n",
      "\u001b[36mcucd7st2m9-algo-1-w67e8 |\u001b[0m Epoch 1:  34%|███▍      | 55/162 [00:14<00:27,  3.87it/s, loss=117, v_num=0, train_loss_step=132.0, val_loss=199.0, train_loss_epoch=161.0]\n",
      "\u001b[36mcucd7st2m9-algo-1-w67e8 |\u001b[0m Epoch 1:  35%|███▍      | 56/162 [00:14<00:27,  3.87it/s, loss=117, v_num=0, train_loss_step=132.0, val_loss=199.0, train_loss_epoch=161.0]\n",
      "\u001b[36mcucd7st2m9-algo-1-w67e8 |\u001b[0m Epoch 1:  35%|███▍      | 56/162 [00:14<00:27,  3.87it/s, loss=118, v_num=0, train_loss_step=125.0, val_loss=199.0, train_loss_epoch=161.0]\n",
      "\u001b[36mcucd7st2m9-algo-1-w67e8 |\u001b[0m Epoch 1:  35%|███▌      | 57/162 [00:14<00:27,  3.88it/s, loss=118, v_num=0, train_loss_step=125.0, val_loss=199.0, train_loss_epoch=161.0]\n",
      "\u001b[36mcucd7st2m9-algo-1-w67e8 |\u001b[0m Epoch 1:  35%|███▌      | 57/162 [00:14<00:27,  3.88it/s, loss=118, v_num=0, train_loss_step=121.0, val_loss=199.0, train_loss_epoch=161.0]\n",
      "\u001b[36mcucd7st2m9-algo-1-w67e8 |\u001b[0m Epoch 1:  36%|███▌      | 58/162 [00:14<00:26,  3.88it/s, loss=118, v_num=0, train_loss_step=121.0, val_loss=199.0, train_loss_epoch=161.0]\n",
      "\u001b[36mcucd7st2m9-algo-1-w67e8 |\u001b[0m Epoch 1:  36%|███▌      | 58/162 [00:14<00:26,  3.88it/s, loss=119, v_num=0, train_loss_step=124.0, val_loss=199.0, train_loss_epoch=161.0]\n",
      "\u001b[36mcucd7st2m9-algo-1-w67e8 |\u001b[0m Epoch 1:  36%|███▋      | 59/162 [00:15<00:26,  3.88it/s, loss=119, v_num=0, train_loss_step=124.0, val_loss=199.0, train_loss_epoch=161.0]\n",
      "\u001b[36mcucd7st2m9-algo-1-w67e8 |\u001b[0m Epoch 1:  36%|███▋      | 59/162 [00:15<00:26,  3.88it/s, loss=119, v_num=0, train_loss_step=132.0, val_loss=199.0, train_loss_epoch=161.0]\n",
      "\u001b[36mcucd7st2m9-algo-1-w67e8 |\u001b[0m Epoch 1:  37%|███▋      | 60/162 [00:15<00:26,  3.89it/s, loss=119, v_num=0, train_loss_step=132.0, val_loss=199.0, train_loss_epoch=161.0]\n",
      "\u001b[36mcucd7st2m9-algo-1-w67e8 |\u001b[0m Epoch 1:  37%|███▋      | 60/162 [00:15<00:26,  3.89it/s, loss=116, v_num=0, train_loss_step=93.30, val_loss=199.0, train_loss_epoch=161.0]\n",
      "\u001b[36mcucd7st2m9-algo-1-w67e8 |\u001b[0m Epoch 1:  38%|███▊      | 61/162 [00:15<00:26,  3.88it/s, loss=116, v_num=0, train_loss_step=93.30, val_loss=199.0, train_loss_epoch=161.0]\n",
      "\u001b[36mcucd7st2m9-algo-1-w67e8 |\u001b[0m Epoch 1:  38%|███▊      | 61/162 [00:15<00:26,  3.88it/s, loss=115, v_num=0, train_loss_step=108.0, val_loss=199.0, train_loss_epoch=161.0]\n",
      "\u001b[36mcucd7st2m9-algo-1-w67e8 |\u001b[0m Epoch 1:  38%|███▊      | 62/162 [00:15<00:25,  3.88it/s, loss=115, v_num=0, train_loss_step=108.0, val_loss=199.0, train_loss_epoch=161.0]\n",
      "\u001b[36mcucd7st2m9-algo-1-w67e8 |\u001b[0m Epoch 1:  38%|███▊      | 62/162 [00:15<00:25,  3.88it/s, loss=114, v_num=0, train_loss_step=75.40, val_loss=199.0, train_loss_epoch=161.0]\n",
      "\u001b[36mcucd7st2m9-algo-1-w67e8 |\u001b[0m Epoch 1:  39%|███▉      | 63/162 [00:16<00:25,  3.88it/s, loss=114, v_num=0, train_loss_step=75.40, val_loss=199.0, train_loss_epoch=161.0]\n",
      "\u001b[36mcucd7st2m9-algo-1-w67e8 |\u001b[0m Epoch 1:  39%|███▉      | 63/162 [00:16<00:25,  3.88it/s, loss=115, v_num=0, train_loss_step=124.0, val_loss=199.0, train_loss_epoch=161.0]\n",
      "\u001b[36mcucd7st2m9-algo-1-w67e8 |\u001b[0m Epoch 1:  40%|███▉      | 64/162 [00:16<00:25,  3.88it/s, loss=115, v_num=0, train_loss_step=124.0, val_loss=199.0, train_loss_epoch=161.0]\n",
      "\u001b[36mcucd7st2m9-algo-1-w67e8 |\u001b[0m Epoch 1:  40%|███▉      | 64/162 [00:16<00:25,  3.88it/s, loss=112, v_num=0, train_loss_step=94.30, val_loss=199.0, train_loss_epoch=161.0]\n",
      "\u001b[36mcucd7st2m9-algo-1-w67e8 |\u001b[0m Epoch 1:  40%|████      | 65/162 [00:16<00:24,  3.88it/s, loss=112, v_num=0, train_loss_step=94.30, val_loss=199.0, train_loss_epoch=161.0]\n",
      "\u001b[36mcucd7st2m9-algo-1-w67e8 |\u001b[0m Epoch 1:  40%|████      | 65/162 [00:16<00:24,  3.88it/s, loss=112, v_num=0, train_loss_step=123.0, val_loss=199.0, train_loss_epoch=161.0]\n",
      "\u001b[36mcucd7st2m9-algo-1-w67e8 |\u001b[0m Epoch 1:  41%|████      | 66/162 [00:16<00:24,  3.89it/s, loss=112, v_num=0, train_loss_step=123.0, val_loss=199.0, train_loss_epoch=161.0]\n",
      "\u001b[36mcucd7st2m9-algo-1-w67e8 |\u001b[0m Epoch 1:  41%|████      | 66/162 [00:16<00:24,  3.89it/s, loss=113, v_num=0, train_loss_step=165.0, val_loss=199.0, train_loss_epoch=161.0]\n",
      "\u001b[36mcucd7st2m9-algo-1-w67e8 |\u001b[0m Epoch 1:  41%|████▏     | 67/162 [00:17<00:24,  3.89it/s, loss=113, v_num=0, train_loss_step=165.0, val_loss=199.0, train_loss_epoch=161.0]\n",
      "\u001b[36mcucd7st2m9-algo-1-w67e8 |\u001b[0m Epoch 1:  41%|████▏     | 67/162 [00:17<00:24,  3.89it/s, loss=113, v_num=0, train_loss_step=108.0, val_loss=199.0, train_loss_epoch=161.0]\n",
      "\u001b[36mcucd7st2m9-algo-1-w67e8 |\u001b[0m Epoch 1:  42%|████▏     | 68/162 [00:17<00:24,  3.88it/s, loss=113, v_num=0, train_loss_step=108.0, val_loss=199.0, train_loss_epoch=161.0]\n",
      "\u001b[36mcucd7st2m9-algo-1-w67e8 |\u001b[0m Epoch 1:  42%|████▏     | 68/162 [00:17<00:24,  3.88it/s, loss=114, v_num=0, train_loss_step=111.0, val_loss=199.0, train_loss_epoch=161.0]\n",
      "\u001b[36mcucd7st2m9-algo-1-w67e8 |\u001b[0m Epoch 1:  43%|████▎     | 69/162 [00:17<00:23,  3.88it/s, loss=114, v_num=0, train_loss_step=111.0, val_loss=199.0, train_loss_epoch=161.0]\n",
      "\u001b[36mcucd7st2m9-algo-1-w67e8 |\u001b[0m Epoch 1:  43%|████▎     | 69/162 [00:17<00:23,  3.88it/s, loss=113, v_num=0, train_loss_step=119.0, val_loss=199.0, train_loss_epoch=161.0]\n",
      "\u001b[36mcucd7st2m9-algo-1-w67e8 |\u001b[0m Epoch 1:  43%|████▎     | 70/162 [00:18<00:23,  3.89it/s, loss=113, v_num=0, train_loss_step=119.0, val_loss=199.0, train_loss_epoch=161.0]\n",
      "\u001b[36mcucd7st2m9-algo-1-w67e8 |\u001b[0m Epoch 1:  43%|████▎     | 70/162 [00:18<00:23,  3.89it/s, loss=114, v_num=0, train_loss_step=113.0, val_loss=199.0, train_loss_epoch=161.0]\n",
      "\u001b[36mcucd7st2m9-algo-1-w67e8 |\u001b[0m Epoch 1:  44%|████▍     | 71/162 [00:18<00:23,  3.89it/s, loss=114, v_num=0, train_loss_step=113.0, val_loss=199.0, train_loss_epoch=161.0]\n",
      "\u001b[36mcucd7st2m9-algo-1-w67e8 |\u001b[0m Epoch 1:  44%|████▍     | 71/162 [00:18<00:23,  3.89it/s, loss=115, v_num=0, train_loss_step=103.0, val_loss=199.0, train_loss_epoch=161.0]\n",
      "\u001b[36mcucd7st2m9-algo-1-w67e8 |\u001b[0m Epoch 1:  44%|████▍     | 72/162 [00:18<00:23,  3.89it/s, loss=115, v_num=0, train_loss_step=103.0, val_loss=199.0, train_loss_epoch=161.0]\n",
      "\u001b[36mcucd7st2m9-algo-1-w67e8 |\u001b[0m Epoch 1:  44%|████▍     | 72/162 [00:18<00:23,  3.89it/s, loss=114, v_num=0, train_loss_step=100.0, val_loss=199.0, train_loss_epoch=161.0]\n",
      "\u001b[36mcucd7st2m9-algo-1-w67e8 |\u001b[0m Epoch 1:  45%|████▌     | 73/162 [00:18<00:22,  3.89it/s, loss=114, v_num=0, train_loss_step=100.0, val_loss=199.0, train_loss_epoch=161.0]\n",
      "\u001b[36mcucd7st2m9-algo-1-w67e8 |\u001b[0m Epoch 1:  45%|████▌     | 73/162 [00:18<00:22,  3.89it/s, loss=113, v_num=0, train_loss_step=93.90, val_loss=199.0, train_loss_epoch=161.0]\n",
      "\u001b[36mcucd7st2m9-algo-1-w67e8 |\u001b[0m Epoch 1:  46%|████▌     | 74/162 [00:18<00:22,  3.90it/s, loss=113, v_num=0, train_loss_step=93.90, val_loss=199.0, train_loss_epoch=161.0]\n",
      "\u001b[36mcucd7st2m9-algo-1-w67e8 |\u001b[0m Epoch 1:  46%|████▌     | 74/162 [00:18<00:22,  3.90it/s, loss=115, v_num=0, train_loss_step=127.0, val_loss=199.0, train_loss_epoch=161.0]\n",
      "\u001b[36mcucd7st2m9-algo-1-w67e8 |\u001b[0m Epoch 1:  46%|████▋     | 75/162 [00:19<00:22,  3.90it/s, loss=115, v_num=0, train_loss_step=127.0, val_loss=199.0, train_loss_epoch=161.0]\n",
      "\u001b[36mcucd7st2m9-algo-1-w67e8 |\u001b[0m Epoch 1:  46%|████▋     | 75/162 [00:19<00:22,  3.90it/s, loss=115, v_num=0, train_loss_step=146.0, val_loss=199.0, train_loss_epoch=161.0]\n",
      "\u001b[36mcucd7st2m9-algo-1-w67e8 |\u001b[0m Epoch 1:  47%|████▋     | 76/162 [00:19<00:22,  3.90it/s, loss=115, v_num=0, train_loss_step=146.0, val_loss=199.0, train_loss_epoch=161.0]\n",
      "\u001b[36mcucd7st2m9-algo-1-w67e8 |\u001b[0m Epoch 1:  47%|████▋     | 76/162 [00:19<00:22,  3.90it/s, loss=116, v_num=0, train_loss_step=141.0, val_loss=199.0, train_loss_epoch=161.0]\n",
      "\u001b[36mcucd7st2m9-algo-1-w67e8 |\u001b[0m Epoch 1:  48%|████▊     | 77/162 [00:19<00:21,  3.91it/s, loss=116, v_num=0, train_loss_step=141.0, val_loss=199.0, train_loss_epoch=161.0]\n",
      "\u001b[36mcucd7st2m9-algo-1-w67e8 |\u001b[0m Epoch 1:  48%|████▊     | 77/162 [00:19<00:21,  3.91it/s, loss=115, v_num=0, train_loss_step=97.80, val_loss=199.0, train_loss_epoch=161.0]\n",
      "\u001b[36mcucd7st2m9-algo-1-w67e8 |\u001b[0m Epoch 1:  48%|████▊     | 78/162 [00:19<00:21,  3.91it/s, loss=115, v_num=0, train_loss_step=97.80, val_loss=199.0, train_loss_epoch=161.0]\n",
      "\u001b[36mcucd7st2m9-algo-1-w67e8 |\u001b[0m Epoch 1:  48%|████▊     | 78/162 [00:19<00:21,  3.91it/s, loss=115, v_num=0, train_loss_step=122.0, val_loss=199.0, train_loss_epoch=161.0]\n",
      "\u001b[36mcucd7st2m9-algo-1-w67e8 |\u001b[0m Epoch 1:  49%|████▉     | 79/162 [00:20<00:21,  3.91it/s, loss=115, v_num=0, train_loss_step=122.0, val_loss=199.0, train_loss_epoch=161.0]\n",
      "\u001b[36mcucd7st2m9-algo-1-w67e8 |\u001b[0m Epoch 1:  49%|████▉     | 79/162 [00:20<00:21,  3.91it/s, loss=114, v_num=0, train_loss_step=106.0, val_loss=199.0, train_loss_epoch=161.0]\n",
      "\u001b[36mcucd7st2m9-algo-1-w67e8 |\u001b[0m Epoch 1:  49%|████▉     | 80/162 [00:20<00:20,  3.91it/s, loss=114, v_num=0, train_loss_step=106.0, val_loss=199.0, train_loss_epoch=161.0]\n",
      "\u001b[36mcucd7st2m9-algo-1-w67e8 |\u001b[0m Epoch 1:  49%|████▉     | 80/162 [00:20<00:20,  3.91it/s, loss=115, v_num=0, train_loss_step=130.0, val_loss=199.0, train_loss_epoch=161.0]\n",
      "\u001b[36mcucd7st2m9-algo-1-w67e8 |\u001b[0m Epoch 1:  50%|█████     | 81/162 [00:20<00:20,  3.91it/s, loss=115, v_num=0, train_loss_step=130.0, val_loss=199.0, train_loss_epoch=161.0]\n",
      "\u001b[36mcucd7st2m9-algo-1-w67e8 |\u001b[0m Epoch 1:  50%|█████     | 81/162 [00:20<00:20,  3.91it/s, loss=116, v_num=0, train_loss_step=125.0, val_loss=199.0, train_loss_epoch=161.0]\n",
      "\u001b[36mcucd7st2m9-algo-1-w67e8 |\u001b[0m Epoch 1:  51%|█████     | 82/162 [00:20<00:20,  3.91it/s, loss=116, v_num=0, train_loss_step=125.0, val_loss=199.0, train_loss_epoch=161.0]\n",
      "\u001b[36mcucd7st2m9-algo-1-w67e8 |\u001b[0m Epoch 1:  51%|█████     | 82/162 [00:20<00:20,  3.91it/s, loss=119, v_num=0, train_loss_step=134.0, val_loss=199.0, train_loss_epoch=161.0]\n",
      "\u001b[36mcucd7st2m9-algo-1-w67e8 |\u001b[0m Epoch 1:  51%|█████     | 83/162 [00:21<00:20,  3.91it/s, loss=119, v_num=0, train_loss_step=134.0, val_loss=199.0, train_loss_epoch=161.0]\n",
      "\u001b[36mcucd7st2m9-algo-1-w67e8 |\u001b[0m Epoch 1:  51%|█████     | 83/162 [00:21<00:20,  3.91it/s, loss=118, v_num=0, train_loss_step=101.0, val_loss=199.0, train_loss_epoch=161.0]\n",
      "\u001b[36mcucd7st2m9-algo-1-w67e8 |\u001b[0m Epoch 1:  52%|█████▏    | 84/162 [00:21<00:19,  3.92it/s, loss=118, v_num=0, train_loss_step=101.0, val_loss=199.0, train_loss_epoch=161.0]\n",
      "\u001b[36mcucd7st2m9-algo-1-w67e8 |\u001b[0m Epoch 1:  52%|█████▏    | 84/162 [00:21<00:19,  3.92it/s, loss=118, v_num=0, train_loss_step=97.10, val_loss=199.0, train_loss_epoch=161.0]\n",
      "\u001b[36mcucd7st2m9-algo-1-w67e8 |\u001b[0m Epoch 1:  52%|█████▏    | 85/162 [00:21<00:19,  3.92it/s, loss=118, v_num=0, train_loss_step=97.10, val_loss=199.0, train_loss_epoch=161.0]\n",
      "\u001b[36mcucd7st2m9-algo-1-w67e8 |\u001b[0m Epoch 1:  52%|█████▏    | 85/162 [00:21<00:19,  3.92it/s, loss=116, v_num=0, train_loss_step=82.60, val_loss=199.0, train_loss_epoch=161.0]\n",
      "\u001b[36mcucd7st2m9-algo-1-w67e8 |\u001b[0m Epoch 1:  53%|█████▎    | 86/162 [00:21<00:19,  3.92it/s, loss=116, v_num=0, train_loss_step=82.60, val_loss=199.0, train_loss_epoch=161.0]\n",
      "\u001b[36mcucd7st2m9-algo-1-w67e8 |\u001b[0m Epoch 1:  53%|█████▎    | 86/162 [00:21<00:19,  3.92it/s, loss=112, v_num=0, train_loss_step=78.30, val_loss=199.0, train_loss_epoch=161.0]\n",
      "\u001b[36mcucd7st2m9-algo-1-w67e8 |\u001b[0m Epoch 1:  54%|█████▎    | 87/162 [00:22<00:19,  3.92it/s, loss=112, v_num=0, train_loss_step=78.30, val_loss=199.0, train_loss_epoch=161.0]\n",
      "\u001b[36mcucd7st2m9-algo-1-w67e8 |\u001b[0m Epoch 1:  54%|█████▎    | 87/162 [00:22<00:19,  3.92it/s, loss=113, v_num=0, train_loss_step=126.0, val_loss=199.0, train_loss_epoch=161.0]\n",
      "\u001b[36mcucd7st2m9-algo-1-w67e8 |\u001b[0m Epoch 1:  54%|█████▍    | 88/162 [00:22<00:18,  3.92it/s, loss=113, v_num=0, train_loss_step=126.0, val_loss=199.0, train_loss_epoch=161.0]\n",
      "\u001b[36mcucd7st2m9-algo-1-w67e8 |\u001b[0m Epoch 1:  54%|█████▍    | 88/162 [00:22<00:18,  3.92it/s, loss=114, v_num=0, train_loss_step=132.0, val_loss=199.0, train_loss_epoch=161.0]\n",
      "\u001b[36mcucd7st2m9-algo-1-w67e8 |\u001b[0m Epoch 1:  55%|█████▍    | 89/162 [00:22<00:18,  3.92it/s, loss=114, v_num=0, train_loss_step=132.0, val_loss=199.0, train_loss_epoch=161.0]\n",
      "\u001b[36mcucd7st2m9-algo-1-w67e8 |\u001b[0m Epoch 1:  55%|█████▍    | 89/162 [00:22<00:18,  3.92it/s, loss=112, v_num=0, train_loss_step=92.80, val_loss=199.0, train_loss_epoch=161.0]\n",
      "\u001b[36mcucd7st2m9-algo-1-w67e8 |\u001b[0m Epoch 1:  56%|█████▌    | 90/162 [00:22<00:18,  3.92it/s, loss=112, v_num=0, train_loss_step=92.80, val_loss=199.0, train_loss_epoch=161.0]\n",
      "\u001b[36mcucd7st2m9-algo-1-w67e8 |\u001b[0m Epoch 1:  56%|█████▌    | 90/162 [00:22<00:18,  3.92it/s, loss=111, v_num=0, train_loss_step=85.90, val_loss=199.0, train_loss_epoch=161.0]\n",
      "\u001b[36mcucd7st2m9-algo-1-w67e8 |\u001b[0m Epoch 1:  56%|█████▌    | 91/162 [00:23<00:18,  3.91it/s, loss=111, v_num=0, train_loss_step=85.90, val_loss=199.0, train_loss_epoch=161.0]\n",
      "\u001b[36mcucd7st2m9-algo-1-w67e8 |\u001b[0m Epoch 1:  56%|█████▌    | 91/162 [00:23<00:18,  3.91it/s, loss=110, v_num=0, train_loss_step=88.90, val_loss=199.0, train_loss_epoch=161.0]\n",
      "\u001b[36mcucd7st2m9-algo-1-w67e8 |\u001b[0m Epoch 1:  57%|█████▋    | 92/162 [00:23<00:17,  3.91it/s, loss=110, v_num=0, train_loss_step=88.90, val_loss=199.0, train_loss_epoch=161.0]\n",
      "\u001b[36mcucd7st2m9-algo-1-w67e8 |\u001b[0m Epoch 1:  57%|█████▋    | 92/162 [00:23<00:17,  3.91it/s, loss=111, v_num=0, train_loss_step=109.0, val_loss=199.0, train_loss_epoch=161.0]\n",
      "\u001b[36mcucd7st2m9-algo-1-w67e8 |\u001b[0m Epoch 1:  57%|█████▋    | 93/162 [00:23<00:17,  3.92it/s, loss=111, v_num=0, train_loss_step=109.0, val_loss=199.0, train_loss_epoch=161.0]\n",
      "\u001b[36mcucd7st2m9-algo-1-w67e8 |\u001b[0m Epoch 1:  57%|█████▋    | 93/162 [00:23<00:17,  3.92it/s, loss=113, v_num=0, train_loss_step=140.0, val_loss=199.0, train_loss_epoch=161.0]\n",
      "\u001b[36mcucd7st2m9-algo-1-w67e8 |\u001b[0m Epoch 1:  58%|█████▊    | 94/162 [00:23<00:17,  3.92it/s, loss=113, v_num=0, train_loss_step=140.0, val_loss=199.0, train_loss_epoch=161.0]\n",
      "\u001b[36mcucd7st2m9-algo-1-w67e8 |\u001b[0m Epoch 1:  58%|█████▊    | 94/162 [00:23<00:17,  3.92it/s, loss=113, v_num=0, train_loss_step=136.0, val_loss=199.0, train_loss_epoch=161.0]\n",
      "\u001b[36mcucd7st2m9-algo-1-w67e8 |\u001b[0m Epoch 1:  59%|█████▊    | 95/162 [00:24<00:17,  3.92it/s, loss=113, v_num=0, train_loss_step=136.0, val_loss=199.0, train_loss_epoch=161.0]\n",
      "\u001b[36mcucd7st2m9-algo-1-w67e8 |\u001b[0m Epoch 1:  59%|█████▊    | 95/162 [00:24<00:17,  3.92it/s, loss=113, v_num=0, train_loss_step=139.0, val_loss=199.0, train_loss_epoch=161.0]\n",
      "\u001b[36mcucd7st2m9-algo-1-w67e8 |\u001b[0m Epoch 1:  59%|█████▉    | 96/162 [00:24<00:16,  3.92it/s, loss=113, v_num=0, train_loss_step=139.0, val_loss=199.0, train_loss_epoch=161.0]\n",
      "\u001b[36mcucd7st2m9-algo-1-w67e8 |\u001b[0m Epoch 1:  59%|█████▉    | 96/162 [00:24<00:16,  3.92it/s, loss=112, v_num=0, train_loss_step=112.0, val_loss=199.0, train_loss_epoch=161.0]\n",
      "\u001b[36mcucd7st2m9-algo-1-w67e8 |\u001b[0m Epoch 1:  60%|█████▉    | 97/162 [00:24<00:16,  3.92it/s, loss=112, v_num=0, train_loss_step=112.0, val_loss=199.0, train_loss_epoch=161.0]\n",
      "\u001b[36mcucd7st2m9-algo-1-w67e8 |\u001b[0m Epoch 1:  60%|█████▉    | 97/162 [00:24<00:16,  3.92it/s, loss=114, v_num=0, train_loss_step=142.0, val_loss=199.0, train_loss_epoch=161.0]\n",
      "\u001b[36mcucd7st2m9-algo-1-w67e8 |\u001b[0m Epoch 1:  60%|██████    | 98/162 [00:25<00:16,  3.91it/s, loss=114, v_num=0, train_loss_step=142.0, val_loss=199.0, train_loss_epoch=161.0]\n",
      "\u001b[36mcucd7st2m9-algo-1-w67e8 |\u001b[0m Epoch 1:  60%|██████    | 98/162 [00:25<00:16,  3.91it/s, loss=114, v_num=0, train_loss_step=127.0, val_loss=199.0, train_loss_epoch=161.0]\n",
      "\u001b[36mcucd7st2m9-algo-1-w67e8 |\u001b[0m Epoch 1:  61%|██████    | 99/162 [00:25<00:16,  3.91it/s, loss=114, v_num=0, train_loss_step=127.0, val_loss=199.0, train_loss_epoch=161.0]\n",
      "\u001b[36mcucd7st2m9-algo-1-w67e8 |\u001b[0m Epoch 1:  61%|██████    | 99/162 [00:25<00:16,  3.91it/s, loss=113, v_num=0, train_loss_step=86.70, val_loss=199.0, train_loss_epoch=161.0]\n",
      "\u001b[36mcucd7st2m9-algo-1-w67e8 |\u001b[0m Epoch 1:  62%|██████▏   | 100/162 [00:25<00:15,  3.91it/s, loss=113, v_num=0, train_loss_step=86.70, val_loss=199.0, train_loss_epoch=161.0]\n",
      "\u001b[36mcucd7st2m9-algo-1-w67e8 |\u001b[0m Epoch 1:  62%|██████▏   | 100/162 [00:25<00:15,  3.91it/s, loss=111, v_num=0, train_loss_step=81.60, val_loss=199.0, train_loss_epoch=161.0]\n",
      "\u001b[36mcucd7st2m9-algo-1-w67e8 |\u001b[0m Epoch 1:  62%|██████▏   | 101/162 [00:25<00:15,  3.91it/s, loss=111, v_num=0, train_loss_step=81.60, val_loss=199.0, train_loss_epoch=161.0]\n",
      "\u001b[36mcucd7st2m9-algo-1-w67e8 |\u001b[0m Epoch 1:  62%|██████▏   | 101/162 [00:25<00:15,  3.91it/s, loss=109, v_num=0, train_loss_step=94.20, val_loss=199.0, train_loss_epoch=161.0]\n",
      "\u001b[36mcucd7st2m9-algo-1-w67e8 |\u001b[0m Epoch 1:  63%|██████▎   | 102/162 [00:26<00:15,  3.91it/s, loss=109, v_num=0, train_loss_step=94.20, val_loss=199.0, train_loss_epoch=161.0]\n",
      "\u001b[36mcucd7st2m9-algo-1-w67e8 |\u001b[0m Epoch 1:  63%|██████▎   | 102/162 [00:26<00:15,  3.91it/s, loss=107, v_num=0, train_loss_step=98.00, val_loss=199.0, train_loss_epoch=161.0]\n",
      "\u001b[36mcucd7st2m9-algo-1-w67e8 |\u001b[0m Epoch 1:  64%|██████▎   | 103/162 [00:26<00:15,  3.91it/s, loss=107, v_num=0, train_loss_step=98.00, val_loss=199.0, train_loss_epoch=161.0]\n",
      "\u001b[36mcucd7st2m9-algo-1-w67e8 |\u001b[0m Epoch 1:  64%|██████▎   | 103/162 [00:26<00:15,  3.91it/s, loss=108, v_num=0, train_loss_step=108.0, val_loss=199.0, train_loss_epoch=161.0]\n",
      "\u001b[36mcucd7st2m9-algo-1-w67e8 |\u001b[0m Epoch 1:  64%|██████▍   | 104/162 [00:26<00:14,  3.91it/s, loss=108, v_num=0, train_loss_step=108.0, val_loss=199.0, train_loss_epoch=161.0]\n",
      "\u001b[36mcucd7st2m9-algo-1-w67e8 |\u001b[0m Epoch 1:  64%|██████▍   | 104/162 [00:26<00:14,  3.91it/s, loss=107, v_num=0, train_loss_step=83.20, val_loss=199.0, train_loss_epoch=161.0]\n",
      "\u001b[36mcucd7st2m9-algo-1-w67e8 |\u001b[0m Epoch 1:  65%|██████▍   | 105/162 [00:26<00:14,  3.91it/s, loss=107, v_num=0, train_loss_step=83.20, val_loss=199.0, train_loss_epoch=161.0]\n",
      "\u001b[36mcucd7st2m9-algo-1-w67e8 |\u001b[0m Epoch 1:  65%|██████▍   | 105/162 [00:26<00:14,  3.91it/s, loss=108, v_num=0, train_loss_step=91.20, val_loss=199.0, train_loss_epoch=161.0]\n",
      "\u001b[36mcucd7st2m9-algo-1-w67e8 |\u001b[0m Epoch 1:  65%|██████▌   | 106/162 [00:27<00:14,  3.91it/s, loss=108, v_num=0, train_loss_step=91.20, val_loss=199.0, train_loss_epoch=161.0]\n",
      "\u001b[36mcucd7st2m9-algo-1-w67e8 |\u001b[0m Epoch 1:  65%|██████▌   | 106/162 [00:27<00:14,  3.91it/s, loss=109, v_num=0, train_loss_step=105.0, val_loss=199.0, train_loss_epoch=161.0]\n",
      "\u001b[36mcucd7st2m9-algo-1-w67e8 |\u001b[0m Epoch 1:  66%|██████▌   | 107/162 [00:27<00:14,  3.91it/s, loss=109, v_num=0, train_loss_step=105.0, val_loss=199.0, train_loss_epoch=161.0]\n",
      "\u001b[36mcucd7st2m9-algo-1-w67e8 |\u001b[0m Epoch 1:  66%|██████▌   | 107/162 [00:27<00:14,  3.91it/s, loss=110, v_num=0, train_loss_step=151.0, val_loss=199.0, train_loss_epoch=161.0]\n",
      "\u001b[36mcucd7st2m9-algo-1-w67e8 |\u001b[0m Epoch 1:  67%|██████▋   | 108/162 [00:27<00:13,  3.91it/s, loss=110, v_num=0, train_loss_step=151.0, val_loss=199.0, train_loss_epoch=161.0]\n",
      "\u001b[36mcucd7st2m9-algo-1-w67e8 |\u001b[0m Epoch 1:  67%|██████▋   | 108/162 [00:27<00:13,  3.91it/s, loss=108, v_num=0, train_loss_step=79.60, val_loss=199.0, train_loss_epoch=161.0]\n",
      "\u001b[36mcucd7st2m9-algo-1-w67e8 |\u001b[0m Epoch 1:  67%|██████▋   | 109/162 [00:27<00:13,  3.91it/s, loss=108, v_num=0, train_loss_step=79.60, val_loss=199.0, train_loss_epoch=161.0]\n",
      "\u001b[36mcucd7st2m9-algo-1-w67e8 |\u001b[0m Epoch 1:  67%|██████▋   | 109/162 [00:27<00:13,  3.91it/s, loss=110, v_num=0, train_loss_step=151.0, val_loss=199.0, train_loss_epoch=161.0]\n",
      "\u001b[36mcucd7st2m9-algo-1-w67e8 |\u001b[0m Epoch 1:  68%|██████▊   | 110/162 [00:28<00:13,  3.91it/s, loss=110, v_num=0, train_loss_step=151.0, val_loss=199.0, train_loss_epoch=161.0]\n",
      "\u001b[36mcucd7st2m9-algo-1-w67e8 |\u001b[0m Epoch 1:  68%|██████▊   | 110/162 [00:28<00:13,  3.91it/s, loss=111, v_num=0, train_loss_step=99.70, val_loss=199.0, train_loss_epoch=161.0]\n",
      "\u001b[36mcucd7st2m9-algo-1-w67e8 |\u001b[0m Epoch 1:  69%|██████▊   | 111/162 [00:28<00:13,  3.91it/s, loss=111, v_num=0, train_loss_step=99.70, val_loss=199.0, train_loss_epoch=161.0]\n",
      "\u001b[36mcucd7st2m9-algo-1-w67e8 |\u001b[0m Epoch 1:  69%|██████▊   | 111/162 [00:28<00:13,  3.91it/s, loss=114, v_num=0, train_loss_step=141.0, val_loss=199.0, train_loss_epoch=161.0]\n",
      "\u001b[36mcucd7st2m9-algo-1-w67e8 |\u001b[0m Epoch 1:  69%|██████▉   | 112/162 [00:28<00:12,  3.91it/s, loss=114, v_num=0, train_loss_step=141.0, val_loss=199.0, train_loss_epoch=161.0]\n",
      "\u001b[36mcucd7st2m9-algo-1-w67e8 |\u001b[0m Epoch 1:  69%|██████▉   | 112/162 [00:28<00:12,  3.91it/s, loss=114, v_num=0, train_loss_step=113.0, val_loss=199.0, train_loss_epoch=161.0]\n",
      "\u001b[36mcucd7st2m9-algo-1-w67e8 |\u001b[0m Epoch 1:  70%|██████▉   | 113/162 [00:28<00:12,  3.91it/s, loss=114, v_num=0, train_loss_step=113.0, val_loss=199.0, train_loss_epoch=161.0]\n",
      "\u001b[36mcucd7st2m9-algo-1-w67e8 |\u001b[0m Epoch 1:  70%|██████▉   | 113/162 [00:28<00:12,  3.91it/s, loss=112, v_num=0, train_loss_step=110.0, val_loss=199.0, train_loss_epoch=161.0]\n",
      "\u001b[36mcucd7st2m9-algo-1-w67e8 |\u001b[0m Epoch 1:  70%|███████   | 114/162 [00:29<00:12,  3.91it/s, loss=112, v_num=0, train_loss_step=110.0, val_loss=199.0, train_loss_epoch=161.0]\n",
      "\u001b[36mcucd7st2m9-algo-1-w67e8 |\u001b[0m Epoch 1:  70%|███████   | 114/162 [00:29<00:12,  3.91it/s, loss=110, v_num=0, train_loss_step=89.20, val_loss=199.0, train_loss_epoch=161.0]\n",
      "\u001b[36mcucd7st2m9-algo-1-w67e8 |\u001b[0m Epoch 1:  71%|███████   | 115/162 [00:29<00:12,  3.90it/s, loss=110, v_num=0, train_loss_step=89.20, val_loss=199.0, train_loss_epoch=161.0]\n",
      "\u001b[36mcucd7st2m9-algo-1-w67e8 |\u001b[0m Epoch 1:  71%|███████   | 115/162 [00:29<00:12,  3.90it/s, loss=109, v_num=0, train_loss_step=116.0, val_loss=199.0, train_loss_epoch=161.0]\n",
      "\u001b[36mcucd7st2m9-algo-1-w67e8 |\u001b[0m Epoch 1:  72%|███████▏  | 116/162 [00:29<00:11,  3.91it/s, loss=109, v_num=0, train_loss_step=116.0, val_loss=199.0, train_loss_epoch=161.0]\n",
      "\u001b[36mcucd7st2m9-algo-1-w67e8 |\u001b[0m Epoch 1:  72%|███████▏  | 116/162 [00:29<00:11,  3.91it/s, loss=110, v_num=0, train_loss_step=127.0, val_loss=199.0, train_loss_epoch=161.0]\n",
      "\u001b[36mcucd7st2m9-algo-1-w67e8 |\u001b[0m Epoch 1:  72%|███████▏  | 117/162 [00:29<00:11,  3.91it/s, loss=110, v_num=0, train_loss_step=127.0, val_loss=199.0, train_loss_epoch=161.0]\n",
      "\u001b[36mcucd7st2m9-algo-1-w67e8 |\u001b[0m Epoch 1:  72%|███████▏  | 117/162 [00:29<00:11,  3.91it/s, loss=107, v_num=0, train_loss_step=91.30, val_loss=199.0, train_loss_epoch=161.0]\n",
      "\u001b[36mcucd7st2m9-algo-1-w67e8 |\u001b[0m Epoch 1:  73%|███████▎  | 118/162 [00:30<00:11,  3.91it/s, loss=107, v_num=0, train_loss_step=91.30, val_loss=199.0, train_loss_epoch=161.0]\n",
      "\u001b[36mcucd7st2m9-algo-1-w67e8 |\u001b[0m Epoch 1:  73%|███████▎  | 118/162 [00:30<00:11,  3.91it/s, loss=105, v_num=0, train_loss_step=84.00, val_loss=199.0, train_loss_epoch=161.0]\n",
      "\u001b[36mcucd7st2m9-algo-1-w67e8 |\u001b[0m Epoch 1:  73%|███████▎  | 119/162 [00:30<00:11,  3.91it/s, loss=105, v_num=0, train_loss_step=84.00, val_loss=199.0, train_loss_epoch=161.0]\n",
      "\u001b[36mcucd7st2m9-algo-1-w67e8 |\u001b[0m Epoch 1:  73%|███████▎  | 119/162 [00:30<00:11,  3.91it/s, loss=107, v_num=0, train_loss_step=132.0, val_loss=199.0, train_loss_epoch=161.0]\n",
      "\u001b[36mcucd7st2m9-algo-1-w67e8 |\u001b[0m Epoch 1:  74%|███████▍  | 120/162 [00:30<00:10,  3.91it/s, loss=107, v_num=0, train_loss_step=132.0, val_loss=199.0, train_loss_epoch=161.0]\n",
      "\u001b[36mcucd7st2m9-algo-1-w67e8 |\u001b[0m Epoch 1:  74%|███████▍  | 120/162 [00:30<00:10,  3.91it/s, loss=108, v_num=0, train_loss_step=86.60, val_loss=199.0, train_loss_epoch=161.0]\n",
      "\u001b[36mcucd7st2m9-algo-1-w67e8 |\u001b[0m Epoch 1:  75%|███████▍  | 121/162 [00:31<00:10,  3.90it/s, loss=108, v_num=0, train_loss_step=86.60, val_loss=199.0, train_loss_epoch=161.0]\n",
      "\u001b[36mcucd7st2m9-algo-1-w67e8 |\u001b[0m Epoch 1:  75%|███████▍  | 121/162 [00:31<00:10,  3.90it/s, loss=109, v_num=0, train_loss_step=128.0, val_loss=199.0, train_loss_epoch=161.0]\n",
      "\u001b[36mcucd7st2m9-algo-1-w67e8 |\u001b[0m Epoch 1:  75%|███████▌  | 122/162 [00:31<00:10,  3.90it/s, loss=109, v_num=0, train_loss_step=128.0, val_loss=199.0, train_loss_epoch=161.0]\n",
      "\u001b[36mcucd7st2m9-algo-1-w67e8 |\u001b[0m Epoch 1:  75%|███████▌  | 122/162 [00:31<00:10,  3.90it/s, loss=110, v_num=0, train_loss_step=106.0, val_loss=199.0, train_loss_epoch=161.0]\n",
      "\u001b[36mcucd7st2m9-algo-1-w67e8 |\u001b[0m Epoch 1:  76%|███████▌  | 123/162 [00:31<00:10,  3.90it/s, loss=110, v_num=0, train_loss_step=106.0, val_loss=199.0, train_loss_epoch=161.0]\n",
      "\u001b[36mcucd7st2m9-algo-1-w67e8 |\u001b[0m Epoch 1:  76%|███████▌  | 123/162 [00:31<00:10,  3.90it/s, loss=110, v_num=0, train_loss_step=109.0, val_loss=199.0, train_loss_epoch=161.0]\n",
      "\u001b[36mcucd7st2m9-algo-1-w67e8 |\u001b[0m Epoch 1:  77%|███████▋  | 124/162 [00:31<00:09,  3.89it/s, loss=110, v_num=0, train_loss_step=109.0, val_loss=199.0, train_loss_epoch=161.0]\n",
      "\u001b[36mcucd7st2m9-algo-1-w67e8 |\u001b[0m Epoch 1:  77%|███████▋  | 124/162 [00:31<00:09,  3.89it/s, loss=110, v_num=0, train_loss_step=86.60, val_loss=199.0, train_loss_epoch=161.0]\n",
      "\u001b[36mcucd7st2m9-algo-1-w67e8 |\u001b[0m Epoch 1:  77%|███████▋  | 125/162 [00:32<00:09,  3.89it/s, loss=110, v_num=0, train_loss_step=86.60, val_loss=199.0, train_loss_epoch=161.0]\n",
      "\u001b[36mcucd7st2m9-algo-1-w67e8 |\u001b[0m Epoch 1:  77%|███████▋  | 125/162 [00:32<00:09,  3.89it/s, loss=110, v_num=0, train_loss_step=103.0, val_loss=199.0, train_loss_epoch=161.0]\n",
      "\u001b[36mcucd7st2m9-algo-1-w67e8 |\u001b[0m Epoch 1:  78%|███████▊  | 126/162 [00:32<00:09,  3.89it/s, loss=110, v_num=0, train_loss_step=103.0, val_loss=199.0, train_loss_epoch=161.0]\n",
      "\u001b[36mcucd7st2m9-algo-1-w67e8 |\u001b[0m Epoch 1:  78%|███████▊  | 126/162 [00:32<00:09,  3.89it/s, loss=111, v_num=0, train_loss_step=114.0, val_loss=199.0, train_loss_epoch=161.0]\n",
      "\u001b[36mcucd7st2m9-algo-1-w67e8 |\u001b[0m Epoch 1:  78%|███████▊  | 127/162 [00:32<00:09,  3.89it/s, loss=111, v_num=0, train_loss_step=114.0, val_loss=199.0, train_loss_epoch=161.0]\n",
      "\u001b[36mcucd7st2m9-algo-1-w67e8 |\u001b[0m Epoch 1:  78%|███████▊  | 127/162 [00:32<00:09,  3.89it/s, loss=108, v_num=0, train_loss_step=101.0, val_loss=199.0, train_loss_epoch=161.0]\n",
      "\u001b[36mcucd7st2m9-algo-1-w67e8 |\u001b[0m Epoch 1:  79%|███████▉  | 128/162 [00:32<00:08,  3.89it/s, loss=108, v_num=0, train_loss_step=101.0, val_loss=199.0, train_loss_epoch=161.0]\n",
      "\u001b[36mcucd7st2m9-algo-1-w67e8 |\u001b[0m Epoch 1:  79%|███████▉  | 128/162 [00:32<00:08,  3.89it/s, loss=108, v_num=0, train_loss_step=80.50, val_loss=199.0, train_loss_epoch=161.0]\n",
      "\u001b[36mcucd7st2m9-algo-1-w67e8 |\u001b[0m Epoch 1:  80%|███████▉  | 129/162 [00:33<00:08,  3.89it/s, loss=108, v_num=0, train_loss_step=80.50, val_loss=199.0, train_loss_epoch=161.0]\n",
      "\u001b[36mcucd7st2m9-algo-1-w67e8 |\u001b[0m Epoch 1:  80%|███████▉  | 129/162 [00:33<00:08,  3.89it/s, loss=105, v_num=0, train_loss_step=74.70, val_loss=199.0, train_loss_epoch=161.0]\n",
      "\u001b[36mcucd7st2m9-algo-1-w67e8 |\u001b[0m Epoch 1:  80%|████████  | 130/162 [00:33<00:08,  3.89it/s, loss=105, v_num=0, train_loss_step=74.70, val_loss=199.0, train_loss_epoch=161.0]\n",
      "\u001b[36mcucd7st2m9-algo-1-w67e8 |\u001b[0m Epoch 1:  80%|████████  | 130/162 [00:33<00:08,  3.89it/s, loss=105, v_num=0, train_loss_step=115.0, val_loss=199.0, train_loss_epoch=161.0]\n",
      "\u001b[36mcucd7st2m9-algo-1-w67e8 |\u001b[0m Epoch 1:  81%|████████  | 131/162 [00:33<00:07,  3.89it/s, loss=105, v_num=0, train_loss_step=115.0, val_loss=199.0, train_loss_epoch=161.0]\n",
      "\u001b[36mcucd7st2m9-algo-1-w67e8 |\u001b[0m Epoch 1:  81%|████████  | 131/162 [00:33<00:07,  3.89it/s, loss=103, v_num=0, train_loss_step=86.60, val_loss=199.0, train_loss_epoch=161.0]\n",
      "\u001b[36mcucd7st2m9-algo-1-w67e8 |\u001b[0m Epoch 1:  81%|████████▏ | 132/162 [00:33<00:07,  3.89it/s, loss=103, v_num=0, train_loss_step=86.60, val_loss=199.0, train_loss_epoch=161.0]\n",
      "\u001b[36mcucd7st2m9-algo-1-w67e8 |\u001b[0m Epoch 1:  81%|████████▏ | 132/162 [00:33<00:07,  3.89it/s, loss=101, v_num=0, train_loss_step=87.20, val_loss=199.0, train_loss_epoch=161.0]\n",
      "\u001b[36mcucd7st2m9-algo-1-w67e8 |\u001b[0m Epoch 1:  82%|████████▏ | 133/162 [00:34<00:07,  3.89it/s, loss=101, v_num=0, train_loss_step=87.20, val_loss=199.0, train_loss_epoch=161.0]\n",
      "\u001b[36mcucd7st2m9-algo-1-w67e8 |\u001b[0m Epoch 1:  82%|████████▏ | 133/162 [00:34<00:07,  3.89it/s, loss=101, v_num=0, train_loss_step=108.0, val_loss=199.0, train_loss_epoch=161.0]\n",
      "\u001b[36mcucd7st2m9-algo-1-w67e8 |\u001b[0m Epoch 1:  83%|████████▎ | 134/162 [00:34<00:07,  3.89it/s, loss=101, v_num=0, train_loss_step=108.0, val_loss=199.0, train_loss_epoch=161.0]\n",
      "\u001b[36mcucd7st2m9-algo-1-w67e8 |\u001b[0m Epoch 1:  83%|████████▎ | 134/162 [00:34<00:07,  3.89it/s, loss=101, v_num=0, train_loss_step=80.70, val_loss=199.0, train_loss_epoch=161.0]\n",
      "\u001b[36mcucd7st2m9-algo-1-w67e8 |\u001b[0m Epoch 1:  83%|████████▎ | 135/162 [00:34<00:06,  3.89it/s, loss=101, v_num=0, train_loss_step=80.70, val_loss=199.0, train_loss_epoch=161.0]\n",
      "\u001b[36mcucd7st2m9-algo-1-w67e8 |\u001b[0m Epoch 1:  83%|████████▎ | 135/162 [00:34<00:06,  3.89it/s, loss=101, v_num=0, train_loss_step=127.0, val_loss=199.0, train_loss_epoch=161.0]\n",
      "\u001b[36mcucd7st2m9-algo-1-w67e8 |\u001b[0m Epoch 1:  84%|████████▍ | 136/162 [00:34<00:06,  3.89it/s, loss=101, v_num=0, train_loss_step=127.0, val_loss=199.0, train_loss_epoch=161.0]\n",
      "\u001b[36mcucd7st2m9-algo-1-w67e8 |\u001b[0m Epoch 1:  84%|████████▍ | 136/162 [00:34<00:06,  3.89it/s, loss=101, v_num=0, train_loss_step=115.0, val_loss=199.0, train_loss_epoch=161.0]\n",
      "\u001b[36mcucd7st2m9-algo-1-w67e8 |\u001b[0m Epoch 1:  85%|████████▍ | 137/162 [00:35<00:06,  3.89it/s, loss=101, v_num=0, train_loss_step=115.0, val_loss=199.0, train_loss_epoch=161.0]\n",
      "\u001b[36mcucd7st2m9-algo-1-w67e8 |\u001b[0m Epoch 1:  85%|████████▍ | 137/162 [00:35<00:06,  3.89it/s, loss=103, v_num=0, train_loss_step=140.0, val_loss=199.0, train_loss_epoch=161.0]\n",
      "\u001b[36mcucd7st2m9-algo-1-w67e8 |\u001b[0m Epoch 1:  85%|████████▌ | 138/162 [00:35<00:06,  3.89it/s, loss=103, v_num=0, train_loss_step=140.0, val_loss=199.0, train_loss_epoch=161.0]\n",
      "\u001b[36mcucd7st2m9-algo-1-w67e8 |\u001b[0m Epoch 1:  85%|████████▌ | 138/162 [00:35<00:06,  3.89it/s, loss=104, v_num=0, train_loss_step=99.30, val_loss=199.0, train_loss_epoch=161.0]\n",
      "\u001b[36mcucd7st2m9-algo-1-w67e8 |\u001b[0m Epoch 1:  86%|████████▌ | 139/162 [00:35<00:05,  3.89it/s, loss=104, v_num=0, train_loss_step=99.30, val_loss=199.0, train_loss_epoch=161.0]\n",
      "\u001b[36mcucd7st2m9-algo-1-w67e8 |\u001b[0m Epoch 1:  86%|████████▌ | 139/162 [00:35<00:05,  3.89it/s, loss=104, v_num=0, train_loss_step=129.0, val_loss=199.0, train_loss_epoch=161.0]\n",
      "\u001b[36mcucd7st2m9-algo-1-w67e8 |\u001b[0m Epoch 1:  86%|████████▋ | 140/162 [00:36<00:05,  3.89it/s, loss=104, v_num=0, train_loss_step=129.0, val_loss=199.0, train_loss_epoch=161.0]\n",
      "\u001b[36mcucd7st2m9-algo-1-w67e8 |\u001b[0m Epoch 1:  86%|████████▋ | 140/162 [00:36<00:05,  3.89it/s, loss=103, v_num=0, train_loss_step=73.40, val_loss=199.0, train_loss_epoch=161.0]\n",
      "\u001b[36mcucd7st2m9-algo-1-w67e8 |\u001b[0m Epoch 1:  87%|████████▋ | 141/162 [00:36<00:05,  3.89it/s, loss=103, v_num=0, train_loss_step=73.40, val_loss=199.0, train_loss_epoch=161.0]\n",
      "\u001b[36mcucd7st2m9-algo-1-w67e8 |\u001b[0m Epoch 1:  87%|████████▋ | 141/162 [00:36<00:05,  3.89it/s, loss=102, v_num=0, train_loss_step=95.50, val_loss=199.0, train_loss_epoch=161.0]\n",
      "\u001b[36mcucd7st2m9-algo-1-w67e8 |\u001b[0m Epoch 1:  88%|████████▊ | 142/162 [00:36<00:05,  3.88it/s, loss=102, v_num=0, train_loss_step=95.50, val_loss=199.0, train_loss_epoch=161.0]\n",
      "\u001b[36mcucd7st2m9-algo-1-w67e8 |\u001b[0m Epoch 1:  88%|████████▊ | 142/162 [00:36<00:05,  3.88it/s, loss=102, v_num=0, train_loss_step=125.0, val_loss=199.0, train_loss_epoch=161.0]\n",
      "\u001b[36mcucd7st2m9-algo-1-w67e8 |\u001b[0m Epoch 1:  88%|████████▊ | 143/162 [00:36<00:04,  3.88it/s, loss=102, v_num=0, train_loss_step=125.0, val_loss=199.0, train_loss_epoch=161.0]\n",
      "\u001b[36mcucd7st2m9-algo-1-w67e8 |\u001b[0m Epoch 1:  88%|████████▊ | 143/162 [00:36<00:04,  3.88it/s, loss=102, v_num=0, train_loss_step=103.0, val_loss=199.0, train_loss_epoch=161.0]\n",
      "\u001b[36mcucd7st2m9-algo-1-w67e8 |\u001b[0m Epoch 1:  89%|████████▉ | 144/162 [00:37<00:04,  3.89it/s, loss=102, v_num=0, train_loss_step=103.0, val_loss=199.0, train_loss_epoch=161.0]\n",
      "\u001b[36mcucd7st2m9-algo-1-w67e8 |\u001b[0m Epoch 1:  89%|████████▉ | 144/162 [00:37<00:04,  3.89it/s, loss=102, v_num=0, train_loss_step=91.60, val_loss=199.0, train_loss_epoch=161.0]\n",
      "\u001b[36mcucd7st2m9-algo-1-w67e8 |\u001b[0m Epoch 1:  90%|████████▉ | 145/162 [00:37<00:04,  3.88it/s, loss=102, v_num=0, train_loss_step=91.60, val_loss=199.0, train_loss_epoch=161.0]\n",
      "\u001b[36mcucd7st2m9-algo-1-w67e8 |\u001b[0m Epoch 1:  90%|████████▉ | 145/162 [00:37<00:04,  3.88it/s, loss=103, v_num=0, train_loss_step=124.0, val_loss=199.0, train_loss_epoch=161.0]\n",
      "\u001b[36mcucd7st2m9-algo-1-w67e8 |\u001b[0m Epoch 1:  90%|█████████ | 146/162 [00:37<00:04,  3.88it/s, loss=103, v_num=0, train_loss_step=124.0, val_loss=199.0, train_loss_epoch=161.0]\n",
      "\u001b[36mcucd7st2m9-algo-1-w67e8 |\u001b[0m Epoch 1:  90%|█████████ | 146/162 [00:37<00:04,  3.88it/s, loss=103, v_num=0, train_loss_step=100.0, val_loss=199.0, train_loss_epoch=161.0]\n",
      "\u001b[36mcucd7st2m9-algo-1-w67e8 |\u001b[0m Epoch 1:  91%|█████████ | 147/162 [00:37<00:03,  3.88it/s, loss=103, v_num=0, train_loss_step=100.0, val_loss=199.0, train_loss_epoch=161.0]\n",
      "\u001b[36mcucd7st2m9-algo-1-w67e8 |\u001b[0m Epoch 1:  91%|█████████ | 147/162 [00:37<00:03,  3.88it/s, loss=105, v_num=0, train_loss_step=147.0, val_loss=199.0, train_loss_epoch=161.0]\n",
      "\u001b[36mcucd7st2m9-algo-1-w67e8 |\u001b[0m Epoch 1:  91%|█████████▏| 148/162 [00:38<00:03,  3.88it/s, loss=105, v_num=0, train_loss_step=147.0, val_loss=199.0, train_loss_epoch=161.0]\n",
      "\u001b[36mcucd7st2m9-algo-1-w67e8 |\u001b[0m Epoch 1:  91%|█████████▏| 148/162 [00:38<00:03,  3.88it/s, loss=106, v_num=0, train_loss_step=98.00, val_loss=199.0, train_loss_epoch=161.0]\n",
      "\u001b[36mcucd7st2m9-algo-1-w67e8 |\u001b[0m Epoch 1:  92%|█████████▏| 149/162 [00:38<00:03,  3.88it/s, loss=106, v_num=0, train_loss_step=98.00, val_loss=199.0, train_loss_epoch=161.0]\n",
      "\u001b[36mcucd7st2m9-algo-1-w67e8 |\u001b[0m Epoch 1:  92%|█████████▏| 149/162 [00:38<00:03,  3.88it/s, loss=107, v_num=0, train_loss_step=100.0, val_loss=199.0, train_loss_epoch=161.0]\n",
      "\u001b[36mcucd7st2m9-algo-1-w67e8 |\u001b[0m Epoch 1:  93%|█████████▎| 150/162 [00:38<00:03,  3.88it/s, loss=107, v_num=0, train_loss_step=100.0, val_loss=199.0, train_loss_epoch=161.0]\n",
      "\u001b[36mcucd7st2m9-algo-1-w67e8 |\u001b[0m Epoch 1:  93%|█████████▎| 150/162 [00:38<00:03,  3.88it/s, loss=107, v_num=0, train_loss_step=112.0, val_loss=199.0, train_loss_epoch=161.0]\n",
      "\u001b[36mcucd7st2m9-algo-1-w67e8 |\u001b[0m Epoch 1:  93%|█████████▎| 151/162 [00:39<00:02,  3.87it/s, loss=107, v_num=0, train_loss_step=112.0, val_loss=199.0, train_loss_epoch=161.0]\n",
      "\u001b[36mcucd7st2m9-algo-1-w67e8 |\u001b[0m Epoch 1:  93%|█████████▎| 151/162 [00:39<00:02,  3.87it/s, loss=108, v_num=0, train_loss_step=104.0, val_loss=199.0, train_loss_epoch=161.0]\n",
      "\u001b[36mcucd7st2m9-algo-1-w67e8 |\u001b[0m Epoch 1:  94%|█████████▍| 152/162 [00:39<00:02,  3.87it/s, loss=108, v_num=0, train_loss_step=104.0, val_loss=199.0, train_loss_epoch=161.0]\n",
      "\u001b[36mcucd7st2m9-algo-1-w67e8 |\u001b[0m Epoch 1:  94%|█████████▍| 152/162 [00:39<00:02,  3.87it/s, loss=108, v_num=0, train_loss_step=89.40, val_loss=199.0, train_loss_epoch=161.0]\n",
      "\u001b[36mcucd7st2m9-algo-1-w67e8 |\u001b[0m Epoch 1:  94%|█████████▍| 153/162 [00:39<00:02,  3.87it/s, loss=108, v_num=0, train_loss_step=89.40, val_loss=199.0, train_loss_epoch=161.0]\n",
      "\u001b[36mcucd7st2m9-algo-1-w67e8 |\u001b[0m Epoch 1:  94%|█████████▍| 153/162 [00:39<00:02,  3.87it/s, loss=107, v_num=0, train_loss_step=78.10, val_loss=199.0, train_loss_epoch=161.0]\n",
      "\u001b[36mcucd7st2m9-algo-1-w67e8 |\u001b[0m Epoch 1:  95%|█████████▌| 154/162 [00:39<00:02,  3.87it/s, loss=107, v_num=0, train_loss_step=78.10, val_loss=199.0, train_loss_epoch=161.0]\n",
      "\u001b[36mcucd7st2m9-algo-1-w67e8 |\u001b[0m Epoch 1:  95%|█████████▌| 154/162 [00:39<00:02,  3.87it/s, loss=107, v_num=0, train_loss_step=92.60, val_loss=199.0, train_loss_epoch=161.0]\n",
      "\u001b[36mcucd7st2m9-algo-1-w67e8 |\u001b[0m Epoch 1:  96%|█████████▌| 155/162 [00:40<00:01,  3.87it/s, loss=107, v_num=0, train_loss_step=92.60, val_loss=199.0, train_loss_epoch=161.0]\n",
      "\u001b[36mcucd7st2m9-algo-1-w67e8 |\u001b[0m Epoch 1:  96%|█████████▌| 155/162 [00:40<00:01,  3.87it/s, loss=107, v_num=0, train_loss_step=114.0, val_loss=199.0, train_loss_epoch=161.0]\n",
      "\u001b[36mcucd7st2m9-algo-1-w67e8 |\u001b[0m Epoch 1:  96%|█████████▋| 156/162 [00:40<00:01,  3.87it/s, loss=107, v_num=0, train_loss_step=114.0, val_loss=199.0, train_loss_epoch=161.0]\n",
      "\u001b[36mcucd7st2m9-algo-1-w67e8 |\u001b[0m Epoch 1:  96%|█████████▋| 156/162 [00:40<00:01,  3.87it/s, loss=106, v_num=0, train_loss_step=106.0, val_loss=199.0, train_loss_epoch=161.0]\n",
      "\u001b[36mcucd7st2m9-algo-1-w67e8 |\u001b[0m Epoch 1:  97%|█████████▋| 157/162 [00:40<00:01,  3.87it/s, loss=106, v_num=0, train_loss_step=106.0, val_loss=199.0, train_loss_epoch=161.0]\n",
      "\u001b[36mcucd7st2m9-algo-1-w67e8 |\u001b[0m Epoch 1:  97%|█████████▋| 157/162 [00:40<00:01,  3.87it/s, loss=105, v_num=0, train_loss_step=122.0, val_loss=199.0, train_loss_epoch=161.0]\n",
      "\u001b[36mcucd7st2m9-algo-1-w67e8 |\u001b[0m Epoch 1:  98%|█████████▊| 158/162 [00:40<00:01,  3.87it/s, loss=105, v_num=0, train_loss_step=122.0, val_loss=199.0, train_loss_epoch=161.0]\n",
      "\u001b[36mcucd7st2m9-algo-1-w67e8 |\u001b[0m Epoch 1:  98%|█████████▊| 158/162 [00:40<00:01,  3.87it/s, loss=106, v_num=0, train_loss_step=105.0, val_loss=199.0, train_loss_epoch=161.0]\n",
      "\u001b[36mcucd7st2m9-algo-1-w67e8 |\u001b[0m Epoch 1:  98%|█████████▊| 159/162 [00:41<00:00,  3.87it/s, loss=106, v_num=0, train_loss_step=105.0, val_loss=199.0, train_loss_epoch=161.0]\n",
      "\u001b[36mcucd7st2m9-algo-1-w67e8 |\u001b[0m Epoch 1:  98%|█████████▊| 159/162 [00:41<00:00,  3.87it/s, loss=103, v_num=0, train_loss_step=87.50, val_loss=199.0, train_loss_epoch=161.0]\n",
      "\u001b[36mcucd7st2m9-algo-1-w67e8 |\u001b[0m Epoch 1:  99%|█████████▉| 160/162 [00:41<00:00,  3.88it/s, loss=103, v_num=0, train_loss_step=87.50, val_loss=199.0, train_loss_epoch=161.0]\n",
      "\u001b[36mcucd7st2m9-algo-1-w67e8 |\u001b[0m Epoch 1:  99%|█████████▉| 160/162 [00:41<00:00,  3.88it/s, loss=104, v_num=0, train_loss_step=81.20, val_loss=199.0, train_loss_epoch=161.0]\n",
      "\u001b[36mcucd7st2m9-algo-1-w67e8 |\u001b[0m Epoch 1:  99%|█████████▉| 161/162 [00:41<00:00,  3.88it/s, loss=104, v_num=0, train_loss_step=81.20, val_loss=199.0, train_loss_epoch=161.0]\n",
      "\u001b[36mcucd7st2m9-algo-1-w67e8 |\u001b[0m Epoch 1:  99%|█████████▉| 161/162 [00:41<00:00,  3.88it/s, loss=104, v_num=0, train_loss_step=94.90, val_loss=199.0, train_loss_epoch=161.0]\n",
      "\u001b[36mcucd7st2m9-algo-1-w67e8 |\u001b[0m \n",
      "\u001b[36mcucd7st2m9-algo-1-w67e8 |\u001b[0m Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "\u001b[36mcucd7st2m9-algo-1-w67e8 |\u001b[0m \n",
      "\u001b[36mcucd7st2m9-algo-1-w67e8 |\u001b[0m Validation:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "\u001b[36mcucd7st2m9-algo-1-w67e8 |\u001b[0m \n",
      "\u001b[36mcucd7st2m9-algo-1-w67e8 |\u001b[0m Validation DataLoader 0:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "\u001b[36mcucd7st2m9-algo-1-w67e8 |\u001b[0m \n",
      "\u001b[36mcucd7st2m9-algo-1-w67e8 |\u001b[0m Validation DataLoader 0: 100%|██████████| 1/1 [00:00<00:00,  3.13it/s]\u001b[A\n",
      "\u001b[36mcucd7st2m9-algo-1-w67e8 |\u001b[0m Epoch 1: 100%|██████████| 162/162 [00:42<00:00,  3.84it/s, loss=104, v_num=0, train_loss_step=94.90, val_loss=199.0, train_loss_epoch=161.0]\n",
      "\u001b[36mcucd7st2m9-algo-1-w67e8 |\u001b[0m Epoch 1: 100%|██████████| 162/162 [00:42<00:00,  3.78it/s, loss=104, v_num=0, train_loss_step=94.90, val_loss=160.0, train_loss_epoch=161.0]\n",
      "\u001b[36mcucd7st2m9-algo-1-w67e8 |\u001b[0m \u001b[A\n",
      "\u001b[36mcucd7st2m9-algo-1-w67e8 |\u001b[0m Epoch 1: 100%|██████████| 162/162 [00:42<00:00,  3.77it/s, loss=104, v_num=0, train_loss_step=94.90, val_loss=160.0, train_loss_epoch=111.0]\n",
      "\u001b[36mcucd7st2m9-algo-1-w67e8 |\u001b[0m `Trainer.fit` stopped: `max_epochs=2` reached.\n",
      "\u001b[36mcucd7st2m9-algo-1-w67e8 |\u001b[0m Epoch 1: 100%|██████████| 162/162 [00:43<00:00,  3.76it/s, loss=104, v_num=0, train_loss_step=94.90, val_loss=160.0, train_loss_epoch=111.0]\n",
      "\u001b[36mcucd7st2m9-algo-1-w67e8 |\u001b[0m 2023-03-04 09:19:53,151 sagemaker-training-toolkit INFO     Waiting for the process to finish and give a return code.\n",
      "\u001b[36mcucd7st2m9-algo-1-w67e8 |\u001b[0m 2023-03-04 09:19:53,151 sagemaker-training-toolkit INFO     Done waiting for a return code. Received 0 from exiting process.\n",
      "\u001b[36mcucd7st2m9-algo-1-w67e8 |\u001b[0m 2023-03-04 09:19:53,152 sagemaker-training-toolkit INFO     Reporting training SUCCESS\n",
      "\u001b[36mcucd7st2m9-algo-1-w67e8 exited with code 0\n",
      "\u001b[0mAborting on container exit...\n",
      "===== Job Complete =====\n"
     ]
    }
   ],
   "source": [
    "from sagemaker.pytorch import PyTorch\n",
    "import os\n",
    "import subprocess\n",
    "\n",
    "# region = sagemaker_session.boto_region_name\n",
    "\n",
    "\n",
    "local_estimator = PyTorch(\n",
    "    entry_point=\"TFT_Train.py\",    \n",
    "    source_dir='scripts',    \n",
    "    role=role,\n",
    "    framework_version='1.12.1',    \n",
    "    py_version='py38',        \n",
    "    instance_count=1,\n",
    "    instance_type=instance_type, # local_gpu or local 지정\n",
    "    session = sagemaker.LocalSession(), # 로컬 세션을 사용합니다.\n",
    "    hyperparameters= hyperparameters               \n",
    "    \n",
    ")\n",
    "local_estimator.fit()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2c07adc-d09c-41b0-b695-5275163bb916",
   "metadata": {},
   "source": [
    "# 2. 훈련 코드를 직접 로컬에서 실행\n",
    "- --n_gpus {num_gpus} --epochs {epochs} 와 같은 파이라미터를 전달하여 실행 합니다.\n",
    "- 실행 완료 후에 model_dir 경로에 모델 가중치 파일이 저장 됩니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "811536df-a0b1-4e8b-9e86-5bee8aedd412",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# import sagemaker\n",
    "# from sagemaker.pytorch import PyTorch\n",
    "# from sagemaker.local import LocalSession\n",
    "\n",
    "# sagemaker_session = sagemaker.Session()\n",
    "# role = sagemaker.get_execution_role()\n",
    "# region = sagemaker_session.boto_region_name\n",
    "\n",
    "# # hard code point to the DLC images\n",
    "# image_uri = '763104351884.dkr.ecr.{}.amazonaws.com/pytorch-training:1.12.0-gpu-py38-cu113-ubuntu20.04-sagemaker'.format(region)\n",
    "\n",
    "# estimator = PyTorch(\n",
    "#   entry_point=\"mnist.py\",\n",
    "#   base_job_name=\"{}-ddp-mnist\".format(your_user_string),\n",
    "#   image_uri = image_uri,\n",
    "#   role=role,\n",
    "#   source_dir=\"scripts\",\n",
    "#   # configures the SageMaker training resource, you can increase as you need\n",
    "#   instance_count=1,\n",
    "#   instance_type=\"ml.g4dn.12xlarge\",\n",
    "#   py_version=\"py38\",\n",
    "#   sagemaker_session=sagemaker_session,\n",
    "#   distribution={\"pytorchddp\":{\"enabled\": True}},\n",
    "#   debugger_hook_config=False,\n",
    "#   #profiler_config=profiler_config,\n",
    "#   hyperparameters={\"batch_size\":32, \"epochs\":300},\n",
    "#   # enable warm pools for 20 minutes\n",
    "#   keep_alive_period_in_seconds = 20 *60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68fc8308-d46e-4017-82d4-5d5b0288b2c2",
   "metadata": {},
   "source": [
    "# 3. 모델 가중치 파일 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aad560bd-200f-46af-8548-e4506cb55265",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.listdir(model_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3564849c-9754-4a10-b417-86ec80a74d0f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
