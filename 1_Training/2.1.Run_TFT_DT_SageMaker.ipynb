{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4b27d946-a068-458a-9a19-a4e1f4abfcf0",
   "metadata": {
    "tags": []
   },
   "source": [
    "# [모듈 2.1] 세이지메이커에서 분산 훈련 하기\n",
    "\n",
    "이 노트북은 커널을 'conda_python3' 를 사용합니다.\n",
    "\n",
    "---\n",
    "이 노트북은 PyTorch Lightning 의 Multi GPUs 기능으로 1개의 인스턴스에서 (ml.g4dn.12xlarge) 에서 훈련 합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7444f2fd-db36-4489-a066-8bca587dc171",
   "metadata": {},
   "source": [
    "# 1. 환경 설정\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27b9bd08-1fe9-4306-ae4b-8cd0a99ac564",
   "metadata": {},
   "source": [
    "## 기본 세팅\n",
    "사용하는 패키지는 import 시점에 다시 재로딩 합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e5e68172-0ed2-448b-ad99-0659323e4848",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import sys\n",
    "sys.path.append('./scripts')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "97204ea5-68c0-4f1e-8dec-0318772d38a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sagemaker\n",
    "\n",
    "sagemaker.__version__\n",
    "\n",
    "# sagemaker_session = sagemaker.Session()\n",
    "role = sagemaker.get_execution_role()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "772a23fa-eee6-4b29-945e-751105551487",
   "metadata": {},
   "source": [
    "## 파라미터 세팅"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f41139d8-0a16-4a4b-874e-895745acd082",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_gpus:  8\n",
      "epochs:  2\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import os\n",
    "\n",
    "epochs = 2\n",
    "num_gpus = torch.cuda.device_count()\n",
    "# model_dir = 'model'\n",
    "# num_gpus = 4\n",
    "# train_notebook = True\n",
    "\n",
    "print(\"num_gpus: \", num_gpus)\n",
    "print(\"epochs: \", epochs)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a2c0eafa-d780-44aa-89da-6934f675e993",
   "metadata": {},
   "outputs": [],
   "source": [
    "# if train_notebook:\n",
    "\n",
    "\n",
    "#     os.makedirs(model_dir, exist_ok=True)\n",
    "        \n",
    "#     src_dir = os.getcwd()\n",
    "#     os.environ['SM_MODEL_DIR'] = f'{src_dir}/{model_dir}'\n",
    "#     os.environ['SM_NUM_GPUS'] = str(num_gpus)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fd10e802-d86c-4019-8949-6636b7ba13c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing scripts/requirements.txt\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'scripts/requirements.txt'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mget_ipython\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_cell_magic\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mwritefile\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mscripts/requirements.txt\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43mpytorch-forecasting==0.10.3\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43mpytorch_lightning==1.9.0\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m# Use the torch version\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m# torch==1.12.1\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/python3/lib/python3.10/site-packages/IPython/core/interactiveshell.py:2422\u001b[0m, in \u001b[0;36mInteractiveShell.run_cell_magic\u001b[0;34m(self, magic_name, line, cell)\u001b[0m\n\u001b[1;32m   2420\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbuiltin_trap:\n\u001b[1;32m   2421\u001b[0m     args \u001b[38;5;241m=\u001b[39m (magic_arg_s, cell)\n\u001b[0;32m-> 2422\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2423\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "File \u001b[0;32m~/anaconda3/envs/python3/lib/python3.10/site-packages/IPython/core/magics/osm.py:854\u001b[0m, in \u001b[0;36mOSMagics.writefile\u001b[0;34m(self, line, cell)\u001b[0m\n\u001b[1;32m    851\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWriting \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m filename)\n\u001b[1;32m    853\u001b[0m mode \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124ma\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m args\u001b[38;5;241m.\u001b[39mappend \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mw\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m--> 854\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43mio\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mutf-8\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[1;32m    855\u001b[0m     f\u001b[38;5;241m.\u001b[39mwrite(cell)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'scripts/requirements.txt'"
     ]
    }
   ],
   "source": [
    "%%writefile scripts/requirements.txt\n",
    "\n",
    "pytorch-forecasting==0.10.3\n",
    "pytorch_lightning==1.9.0\n",
    "# Use the torch version\n",
    "# torch==1.12.1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a28bcda9-79ef-4f79-9721-ee13e2618439",
   "metadata": {},
   "source": [
    "# 2. 세이지 메이크 로컬 모드 훈련\n",
    "#### 로컬의 GPU, CPU 여부로 instance_type 결정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "07a8e179-f1a7-40b3-a275-fcba9bf11ede",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sat Mar  4 14:23:36 2023       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 515.65.01    Driver Version: 515.65.01    CUDA Version: 11.7     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  Tesla V100-SXM2...  On   | 00000000:00:17.0 Off |                    0 |\n",
      "| N/A   29C    P0    41W / 300W |      3MiB / 16384MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   1  Tesla V100-SXM2...  On   | 00000000:00:18.0 Off |                    0 |\n",
      "| N/A   28C    P0    41W / 300W |      3MiB / 16384MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   2  Tesla V100-SXM2...  On   | 00000000:00:19.0 Off |                    0 |\n",
      "| N/A   27C    P0    38W / 300W |      3MiB / 16384MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   3  Tesla V100-SXM2...  On   | 00000000:00:1A.0 Off |                    0 |\n",
      "| N/A   28C    P0    41W / 300W |      3MiB / 16384MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   4  Tesla V100-SXM2...  On   | 00000000:00:1B.0 Off |                    0 |\n",
      "| N/A   28C    P0    42W / 300W |      3MiB / 16384MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   5  Tesla V100-SXM2...  On   | 00000000:00:1C.0 Off |                    0 |\n",
      "| N/A   28C    P0    41W / 300W |      3MiB / 16384MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   6  Tesla V100-SXM2...  On   | 00000000:00:1D.0 Off |                    0 |\n",
      "| N/A   28C    P0    41W / 300W |      3MiB / 16384MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   7  Tesla V100-SXM2...  On   | 00000000:00:1E.0 Off |                    0 |\n",
      "| N/A   28C    P0    43W / 300W |      3MiB / 16384MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "|  No running processes found                                                 |\n",
      "+-----------------------------------------------------------------------------+\n",
      "Instance type = local_gpu\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import subprocess\n",
    "\n",
    "\n",
    "try:\n",
    "    if subprocess.call(\"nvidia-smi\") == 0:\n",
    "        ## Set type to GPU if one is present\n",
    "        instance_type = \"local_gpu\"\n",
    "    else:\n",
    "        instance_type = \"local\"        \n",
    "except:\n",
    "    pass\n",
    "\n",
    "print(\"Instance type = \" + instance_type)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e25ed9c-3f2a-40b1-ae9b-6ec7b8b8687f",
   "metadata": {},
   "source": [
    "## 로컬 모드로 훈련 실행\n",
    "- 아래의 두 라인이 로컬모드로 훈련을 지시 합니다.\n",
    "```python\n",
    "    instance_type=instance_type, # local_gpu or local 지정\n",
    "    session = sagemaker.LocalSession(), # 로컬 세션을 사용합니다.\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bdb1893b-20a3-4867-bdac-eec5206e1867",
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperparameters = {'epochs': epochs, \n",
    "                   'n_gpus': num_gpus,\n",
    "                    }  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2e76fe8b-e1ed-4438-8d2d-3a662f37c558",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating 8bbbkc9zqw-algo-1-7uphg ... \n",
      "Creating 8bbbkc9zqw-algo-1-7uphg ... done\n",
      "Attaching to 8bbbkc9zqw-algo-1-7uphg\n",
      "\u001b[36m8bbbkc9zqw-algo-1-7uphg |\u001b[0m 2023-03-04 14:24:01,116 sagemaker-training-toolkit INFO     Imported framework sagemaker_pytorch_container.training\n",
      "\u001b[36m8bbbkc9zqw-algo-1-7uphg |\u001b[0m 2023-03-04 14:24:01,181 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\n",
      "\u001b[36m8bbbkc9zqw-algo-1-7uphg |\u001b[0m 2023-03-04 14:24:01,191 sagemaker-training-toolkit INFO     instance_groups entry not present in resource_config\n",
      "\u001b[36m8bbbkc9zqw-algo-1-7uphg |\u001b[0m 2023-03-04 14:24:01,194 sagemaker_pytorch_container.training INFO     Block until all host DNS lookups succeed.\n",
      "\u001b[36m8bbbkc9zqw-algo-1-7uphg |\u001b[0m 2023-03-04 14:24:01,201 sagemaker_pytorch_container.training INFO     Invoking user training script.\n",
      "\u001b[36m8bbbkc9zqw-algo-1-7uphg |\u001b[0m 2023-03-04 14:24:01,271 botocore.credentials INFO     Found credentials from IAM Role: BaseNotebookInstanceEc2InstanceRole\n",
      "\u001b[36m8bbbkc9zqw-algo-1-7uphg |\u001b[0m 2023-03-04 14:24:01,489 sagemaker-training-toolkit INFO     Installing dependencies from requirements.txt:\n",
      "\u001b[36m8bbbkc9zqw-algo-1-7uphg |\u001b[0m /opt/conda/bin/python3.8 -m pip install -r requirements.txt\n",
      "\u001b[36m8bbbkc9zqw-algo-1-7uphg |\u001b[0m Collecting pytorch-forecasting==0.10.3\n",
      "\u001b[36m8bbbkc9zqw-algo-1-7uphg |\u001b[0m Downloading pytorch_forecasting-0.10.3-py3-none-any.whl (141 kB)\n",
      "\u001b[36m8bbbkc9zqw-algo-1-7uphg |\u001b[0m ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 141.4/141.4 kB 5.6 MB/s eta 0:00:00\n",
      "\u001b[36m8bbbkc9zqw-algo-1-7uphg |\u001b[0m \n",
      "\u001b[36m8bbbkc9zqw-algo-1-7uphg |\u001b[0m Collecting pytorch-lightning==1.6.3\n",
      "\u001b[36m8bbbkc9zqw-algo-1-7uphg |\u001b[0m Downloading pytorch_lightning-1.6.3-py3-none-any.whl (584 kB)\n",
      "\u001b[36m8bbbkc9zqw-algo-1-7uphg |\u001b[0m ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 584.0/584.0 kB 49.5 MB/s eta 0:00:00\n",
      "\u001b[36m8bbbkc9zqw-algo-1-7uphg |\u001b[0m \n",
      "\u001b[36m8bbbkc9zqw-algo-1-7uphg |\u001b[0m Requirement already satisfied: pyarrow==11.0.0 in /opt/conda/lib/python3.8/site-packages (from -r requirements.txt (line 5)) (11.0.0)\n",
      "\u001b[36m8bbbkc9zqw-algo-1-7uphg |\u001b[0m Collecting tensorboard==2.12.0\n",
      "\u001b[36m8bbbkc9zqw-algo-1-7uphg |\u001b[0m Downloading tensorboard-2.12.0-py3-none-any.whl (5.6 MB)\n",
      "\u001b[36m8bbbkc9zqw-algo-1-7uphg |\u001b[0m ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 5.6/5.6 MB 81.1 MB/s eta 0:00:00\n",
      "\u001b[36m8bbbkc9zqw-algo-1-7uphg |\u001b[0m \n",
      "\u001b[36m8bbbkc9zqw-algo-1-7uphg |\u001b[0m Collecting optuna<3.0.0,>=2.3.0\n",
      "\u001b[36m8bbbkc9zqw-algo-1-7uphg |\u001b[0m Downloading optuna-2.10.1-py3-none-any.whl (308 kB)\n",
      "\u001b[36m8bbbkc9zqw-algo-1-7uphg |\u001b[0m ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 308.2/308.2 kB 40.3 MB/s eta 0:00:00\n",
      "\u001b[36m8bbbkc9zqw-algo-1-7uphg |\u001b[0m \n",
      "\u001b[36m8bbbkc9zqw-algo-1-7uphg |\u001b[0m Requirement already satisfied: scipy<2.0,>=1.8 in /opt/conda/lib/python3.8/site-packages (from pytorch-forecasting==0.10.3->-r requirements.txt (line 2)) (1.10.0)\n",
      "\u001b[36m8bbbkc9zqw-algo-1-7uphg |\u001b[0m Collecting statsmodels\n",
      "\u001b[36m8bbbkc9zqw-algo-1-7uphg |\u001b[0m Downloading statsmodels-0.13.5-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (9.9 MB)\n",
      "\u001b[36m8bbbkc9zqw-algo-1-7uphg |\u001b[0m ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 9.9/9.9 MB 71.9 MB/s eta 0:00:00\n",
      "\u001b[36m8bbbkc9zqw-algo-1-7uphg |\u001b[0m \n",
      "\u001b[36m8bbbkc9zqw-algo-1-7uphg |\u001b[0m Collecting scikit-learn<1.2,>=0.24\n",
      "\u001b[36m8bbbkc9zqw-algo-1-7uphg |\u001b[0m Downloading scikit_learn-1.1.3-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (31.2 MB)\n",
      "\u001b[36m8bbbkc9zqw-algo-1-7uphg |\u001b[0m ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 31.2/31.2 MB 52.9 MB/s eta 0:00:00\n",
      "\u001b[36m8bbbkc9zqw-algo-1-7uphg |\u001b[0m \n",
      "\u001b[36m8bbbkc9zqw-algo-1-7uphg |\u001b[0m Requirement already satisfied: pandas<2.0.0,>=1.3.0 in /opt/conda/lib/python3.8/site-packages (from pytorch-forecasting==0.10.3->-r requirements.txt (line 2)) (1.5.3)\n",
      "\u001b[36m8bbbkc9zqw-algo-1-7uphg |\u001b[0m Requirement already satisfied: torch<2.0,>=1.7 in /opt/conda/lib/python3.8/site-packages (from pytorch-forecasting==0.10.3->-r requirements.txt (line 2)) (1.12.1+cu113)\n",
      "\u001b[36m8bbbkc9zqw-algo-1-7uphg |\u001b[0m Requirement already satisfied: matplotlib in /opt/conda/lib/python3.8/site-packages (from pytorch-forecasting==0.10.3->-r requirements.txt (line 2)) (3.7.0)\n",
      "\u001b[36m8bbbkc9zqw-algo-1-7uphg |\u001b[0m Requirement already satisfied: typing-extensions>=4.0.0 in /opt/conda/lib/python3.8/site-packages (from pytorch-lightning==1.6.3->-r requirements.txt (line 4)) (4.4.0)\n",
      "\u001b[36m8bbbkc9zqw-algo-1-7uphg |\u001b[0m Requirement already satisfied: packaging>=17.0 in /opt/conda/lib/python3.8/site-packages (from pytorch-lightning==1.6.3->-r requirements.txt (line 4)) (23.0)\n",
      "\u001b[36m8bbbkc9zqw-algo-1-7uphg |\u001b[0m Collecting torchmetrics>=0.4.1\n",
      "\u001b[36m8bbbkc9zqw-algo-1-7uphg |\u001b[0m Downloading torchmetrics-0.11.3-py3-none-any.whl (518 kB)\n",
      "\u001b[36m8bbbkc9zqw-algo-1-7uphg |\u001b[0m ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 518.6/518.6 kB 47.7 MB/s eta 0:00:00\n",
      "\u001b[36m8bbbkc9zqw-algo-1-7uphg |\u001b[0m \n",
      "\u001b[36m8bbbkc9zqw-algo-1-7uphg |\u001b[0m Requirement already satisfied: PyYAML>=5.4 in /opt/conda/lib/python3.8/site-packages (from pytorch-lightning==1.6.3->-r requirements.txt (line 4)) (5.4.1)\n",
      "\u001b[36m8bbbkc9zqw-algo-1-7uphg |\u001b[0m Requirement already satisfied: fsspec[http]!=2021.06.0,>=2021.05.0 in /opt/conda/lib/python3.8/site-packages (from pytorch-lightning==1.6.3->-r requirements.txt (line 4)) (2023.1.0)\n",
      "\u001b[36m8bbbkc9zqw-algo-1-7uphg |\u001b[0m Collecting pyDeprecate<0.4.0,>=0.3.1\n",
      "\u001b[36m8bbbkc9zqw-algo-1-7uphg |\u001b[0m Downloading pyDeprecate-0.3.2-py3-none-any.whl (10 kB)\n",
      "\u001b[36m8bbbkc9zqw-algo-1-7uphg |\u001b[0m Requirement already satisfied: tqdm>=4.57.0 in /opt/conda/lib/python3.8/site-packages (from pytorch-lightning==1.6.3->-r requirements.txt (line 4)) (4.64.1)\n",
      "\u001b[36m8bbbkc9zqw-algo-1-7uphg |\u001b[0m Requirement already satisfied: numpy>=1.17.2 in /opt/conda/lib/python3.8/site-packages (from pytorch-lightning==1.6.3->-r requirements.txt (line 4)) (1.23.5)\n",
      "\u001b[36m8bbbkc9zqw-algo-1-7uphg |\u001b[0m Requirement already satisfied: werkzeug>=1.0.1 in /opt/conda/lib/python3.8/site-packages (from tensorboard==2.12.0->-r requirements.txt (line 6)) (2.2.3)\n",
      "\u001b[36m8bbbkc9zqw-algo-1-7uphg |\u001b[0m Requirement already satisfied: setuptools>=41.0.0 in /opt/conda/lib/python3.8/site-packages (from tensorboard==2.12.0->-r requirements.txt (line 6)) (65.6.3)\n",
      "\u001b[36m8bbbkc9zqw-algo-1-7uphg |\u001b[0m Requirement already satisfied: protobuf>=3.19.6 in /opt/conda/lib/python3.8/site-packages (from tensorboard==2.12.0->-r requirements.txt (line 6)) (3.20.2)\n",
      "\u001b[36m8bbbkc9zqw-algo-1-7uphg |\u001b[0m Requirement already satisfied: wheel>=0.26 in /opt/conda/lib/python3.8/site-packages (from tensorboard==2.12.0->-r requirements.txt (line 6)) (0.38.4)\n",
      "\u001b[36m8bbbkc9zqw-algo-1-7uphg |\u001b[0m Collecting absl-py>=0.4\n",
      "\u001b[36m8bbbkc9zqw-algo-1-7uphg |\u001b[0m Downloading absl_py-1.4.0-py3-none-any.whl (126 kB)\n",
      "\u001b[36m8bbbkc9zqw-algo-1-7uphg |\u001b[0m ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 126.5/126.5 kB 15.4 MB/s eta 0:00:00\n",
      "\u001b[36m8bbbkc9zqw-algo-1-7uphg |\u001b[0m \n",
      "\u001b[36m8bbbkc9zqw-algo-1-7uphg |\u001b[0m Collecting grpcio>=1.48.2\n",
      "\u001b[36m8bbbkc9zqw-algo-1-7uphg |\u001b[0m Downloading grpcio-1.51.3-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.8 MB)\n",
      "\u001b[36m8bbbkc9zqw-algo-1-7uphg |\u001b[0m ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 4.8/4.8 MB 74.2 MB/s eta 0:00:00\n",
      "\u001b[36m8bbbkc9zqw-algo-1-7uphg |\u001b[0m \n",
      "\u001b[36m8bbbkc9zqw-algo-1-7uphg |\u001b[0m Collecting google-auth-oauthlib<0.5,>=0.4.1\n",
      "\u001b[36m8bbbkc9zqw-algo-1-7uphg |\u001b[0m Downloading google_auth_oauthlib-0.4.6-py2.py3-none-any.whl (18 kB)\n",
      "\u001b[36m8bbbkc9zqw-algo-1-7uphg |\u001b[0m Collecting tensorboard-plugin-wit>=1.6.0\n",
      "\u001b[36m8bbbkc9zqw-algo-1-7uphg |\u001b[0m Downloading tensorboard_plugin_wit-1.8.1-py3-none-any.whl (781 kB)\n",
      "\u001b[36m8bbbkc9zqw-algo-1-7uphg |\u001b[0m ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 781.3/781.3 kB 63.6 MB/s eta 0:00:00\n",
      "\u001b[36m8bbbkc9zqw-algo-1-7uphg |\u001b[0m \n",
      "\u001b[36m8bbbkc9zqw-algo-1-7uphg |\u001b[0m Requirement already satisfied: requests<3,>=2.21.0 in /opt/conda/lib/python3.8/site-packages (from tensorboard==2.12.0->-r requirements.txt (line 6)) (2.28.2)\n",
      "\u001b[36m8bbbkc9zqw-algo-1-7uphg |\u001b[0m Collecting tensorboard-data-server<0.8.0,>=0.7.0\n",
      "\u001b[36m8bbbkc9zqw-algo-1-7uphg |\u001b[0m Downloading tensorboard_data_server-0.7.0-py3-none-manylinux2014_x86_64.whl (6.6 MB)\n",
      "\u001b[36m8bbbkc9zqw-algo-1-7uphg |\u001b[0m ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 6.6/6.6 MB 75.8 MB/s eta 0:00:00\n",
      "\u001b[36m8bbbkc9zqw-algo-1-7uphg |\u001b[0m \n",
      "\u001b[36m8bbbkc9zqw-algo-1-7uphg |\u001b[0m Collecting google-auth<3,>=1.6.3\n",
      "\u001b[36m8bbbkc9zqw-algo-1-7uphg |\u001b[0m Downloading google_auth-2.16.2-py2.py3-none-any.whl (177 kB)\n",
      "\u001b[36m8bbbkc9zqw-algo-1-7uphg |\u001b[0m ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 177.2/177.2 kB 27.5 MB/s eta 0:00:00\n",
      "\u001b[36m8bbbkc9zqw-algo-1-7uphg |\u001b[0m \n",
      "\u001b[36m8bbbkc9zqw-algo-1-7uphg |\u001b[0m Collecting markdown>=2.6.8\n",
      "\u001b[36m8bbbkc9zqw-algo-1-7uphg |\u001b[0m Downloading Markdown-3.4.1-py3-none-any.whl (93 kB)\n",
      "\u001b[36m8bbbkc9zqw-algo-1-7uphg |\u001b[0m ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 93.3/93.3 kB 16.8 MB/s eta 0:00:00\n",
      "\u001b[36m8bbbkc9zqw-algo-1-7uphg |\u001b[0m \n",
      "\u001b[36m8bbbkc9zqw-algo-1-7uphg |\u001b[0m Collecting aiohttp!=4.0.0a0,!=4.0.0a1\n",
      "\u001b[36m8bbbkc9zqw-algo-1-7uphg |\u001b[0m Downloading aiohttp-3.8.4-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.0 MB)\n",
      "\u001b[36m8bbbkc9zqw-algo-1-7uphg |\u001b[0m ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1.0/1.0 MB 70.6 MB/s eta 0:00:00\n",
      "\u001b[36m8bbbkc9zqw-algo-1-7uphg |\u001b[0m \n",
      "\u001b[36m8bbbkc9zqw-algo-1-7uphg |\u001b[0m Collecting cachetools<6.0,>=2.0.0\n",
      "\u001b[36m8bbbkc9zqw-algo-1-7uphg |\u001b[0m Downloading cachetools-5.3.0-py3-none-any.whl (9.3 kB)\n",
      "\u001b[36m8bbbkc9zqw-algo-1-7uphg |\u001b[0m Requirement already satisfied: six>=1.9.0 in /opt/conda/lib/python3.8/site-packages (from google-auth<3,>=1.6.3->tensorboard==2.12.0->-r requirements.txt (line 6)) (1.16.0)\n",
      "\u001b[36m8bbbkc9zqw-algo-1-7uphg |\u001b[0m Collecting pyasn1-modules>=0.2.1\n",
      "\u001b[36m8bbbkc9zqw-algo-1-7uphg |\u001b[0m Downloading pyasn1_modules-0.2.8-py2.py3-none-any.whl (155 kB)\n",
      "\u001b[36m8bbbkc9zqw-algo-1-7uphg |\u001b[0m ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 155.3/155.3 kB 26.6 MB/s eta 0:00:00\n",
      "\u001b[36m8bbbkc9zqw-algo-1-7uphg |\u001b[0m \n",
      "\u001b[36m8bbbkc9zqw-algo-1-7uphg |\u001b[0m Requirement already satisfied: rsa<5,>=3.1.4 in /opt/conda/lib/python3.8/site-packages (from google-auth<3,>=1.6.3->tensorboard==2.12.0->-r requirements.txt (line 6)) (4.7.2)\n",
      "\u001b[36m8bbbkc9zqw-algo-1-7uphg |\u001b[0m Collecting requests-oauthlib>=0.7.0\n",
      "\u001b[36m8bbbkc9zqw-algo-1-7uphg |\u001b[0m Downloading requests_oauthlib-1.3.1-py2.py3-none-any.whl (23 kB)\n",
      "\u001b[36m8bbbkc9zqw-algo-1-7uphg |\u001b[0m Requirement already satisfied: importlib-metadata>=4.4 in /opt/conda/lib/python3.8/site-packages (from markdown>=2.6.8->tensorboard==2.12.0->-r requirements.txt (line 6)) (4.13.0)\n",
      "\u001b[36m8bbbkc9zqw-algo-1-7uphg |\u001b[0m Collecting colorlog\n",
      "\u001b[36m8bbbkc9zqw-algo-1-7uphg |\u001b[0m Downloading colorlog-6.7.0-py2.py3-none-any.whl (11 kB)\n",
      "\u001b[36m8bbbkc9zqw-algo-1-7uphg |\u001b[0m Collecting sqlalchemy>=1.1.0\n",
      "\u001b[36m8bbbkc9zqw-algo-1-7uphg |\u001b[0m Downloading SQLAlchemy-2.0.4-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.8 MB)\n",
      "\u001b[36m8bbbkc9zqw-algo-1-7uphg |\u001b[0m ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 2.8/2.8 MB 80.3 MB/s eta 0:00:00\n",
      "\u001b[36m8bbbkc9zqw-algo-1-7uphg |\u001b[0m \n",
      "\u001b[36m8bbbkc9zqw-algo-1-7uphg |\u001b[0m Collecting cliff\n",
      "\u001b[36m8bbbkc9zqw-algo-1-7uphg |\u001b[0m Downloading cliff-4.2.0-py3-none-any.whl (81 kB)\n",
      "\u001b[36m8bbbkc9zqw-algo-1-7uphg |\u001b[0m ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 81.0/81.0 kB 14.1 MB/s eta 0:00:00\n",
      "\u001b[36m8bbbkc9zqw-algo-1-7uphg |\u001b[0m \n",
      "\u001b[36m8bbbkc9zqw-algo-1-7uphg |\u001b[0m Collecting cmaes>=0.8.2\n",
      "\u001b[36m8bbbkc9zqw-algo-1-7uphg |\u001b[0m Downloading cmaes-0.9.1-py3-none-any.whl (21 kB)\n",
      "\u001b[36m8bbbkc9zqw-algo-1-7uphg |\u001b[0m Collecting alembic\n",
      "\u001b[36m8bbbkc9zqw-algo-1-7uphg |\u001b[0m Downloading alembic-1.9.4-py3-none-any.whl (210 kB)\n",
      "\u001b[36m8bbbkc9zqw-algo-1-7uphg |\u001b[0m ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 210.5/210.5 kB 32.3 MB/s eta 0:00:00\n",
      "\u001b[36m8bbbkc9zqw-algo-1-7uphg |\u001b[0m \n",
      "\u001b[36m8bbbkc9zqw-algo-1-7uphg |\u001b[0m Requirement already satisfied: python-dateutil>=2.8.1 in /opt/conda/lib/python3.8/site-packages (from pandas<2.0.0,>=1.3.0->pytorch-forecasting==0.10.3->-r requirements.txt (line 2)) (2.8.2)\n",
      "\u001b[36m8bbbkc9zqw-algo-1-7uphg |\u001b[0m Requirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.8/site-packages (from pandas<2.0.0,>=1.3.0->pytorch-forecasting==0.10.3->-r requirements.txt (line 2)) (2022.7.1)\n",
      "\u001b[36m8bbbkc9zqw-algo-1-7uphg |\u001b[0m Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.8/site-packages (from requests<3,>=2.21.0->tensorboard==2.12.0->-r requirements.txt (line 6)) (2022.12.7)\n",
      "\u001b[36m8bbbkc9zqw-algo-1-7uphg |\u001b[0m Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.8/site-packages (from requests<3,>=2.21.0->tensorboard==2.12.0->-r requirements.txt (line 6)) (3.4)\n",
      "\u001b[36m8bbbkc9zqw-algo-1-7uphg |\u001b[0m Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.8/site-packages (from requests<3,>=2.21.0->tensorboard==2.12.0->-r requirements.txt (line 6)) (2.1.1)\n",
      "\u001b[36m8bbbkc9zqw-algo-1-7uphg |\u001b[0m Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.8/site-packages (from requests<3,>=2.21.0->tensorboard==2.12.0->-r requirements.txt (line 6)) (1.26.14)\n",
      "\u001b[36m8bbbkc9zqw-algo-1-7uphg |\u001b[0m Requirement already satisfied: threadpoolctl>=2.0.0 in /opt/conda/lib/python3.8/site-packages (from scikit-learn<1.2,>=0.24->pytorch-forecasting==0.10.3->-r requirements.txt (line 2)) (3.1.0)\n",
      "\u001b[36m8bbbkc9zqw-algo-1-7uphg |\u001b[0m Requirement already satisfied: joblib>=1.0.0 in /opt/conda/lib/python3.8/site-packages (from scikit-learn<1.2,>=0.24->pytorch-forecasting==0.10.3->-r requirements.txt (line 2)) (1.2.0)\n",
      "\u001b[36m8bbbkc9zqw-algo-1-7uphg |\u001b[0m Requirement already satisfied: MarkupSafe>=2.1.1 in /opt/conda/lib/python3.8/site-packages (from werkzeug>=1.0.1->tensorboard==2.12.0->-r requirements.txt (line 6)) (2.1.2)\n",
      "\u001b[36m8bbbkc9zqw-algo-1-7uphg |\u001b[0m Requirement already satisfied: kiwisolver>=1.0.1 in /opt/conda/lib/python3.8/site-packages (from matplotlib->pytorch-forecasting==0.10.3->-r requirements.txt (line 2)) (1.4.4)\n",
      "\u001b[36m8bbbkc9zqw-algo-1-7uphg |\u001b[0m Requirement already satisfied: importlib-resources>=3.2.0 in /opt/conda/lib/python3.8/site-packages (from matplotlib->pytorch-forecasting==0.10.3->-r requirements.txt (line 2)) (5.10.2)\n",
      "\u001b[36m8bbbkc9zqw-algo-1-7uphg |\u001b[0m Requirement already satisfied: contourpy>=1.0.1 in /opt/conda/lib/python3.8/site-packages (from matplotlib->pytorch-forecasting==0.10.3->-r requirements.txt (line 2)) (1.0.7)\n",
      "\u001b[36m8bbbkc9zqw-algo-1-7uphg |\u001b[0m Requirement already satisfied: pyparsing>=2.3.1 in /opt/conda/lib/python3.8/site-packages (from matplotlib->pytorch-forecasting==0.10.3->-r requirements.txt (line 2)) (3.0.9)\n",
      "\u001b[36m8bbbkc9zqw-algo-1-7uphg |\u001b[0m Requirement already satisfied: pillow>=6.2.0 in /opt/conda/lib/python3.8/site-packages (from matplotlib->pytorch-forecasting==0.10.3->-r requirements.txt (line 2)) (9.4.0)\n",
      "\u001b[36m8bbbkc9zqw-algo-1-7uphg |\u001b[0m Requirement already satisfied: cycler>=0.10 in /opt/conda/lib/python3.8/site-packages (from matplotlib->pytorch-forecasting==0.10.3->-r requirements.txt (line 2)) (0.11.0)\n",
      "\u001b[36m8bbbkc9zqw-algo-1-7uphg |\u001b[0m Requirement already satisfied: fonttools>=4.22.0 in /opt/conda/lib/python3.8/site-packages (from matplotlib->pytorch-forecasting==0.10.3->-r requirements.txt (line 2)) (4.38.0)\n",
      "\u001b[36m8bbbkc9zqw-algo-1-7uphg |\u001b[0m Collecting patsy>=0.5.2\n",
      "\u001b[36m8bbbkc9zqw-algo-1-7uphg |\u001b[0m Downloading patsy-0.5.3-py2.py3-none-any.whl (233 kB)\n",
      "\u001b[36m8bbbkc9zqw-algo-1-7uphg |\u001b[0m ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 233.8/233.8 kB 31.6 MB/s eta 0:00:00\n",
      "\u001b[36m8bbbkc9zqw-algo-1-7uphg |\u001b[0m \n",
      "\u001b[36m8bbbkc9zqw-algo-1-7uphg |\u001b[0m Collecting async-timeout<5.0,>=4.0.0a3\n",
      "\u001b[36m8bbbkc9zqw-algo-1-7uphg |\u001b[0m Downloading async_timeout-4.0.2-py3-none-any.whl (5.8 kB)\n",
      "\u001b[36m8bbbkc9zqw-algo-1-7uphg |\u001b[0m Collecting multidict<7.0,>=4.5\n",
      "\u001b[36m8bbbkc9zqw-algo-1-7uphg |\u001b[0m Downloading multidict-6.0.4-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (121 kB)\n",
      "\u001b[36m8bbbkc9zqw-algo-1-7uphg |\u001b[0m ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 121.3/121.3 kB 19.6 MB/s eta 0:00:00\n",
      "\u001b[36m8bbbkc9zqw-algo-1-7uphg |\u001b[0m \n",
      "\u001b[36m8bbbkc9zqw-algo-1-7uphg |\u001b[0m Requirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.8/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning==1.6.3->-r requirements.txt (line 4)) (22.2.0)\n",
      "\u001b[36m8bbbkc9zqw-algo-1-7uphg |\u001b[0m Collecting yarl<2.0,>=1.0\n",
      "\u001b[36m8bbbkc9zqw-algo-1-7uphg |\u001b[0m Downloading yarl-1.8.2-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (262 kB)\n",
      "\u001b[36m8bbbkc9zqw-algo-1-7uphg |\u001b[0m ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 262.1/262.1 kB 32.4 MB/s eta 0:00:00\n",
      "\u001b[36m8bbbkc9zqw-algo-1-7uphg |\u001b[0m \n",
      "\u001b[36m8bbbkc9zqw-algo-1-7uphg |\u001b[0m Collecting frozenlist>=1.1.1\n",
      "\u001b[36m8bbbkc9zqw-algo-1-7uphg |\u001b[0m Downloading frozenlist-1.3.3-cp38-cp38-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (161 kB)\n",
      "\u001b[36m8bbbkc9zqw-algo-1-7uphg |\u001b[0m ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 161.3/161.3 kB 24.4 MB/s eta 0:00:00\n",
      "\u001b[36m8bbbkc9zqw-algo-1-7uphg |\u001b[0m \n",
      "\u001b[36m8bbbkc9zqw-algo-1-7uphg |\u001b[0m Collecting aiosignal>=1.1.2\n",
      "\u001b[36m8bbbkc9zqw-algo-1-7uphg |\u001b[0m Downloading aiosignal-1.3.1-py3-none-any.whl (7.6 kB)\n",
      "\u001b[36m8bbbkc9zqw-algo-1-7uphg |\u001b[0m Requirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.8/site-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard==2.12.0->-r requirements.txt (line 6)) (3.13.0)\n",
      "\u001b[36m8bbbkc9zqw-algo-1-7uphg |\u001b[0m Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /opt/conda/lib/python3.8/site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard==2.12.0->-r requirements.txt (line 6)) (0.4.8)\n",
      "\u001b[36m8bbbkc9zqw-algo-1-7uphg |\u001b[0m Collecting oauthlib>=3.0.0\n",
      "\u001b[36m8bbbkc9zqw-algo-1-7uphg |\u001b[0m Downloading oauthlib-3.2.2-py3-none-any.whl (151 kB)\n",
      "\u001b[36m8bbbkc9zqw-algo-1-7uphg |\u001b[0m ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 151.7/151.7 kB 26.3 MB/s eta 0:00:00\n",
      "\u001b[36m8bbbkc9zqw-algo-1-7uphg |\u001b[0m \n",
      "\u001b[36m8bbbkc9zqw-algo-1-7uphg |\u001b[0m Requirement already satisfied: greenlet!=0.4.17 in /opt/conda/lib/python3.8/site-packages (from sqlalchemy>=1.1.0->optuna<3.0.0,>=2.3.0->pytorch-forecasting==0.10.3->-r requirements.txt (line 2)) (2.0.2)\n",
      "\u001b[36m8bbbkc9zqw-algo-1-7uphg |\u001b[0m Collecting Mako\n",
      "\u001b[36m8bbbkc9zqw-algo-1-7uphg |\u001b[0m Downloading Mako-1.2.4-py3-none-any.whl (78 kB)\n",
      "\u001b[36m8bbbkc9zqw-algo-1-7uphg |\u001b[0m ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 78.7/78.7 kB 14.6 MB/s eta 0:00:00\n",
      "\u001b[36m8bbbkc9zqw-algo-1-7uphg |\u001b[0m \n",
      "\u001b[36m8bbbkc9zqw-algo-1-7uphg |\u001b[0m Collecting cmd2>=1.0.0\n",
      "\u001b[36m8bbbkc9zqw-algo-1-7uphg |\u001b[0m Downloading cmd2-2.4.3-py3-none-any.whl (147 kB)\n",
      "\u001b[36m8bbbkc9zqw-algo-1-7uphg |\u001b[0m ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 147.2/147.2 kB 23.3 MB/s eta 0:00:00\n",
      "\u001b[36m8bbbkc9zqw-algo-1-7uphg |\u001b[0m \n",
      "\u001b[36m8bbbkc9zqw-algo-1-7uphg |\u001b[0m Collecting stevedore>=2.0.1\n",
      "\u001b[36m8bbbkc9zqw-algo-1-7uphg |\u001b[0m Downloading stevedore-5.0.0-py3-none-any.whl (49 kB)\n",
      "\u001b[36m8bbbkc9zqw-algo-1-7uphg |\u001b[0m ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 49.6/49.6 kB 8.7 MB/s eta 0:00:00\n",
      "\u001b[36m8bbbkc9zqw-algo-1-7uphg |\u001b[0m \n",
      "\u001b[36m8bbbkc9zqw-algo-1-7uphg |\u001b[0m Collecting PrettyTable>=0.7.2\n",
      "\u001b[36m8bbbkc9zqw-algo-1-7uphg |\u001b[0m Downloading prettytable-3.6.0-py3-none-any.whl (27 kB)\n",
      "\u001b[36m8bbbkc9zqw-algo-1-7uphg |\u001b[0m Collecting autopage>=0.4.0\n",
      "\u001b[36m8bbbkc9zqw-algo-1-7uphg |\u001b[0m Downloading autopage-0.5.1-py3-none-any.whl (29 kB)\n",
      "\u001b[36m8bbbkc9zqw-algo-1-7uphg |\u001b[0m Collecting pyperclip>=1.6\n",
      "\u001b[36m8bbbkc9zqw-algo-1-7uphg |\u001b[0m Downloading pyperclip-1.8.2.tar.gz (20 kB)\n",
      "\u001b[36m8bbbkc9zqw-algo-1-7uphg |\u001b[0m Preparing metadata (setup.py): started\n",
      "\u001b[36m8bbbkc9zqw-algo-1-7uphg |\u001b[0m Preparing metadata (setup.py): finished with status 'done'\n",
      "\u001b[36m8bbbkc9zqw-algo-1-7uphg |\u001b[0m Requirement already satisfied: wcwidth>=0.1.7 in /opt/conda/lib/python3.8/site-packages (from cmd2>=1.0.0->cliff->optuna<3.0.0,>=2.3.0->pytorch-forecasting==0.10.3->-r requirements.txt (line 2)) (0.2.6)\n",
      "\u001b[36m8bbbkc9zqw-algo-1-7uphg |\u001b[0m Collecting pbr!=2.1.0,>=2.0.0\n",
      "\u001b[36m8bbbkc9zqw-algo-1-7uphg |\u001b[0m Downloading pbr-5.11.1-py2.py3-none-any.whl (112 kB)\n",
      "\u001b[36m8bbbkc9zqw-algo-1-7uphg |\u001b[0m ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 112.7/112.7 kB 20.8 MB/s eta 0:00:00\n",
      "\u001b[36m8bbbkc9zqw-algo-1-7uphg |\u001b[0m \n",
      "\u001b[36m8bbbkc9zqw-algo-1-7uphg |\u001b[0m Building wheels for collected packages: pyperclip\n",
      "\u001b[36m8bbbkc9zqw-algo-1-7uphg |\u001b[0m Building wheel for pyperclip (setup.py): started\n",
      "\u001b[36m8bbbkc9zqw-algo-1-7uphg |\u001b[0m Building wheel for pyperclip (setup.py): finished with status 'done'\n",
      "\u001b[36m8bbbkc9zqw-algo-1-7uphg |\u001b[0m Created wheel for pyperclip: filename=pyperclip-1.8.2-py3-none-any.whl size=11124 sha256=4264a08752d9829c330ef237f60f8212e5c4a8e7fd54e95a6ec4fec872861233\n",
      "\u001b[36m8bbbkc9zqw-algo-1-7uphg |\u001b[0m Stored in directory: /root/.cache/pip/wheels/7f/1a/65/84ff8c386bec21fca6d220ea1f5498a0367883a78dd5ba6122\n",
      "\u001b[36m8bbbkc9zqw-algo-1-7uphg |\u001b[0m Successfully built pyperclip\n",
      "\u001b[36m8bbbkc9zqw-algo-1-7uphg |\u001b[0m Installing collected packages: tensorboard-plugin-wit, pyperclip, tensorboard-data-server, sqlalchemy, pyDeprecate, pyasn1-modules, PrettyTable, pbr, patsy, oauthlib, multidict, Mako, grpcio, frozenlist, colorlog, cmd2, cmaes, cachetools, autopage, async-timeout, absl-py, yarl, torchmetrics, stevedore, scikit-learn, requests-oauthlib, markdown, google-auth, alembic, aiosignal, statsmodels, google-auth-oauthlib, cliff, aiohttp, tensorboard, optuna, pytorch-lightning, pytorch-forecasting\n",
      "\u001b[36m8bbbkc9zqw-algo-1-7uphg |\u001b[0m Attempting uninstall: scikit-learn\n",
      "\u001b[36m8bbbkc9zqw-algo-1-7uphg |\u001b[0m Found existing installation: scikit-learn 1.2.1\n",
      "\u001b[36m8bbbkc9zqw-algo-1-7uphg |\u001b[0m Uninstalling scikit-learn-1.2.1:\n",
      "\u001b[36m8bbbkc9zqw-algo-1-7uphg |\u001b[0m Successfully uninstalled scikit-learn-1.2.1\n",
      "\u001b[36m8bbbkc9zqw-algo-1-7uphg |\u001b[0m Successfully installed Mako-1.2.4 PrettyTable-3.6.0 absl-py-1.4.0 aiohttp-3.8.4 aiosignal-1.3.1 alembic-1.9.4 async-timeout-4.0.2 autopage-0.5.1 cachetools-5.3.0 cliff-4.2.0 cmaes-0.9.1 cmd2-2.4.3 colorlog-6.7.0 frozenlist-1.3.3 google-auth-2.16.2 google-auth-oauthlib-0.4.6 grpcio-1.51.3 markdown-3.4.1 multidict-6.0.4 oauthlib-3.2.2 optuna-2.10.1 patsy-0.5.3 pbr-5.11.1 pyDeprecate-0.3.2 pyasn1-modules-0.2.8 pyperclip-1.8.2 pytorch-forecasting-0.10.3 pytorch-lightning-1.6.3 requests-oauthlib-1.3.1 scikit-learn-1.1.3 sqlalchemy-2.0.4 statsmodels-0.13.5 stevedore-5.0.0 tensorboard-2.12.0 tensorboard-data-server-0.7.0 tensorboard-plugin-wit-1.8.1 torchmetrics-0.11.3 yarl-1.8.2\n",
      "\u001b[36m8bbbkc9zqw-algo-1-7uphg |\u001b[0m WARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\n",
      "\u001b[36m8bbbkc9zqw-algo-1-7uphg |\u001b[0m [notice] A new release of pip is available: 23.0 -> 23.0.1\n",
      "\u001b[36m8bbbkc9zqw-algo-1-7uphg |\u001b[0m [notice] To update, run: pip install --upgrade pip\n",
      "\u001b[36m8bbbkc9zqw-algo-1-7uphg |\u001b[0m 2023-03-04 14:24:24,536 sagemaker-training-toolkit INFO     Waiting for the process to finish and give a return code.\n",
      "\u001b[36m8bbbkc9zqw-algo-1-7uphg |\u001b[0m 2023-03-04 14:24:24,536 sagemaker-training-toolkit INFO     Done waiting for a return code. Received 0 from exiting process.\n",
      "\u001b[36m8bbbkc9zqw-algo-1-7uphg |\u001b[0m 2023-03-04 14:24:24,607 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\n",
      "\u001b[36m8bbbkc9zqw-algo-1-7uphg |\u001b[0m 2023-03-04 14:24:24,617 sagemaker-training-toolkit INFO     instance_groups entry not present in resource_config\n",
      "\u001b[36m8bbbkc9zqw-algo-1-7uphg |\u001b[0m 2023-03-04 14:24:24,687 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\n",
      "\u001b[36m8bbbkc9zqw-algo-1-7uphg |\u001b[0m 2023-03-04 14:24:24,698 sagemaker-training-toolkit INFO     instance_groups entry not present in resource_config\n",
      "\u001b[36m8bbbkc9zqw-algo-1-7uphg |\u001b[0m 2023-03-04 14:24:24,767 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\n",
      "\u001b[36m8bbbkc9zqw-algo-1-7uphg |\u001b[0m 2023-03-04 14:24:24,777 sagemaker-training-toolkit INFO     instance_groups entry not present in resource_config\n",
      "\u001b[36m8bbbkc9zqw-algo-1-7uphg |\u001b[0m 2023-03-04 14:24:24,780 sagemaker-training-toolkit INFO     Invoking user script\n",
      "\u001b[36m8bbbkc9zqw-algo-1-7uphg |\u001b[0m \n",
      "\u001b[36m8bbbkc9zqw-algo-1-7uphg |\u001b[0m Training Env:\n",
      "\u001b[36m8bbbkc9zqw-algo-1-7uphg |\u001b[0m \n",
      "\u001b[36m8bbbkc9zqw-algo-1-7uphg |\u001b[0m {\n",
      "\u001b[36m8bbbkc9zqw-algo-1-7uphg |\u001b[0m     \"additional_framework_parameters\": {},\n",
      "\u001b[36m8bbbkc9zqw-algo-1-7uphg |\u001b[0m     \"channel_input_dirs\": {},\n",
      "\u001b[36m8bbbkc9zqw-algo-1-7uphg |\u001b[0m     \"current_host\": \"algo-1-7uphg\",\n",
      "\u001b[36m8bbbkc9zqw-algo-1-7uphg |\u001b[0m     \"current_instance_group\": \"homogeneousCluster\",\n",
      "\u001b[36m8bbbkc9zqw-algo-1-7uphg |\u001b[0m     \"current_instance_group_hosts\": [],\n",
      "\u001b[36m8bbbkc9zqw-algo-1-7uphg |\u001b[0m     \"current_instance_type\": \"local\",\n",
      "\u001b[36m8bbbkc9zqw-algo-1-7uphg |\u001b[0m     \"distribution_hosts\": [\n",
      "\u001b[36m8bbbkc9zqw-algo-1-7uphg |\u001b[0m         \"algo-1-7uphg\"\n",
      "\u001b[36m8bbbkc9zqw-algo-1-7uphg |\u001b[0m     ],\n",
      "\u001b[36m8bbbkc9zqw-algo-1-7uphg |\u001b[0m     \"distribution_instance_groups\": [],\n",
      "\u001b[36m8bbbkc9zqw-algo-1-7uphg |\u001b[0m     \"framework_module\": \"sagemaker_pytorch_container.training:main\",\n",
      "\u001b[36m8bbbkc9zqw-algo-1-7uphg |\u001b[0m     \"hosts\": [\n",
      "\u001b[36m8bbbkc9zqw-algo-1-7uphg |\u001b[0m         \"algo-1-7uphg\"\n",
      "\u001b[36m8bbbkc9zqw-algo-1-7uphg |\u001b[0m     ],\n",
      "\u001b[36m8bbbkc9zqw-algo-1-7uphg |\u001b[0m     \"hyperparameters\": {\n",
      "\u001b[36m8bbbkc9zqw-algo-1-7uphg |\u001b[0m         \"epochs\": 2,\n",
      "\u001b[36m8bbbkc9zqw-algo-1-7uphg |\u001b[0m         \"n_gpus\": 8\n",
      "\u001b[36m8bbbkc9zqw-algo-1-7uphg |\u001b[0m     },\n",
      "\u001b[36m8bbbkc9zqw-algo-1-7uphg |\u001b[0m     \"input_config_dir\": \"/opt/ml/input/config\",\n",
      "\u001b[36m8bbbkc9zqw-algo-1-7uphg |\u001b[0m     \"input_data_config\": {},\n",
      "\u001b[36m8bbbkc9zqw-algo-1-7uphg |\u001b[0m     \"input_dir\": \"/opt/ml/input\",\n",
      "\u001b[36m8bbbkc9zqw-algo-1-7uphg |\u001b[0m     \"instance_groups\": [],\n",
      "\u001b[36m8bbbkc9zqw-algo-1-7uphg |\u001b[0m     \"instance_groups_dict\": {},\n",
      "\u001b[36m8bbbkc9zqw-algo-1-7uphg |\u001b[0m     \"is_hetero\": false,\n",
      "\u001b[36m8bbbkc9zqw-algo-1-7uphg |\u001b[0m     \"is_master\": true,\n",
      "\u001b[36m8bbbkc9zqw-algo-1-7uphg |\u001b[0m     \"is_modelparallel_enabled\": null,\n",
      "\u001b[36m8bbbkc9zqw-algo-1-7uphg |\u001b[0m     \"is_smddpmprun_installed\": true,\n",
      "\u001b[36m8bbbkc9zqw-algo-1-7uphg |\u001b[0m     \"job_name\": \"pytorch-training-2023-03-04-14-23-57-521\",\n",
      "\u001b[36m8bbbkc9zqw-algo-1-7uphg |\u001b[0m     \"log_level\": 20,\n",
      "\u001b[36m8bbbkc9zqw-algo-1-7uphg |\u001b[0m     \"master_hostname\": \"algo-1-7uphg\",\n",
      "\u001b[36m8bbbkc9zqw-algo-1-7uphg |\u001b[0m     \"model_dir\": \"/opt/ml/model\",\n",
      "\u001b[36m8bbbkc9zqw-algo-1-7uphg |\u001b[0m     \"module_dir\": \"s3://sagemaker-us-east-1-057716757052/pytorch-training-2023-03-04-14-23-57-521/source/sourcedir.tar.gz\",\n",
      "\u001b[36m8bbbkc9zqw-algo-1-7uphg |\u001b[0m     \"module_name\": \"TFT_Train\",\n",
      "\u001b[36m8bbbkc9zqw-algo-1-7uphg |\u001b[0m     \"network_interface_name\": \"eth0\",\n",
      "\u001b[36m8bbbkc9zqw-algo-1-7uphg |\u001b[0m     \"num_cpus\": 64,\n",
      "\u001b[36m8bbbkc9zqw-algo-1-7uphg |\u001b[0m     \"num_gpus\": 8,\n",
      "\u001b[36m8bbbkc9zqw-algo-1-7uphg |\u001b[0m     \"num_neurons\": 0,\n",
      "\u001b[36m8bbbkc9zqw-algo-1-7uphg |\u001b[0m     \"output_data_dir\": \"/opt/ml/output/data\",\n",
      "\u001b[36m8bbbkc9zqw-algo-1-7uphg |\u001b[0m     \"output_dir\": \"/opt/ml/output\",\n",
      "\u001b[36m8bbbkc9zqw-algo-1-7uphg |\u001b[0m     \"output_intermediate_dir\": \"/opt/ml/output/intermediate\",\n",
      "\u001b[36m8bbbkc9zqw-algo-1-7uphg |\u001b[0m     \"resource_config\": {\n",
      "\u001b[36m8bbbkc9zqw-algo-1-7uphg |\u001b[0m         \"current_host\": \"algo-1-7uphg\",\n",
      "\u001b[36m8bbbkc9zqw-algo-1-7uphg |\u001b[0m         \"hosts\": [\n",
      "\u001b[36m8bbbkc9zqw-algo-1-7uphg |\u001b[0m             \"algo-1-7uphg\"\n",
      "\u001b[36m8bbbkc9zqw-algo-1-7uphg |\u001b[0m         ]\n",
      "\u001b[36m8bbbkc9zqw-algo-1-7uphg |\u001b[0m     },\n",
      "\u001b[36m8bbbkc9zqw-algo-1-7uphg |\u001b[0m     \"user_entry_point\": \"TFT_Train.py\"\n",
      "\u001b[36m8bbbkc9zqw-algo-1-7uphg |\u001b[0m }\n",
      "\u001b[36m8bbbkc9zqw-algo-1-7uphg |\u001b[0m \n",
      "\u001b[36m8bbbkc9zqw-algo-1-7uphg |\u001b[0m Environment variables:\n",
      "\u001b[36m8bbbkc9zqw-algo-1-7uphg |\u001b[0m \n",
      "\u001b[36m8bbbkc9zqw-algo-1-7uphg |\u001b[0m SM_HOSTS=[\"algo-1-7uphg\"]\n",
      "\u001b[36m8bbbkc9zqw-algo-1-7uphg |\u001b[0m SM_NETWORK_INTERFACE_NAME=eth0\n",
      "\u001b[36m8bbbkc9zqw-algo-1-7uphg |\u001b[0m SM_HPS={\"epochs\":2,\"n_gpus\":8}\n",
      "\u001b[36m8bbbkc9zqw-algo-1-7uphg |\u001b[0m SM_USER_ENTRY_POINT=TFT_Train.py\n",
      "\u001b[36m8bbbkc9zqw-algo-1-7uphg |\u001b[0m SM_FRAMEWORK_PARAMS={}\n",
      "\u001b[36m8bbbkc9zqw-algo-1-7uphg |\u001b[0m SM_RESOURCE_CONFIG={\"current_host\":\"algo-1-7uphg\",\"hosts\":[\"algo-1-7uphg\"]}\n",
      "\u001b[36m8bbbkc9zqw-algo-1-7uphg |\u001b[0m SM_INPUT_DATA_CONFIG={}\n",
      "\u001b[36m8bbbkc9zqw-algo-1-7uphg |\u001b[0m SM_OUTPUT_DATA_DIR=/opt/ml/output/data\n",
      "\u001b[36m8bbbkc9zqw-algo-1-7uphg |\u001b[0m SM_CHANNELS=[]\n",
      "\u001b[36m8bbbkc9zqw-algo-1-7uphg |\u001b[0m SM_CURRENT_HOST=algo-1-7uphg\n",
      "\u001b[36m8bbbkc9zqw-algo-1-7uphg |\u001b[0m SM_CURRENT_INSTANCE_TYPE=local\n",
      "\u001b[36m8bbbkc9zqw-algo-1-7uphg |\u001b[0m SM_CURRENT_INSTANCE_GROUP=homogeneousCluster\n",
      "\u001b[36m8bbbkc9zqw-algo-1-7uphg |\u001b[0m SM_CURRENT_INSTANCE_GROUP_HOSTS=[]\n",
      "\u001b[36m8bbbkc9zqw-algo-1-7uphg |\u001b[0m SM_INSTANCE_GROUPS=[]\n",
      "\u001b[36m8bbbkc9zqw-algo-1-7uphg |\u001b[0m SM_INSTANCE_GROUPS_DICT={}\n",
      "\u001b[36m8bbbkc9zqw-algo-1-7uphg |\u001b[0m SM_DISTRIBUTION_INSTANCE_GROUPS=[]\n",
      "\u001b[36m8bbbkc9zqw-algo-1-7uphg |\u001b[0m SM_IS_HETERO=false\n",
      "\u001b[36m8bbbkc9zqw-algo-1-7uphg |\u001b[0m SM_MODULE_NAME=TFT_Train\n",
      "\u001b[36m8bbbkc9zqw-algo-1-7uphg |\u001b[0m SM_LOG_LEVEL=20\n",
      "\u001b[36m8bbbkc9zqw-algo-1-7uphg |\u001b[0m SM_FRAMEWORK_MODULE=sagemaker_pytorch_container.training:main\n",
      "\u001b[36m8bbbkc9zqw-algo-1-7uphg |\u001b[0m SM_INPUT_DIR=/opt/ml/input\n",
      "\u001b[36m8bbbkc9zqw-algo-1-7uphg |\u001b[0m SM_INPUT_CONFIG_DIR=/opt/ml/input/config\n",
      "\u001b[36m8bbbkc9zqw-algo-1-7uphg |\u001b[0m SM_OUTPUT_DIR=/opt/ml/output\n",
      "\u001b[36m8bbbkc9zqw-algo-1-7uphg |\u001b[0m SM_NUM_CPUS=64\n",
      "\u001b[36m8bbbkc9zqw-algo-1-7uphg |\u001b[0m SM_NUM_GPUS=8\n",
      "\u001b[36m8bbbkc9zqw-algo-1-7uphg |\u001b[0m SM_NUM_NEURONS=0\n",
      "\u001b[36m8bbbkc9zqw-algo-1-7uphg |\u001b[0m SM_MODEL_DIR=/opt/ml/model\n",
      "\u001b[36m8bbbkc9zqw-algo-1-7uphg |\u001b[0m SM_MODULE_DIR=s3://sagemaker-us-east-1-057716757052/pytorch-training-2023-03-04-14-23-57-521/source/sourcedir.tar.gz\n",
      "\u001b[36m8bbbkc9zqw-algo-1-7uphg |\u001b[0m SM_TRAINING_ENV={\"additional_framework_parameters\":{},\"channel_input_dirs\":{},\"current_host\":\"algo-1-7uphg\",\"current_instance_group\":\"homogeneousCluster\",\"current_instance_group_hosts\":[],\"current_instance_type\":\"local\",\"distribution_hosts\":[\"algo-1-7uphg\"],\"distribution_instance_groups\":[],\"framework_module\":\"sagemaker_pytorch_container.training:main\",\"hosts\":[\"algo-1-7uphg\"],\"hyperparameters\":{\"epochs\":2,\"n_gpus\":8},\"input_config_dir\":\"/opt/ml/input/config\",\"input_data_config\":{},\"input_dir\":\"/opt/ml/input\",\"instance_groups\":[],\"instance_groups_dict\":{},\"is_hetero\":false,\"is_master\":true,\"is_modelparallel_enabled\":null,\"is_smddpmprun_installed\":true,\"job_name\":\"pytorch-training-2023-03-04-14-23-57-521\",\"log_level\":20,\"master_hostname\":\"algo-1-7uphg\",\"model_dir\":\"/opt/ml/model\",\"module_dir\":\"s3://sagemaker-us-east-1-057716757052/pytorch-training-2023-03-04-14-23-57-521/source/sourcedir.tar.gz\",\"module_name\":\"TFT_Train\",\"network_interface_name\":\"eth0\",\"num_cpus\":64,\"num_gpus\":8,\"num_neurons\":0,\"output_data_dir\":\"/opt/ml/output/data\",\"output_dir\":\"/opt/ml/output\",\"output_intermediate_dir\":\"/opt/ml/output/intermediate\",\"resource_config\":{\"current_host\":\"algo-1-7uphg\",\"hosts\":[\"algo-1-7uphg\"]},\"user_entry_point\":\"TFT_Train.py\"}\n",
      "\u001b[36m8bbbkc9zqw-algo-1-7uphg |\u001b[0m SM_USER_ARGS=[\"--epochs\",\"2\",\"--n_gpus\",\"8\"]\n",
      "\u001b[36m8bbbkc9zqw-algo-1-7uphg |\u001b[0m SM_OUTPUT_INTERMEDIATE_DIR=/opt/ml/output/intermediate\n",
      "\u001b[36m8bbbkc9zqw-algo-1-7uphg |\u001b[0m SM_HP_EPOCHS=2\n",
      "\u001b[36m8bbbkc9zqw-algo-1-7uphg |\u001b[0m SM_HP_N_GPUS=8\n",
      "\u001b[36m8bbbkc9zqw-algo-1-7uphg |\u001b[0m PYTHONPATH=/opt/ml/code:/opt/conda/bin:/opt/conda/lib/python38.zip:/opt/conda/lib/python3.8:/opt/conda/lib/python3.8/lib-dynload:/opt/conda/lib/python3.8/site-packages:/opt/conda/lib/python3.8/site-packages/smdebug-1.0.24b20230214-py3.8.egg:/opt/conda/lib/python3.8/site-packages/pyinstrument-3.4.2-py3.8.egg:/opt/conda/lib/python3.8/site-packages/pyinstrument_cext-0.2.4-py3.8-linux-x86_64.egg:/opt/conda/lib/python3.8/site-packages/flash_attn-0.1-py3.8-linux-x86_64.egg:/opt/conda/lib/python3.8/site-packages/einops-0.6.0-py3.8.egg\n",
      "\u001b[36m8bbbkc9zqw-algo-1-7uphg |\u001b[0m \n",
      "\u001b[36m8bbbkc9zqw-algo-1-7uphg |\u001b[0m Invoking script with the following command:\n",
      "\u001b[36m8bbbkc9zqw-algo-1-7uphg |\u001b[0m \n",
      "\u001b[36m8bbbkc9zqw-algo-1-7uphg |\u001b[0m /opt/conda/bin/python3.8 TFT_Train.py --epochs 2 --n_gpus 8\n",
      "\u001b[36m8bbbkc9zqw-algo-1-7uphg |\u001b[0m \n",
      "\u001b[36m8bbbkc9zqw-algo-1-7uphg |\u001b[0m \n",
      "\u001b[36m8bbbkc9zqw-algo-1-7uphg |\u001b[0m 2023-03-04 14:24:29,255 sagemaker-training-toolkit INFO     Exceptions not imported for SageMaker TF as Tensorflow is not installed.\n",
      "\u001b[36m8bbbkc9zqw-algo-1-7uphg |\u001b[0m Not running on notebook\n",
      "\u001b[36m8bbbkc9zqw-algo-1-7uphg |\u001b[0m ***** Arguments *****\n",
      "\u001b[36m8bbbkc9zqw-algo-1-7uphg |\u001b[0m epochs=2\n",
      "\u001b[36m8bbbkc9zqw-algo-1-7uphg |\u001b[0m seed=100\n",
      "\u001b[36m8bbbkc9zqw-algo-1-7uphg |\u001b[0m train_batch_size=64\n",
      "\u001b[36m8bbbkc9zqw-algo-1-7uphg |\u001b[0m model_dir=/opt/ml/model\n",
      "\u001b[36m8bbbkc9zqw-algo-1-7uphg |\u001b[0m n_gpus=8\n",
      "\u001b[36m8bbbkc9zqw-algo-1-7uphg |\u001b[0m GPU available: True, used: True\n",
      "\u001b[36m8bbbkc9zqw-algo-1-7uphg |\u001b[0m TPU available: False, using: 0 TPU cores\n",
      "\u001b[36m8bbbkc9zqw-algo-1-7uphg |\u001b[0m IPU available: False, using: 0 IPUs\n",
      "\u001b[36m8bbbkc9zqw-algo-1-7uphg |\u001b[0m HPU available: False, using: 0 HPUs\n",
      "\u001b[36m8bbbkc9zqw-algo-1-7uphg |\u001b[0m Number of parameters in network: 29.7k\n",
      "\u001b[36m8bbbkc9zqw-algo-1-7uphg |\u001b[0m Initializing distributed: GLOBAL_RANK: 0, MEMBER: 1/8\n",
      "\u001b[36m8bbbkc9zqw-algo-1-7uphg |\u001b[0m Initializing distributed: GLOBAL_RANK: 1, MEMBER: 2/8\n",
      "\u001b[36m8bbbkc9zqw-algo-1-7uphg |\u001b[0m Initializing distributed: GLOBAL_RANK: 2, MEMBER: 3/8\n",
      "\u001b[36m8bbbkc9zqw-algo-1-7uphg |\u001b[0m Initializing distributed: GLOBAL_RANK: 3, MEMBER: 4/8\n",
      "\u001b[36m8bbbkc9zqw-algo-1-7uphg |\u001b[0m Initializing distributed: GLOBAL_RANK: 4, MEMBER: 5/8\n",
      "\u001b[36m8bbbkc9zqw-algo-1-7uphg |\u001b[0m Initializing distributed: GLOBAL_RANK: 5, MEMBER: 6/8\n",
      "\u001b[36m8bbbkc9zqw-algo-1-7uphg |\u001b[0m Initializing distributed: GLOBAL_RANK: 6, MEMBER: 7/8\n",
      "\u001b[36m8bbbkc9zqw-algo-1-7uphg |\u001b[0m Initializing distributed: GLOBAL_RANK: 7, MEMBER: 8/8\n",
      "\u001b[36m8bbbkc9zqw-algo-1-7uphg |\u001b[0m ----------------------------------------------------------------------------------------------------\n",
      "\u001b[36m8bbbkc9zqw-algo-1-7uphg |\u001b[0m distributed_backend=nccl\n",
      "\u001b[36m8bbbkc9zqw-algo-1-7uphg |\u001b[0m All distributed processes registered. Starting with 8 processes\n",
      "\u001b[36m8bbbkc9zqw-algo-1-7uphg |\u001b[0m ----------------------------------------------------------------------------------------------------\n",
      "\u001b[36m8bbbkc9zqw-algo-1-7uphg |\u001b[0m Missing logger folder: lightning_logs/lightning_logs\n",
      "\u001b[36m8bbbkc9zqw-algo-1-7uphg |\u001b[0m Missing logger folder: lightning_logs/lightning_logs\n",
      "\u001b[36m8bbbkc9zqw-algo-1-7uphg |\u001b[0m Missing logger folder: lightning_logs/lightning_logs\n",
      "\u001b[36m8bbbkc9zqw-algo-1-7uphg |\u001b[0m Missing logger folder: lightning_logs/lightning_logs\n",
      "\u001b[36m8bbbkc9zqw-algo-1-7uphg |\u001b[0m Missing logger folder: lightning_logs/lightning_logs\n",
      "\u001b[36m8bbbkc9zqw-algo-1-7uphg |\u001b[0m Missing logger folder: lightning_logs/lightning_logs\n",
      "\u001b[36m8bbbkc9zqw-algo-1-7uphg |\u001b[0m Missing logger folder: lightning_logs/lightning_logs\n",
      "\u001b[36m8bbbkc9zqw-algo-1-7uphg |\u001b[0m Missing logger folder: lightning_logs/lightning_logs\n",
      "\u001b[36m8bbbkc9zqw-algo-1-7uphg |\u001b[0m algo-1-7uphg:364:364 [0] ofi_init:1288 NCCL WARN NET/OFI Only EFA provider is supported\n",
      "\u001b[36m8bbbkc9zqw-algo-1-7uphg |\u001b[0m \n",
      "\u001b[36m8bbbkc9zqw-algo-1-7uphg |\u001b[0m algo-1-7uphg:364:364 [0] ofi_init:1339 NCCL WARN NET/OFI aws-ofi-nccl initialization failed\n",
      "\u001b[36m8bbbkc9zqw-algo-1-7uphg |\u001b[0m NCCL version 2.10.3+cuda11.3\n",
      "\u001b[36m8bbbkc9zqw-algo-1-7uphg |\u001b[0m algo-1-7uphg:810:810 [6] ofi_init:1288 NCCL WARN NET/OFI Only EFA provider is supported\n",
      "\u001b[36m8bbbkc9zqw-algo-1-7uphg |\u001b[0m \n",
      "\u001b[36m8bbbkc9zqw-algo-1-7uphg |\u001b[0m algo-1-7uphg:510:510 [2] ofi_init:1288 NCCL WARN NET/OFI Only EFA provider is supported\n",
      "\u001b[36m8bbbkc9zqw-algo-1-7uphg |\u001b[0m \n",
      "\u001b[36m8bbbkc9zqw-algo-1-7uphg |\u001b[0m algo-1-7uphg:885:885 [7] ofi_init:1288 NCCL WARN NET/OFI Only EFA provider is supported\n",
      "\u001b[36m8bbbkc9zqw-algo-1-7uphg |\u001b[0m \n",
      "\u001b[36m8bbbkc9zqw-algo-1-7uphg |\u001b[0m algo-1-7uphg:585:585 [3] ofi_init:1288 NCCL WARN NET/OFI Only EFA provider is supported\n",
      "\u001b[36m8bbbkc9zqw-algo-1-7uphg |\u001b[0m \n",
      "\u001b[36m8bbbkc9zqw-algo-1-7uphg |\u001b[0m algo-1-7uphg:735:735 [5] ofi_init:1288 NCCL WARN NET/OFI Only EFA provider is supported\n",
      "\u001b[36m8bbbkc9zqw-algo-1-7uphg |\u001b[0m \n",
      "\u001b[36m8bbbkc9zqw-algo-1-7uphg |\u001b[0m algo-1-7uphg:810:810 [6] ofi_init:1339 NCCL WARN NET/OFI aws-ofi-nccl initialization failed\n",
      "\u001b[36m8bbbkc9zqw-algo-1-7uphg |\u001b[0m \n",
      "\u001b[36m8bbbkc9zqw-algo-1-7uphg |\u001b[0m algo-1-7uphg:885:885 [7] ofi_init:1339 NCCL WARN NET/OFI aws-ofi-nccl initialization failed\n",
      "\u001b[36m8bbbkc9zqw-algo-1-7uphg |\u001b[0m \n",
      "\u001b[36m8bbbkc9zqw-algo-1-7uphg |\u001b[0m algo-1-7uphg:510:510 [2] ofi_init:1339 NCCL WARN NET/OFI aws-ofi-nccl initialization failed\n",
      "\u001b[36m8bbbkc9zqw-algo-1-7uphg |\u001b[0m \n",
      "\u001b[36m8bbbkc9zqw-algo-1-7uphg |\u001b[0m algo-1-7uphg:585:585 [3] ofi_init:1339 NCCL WARN NET/OFI aws-ofi-nccl initialization failed\n",
      "\u001b[36m8bbbkc9zqw-algo-1-7uphg |\u001b[0m \n",
      "\u001b[36m8bbbkc9zqw-algo-1-7uphg |\u001b[0m algo-1-7uphg:735:735 [5] ofi_init:1339 NCCL WARN NET/OFI aws-ofi-nccl initialization failed\n",
      "\u001b[36m8bbbkc9zqw-algo-1-7uphg |\u001b[0m algo-1-7uphg:660:660 [4] ofi_init:1288 NCCL WARN NET/OFI Only EFA provider is supported\n",
      "\u001b[36m8bbbkc9zqw-algo-1-7uphg |\u001b[0m \n",
      "\u001b[36m8bbbkc9zqw-algo-1-7uphg |\u001b[0m algo-1-7uphg:660:660 [4] ofi_init:1339 NCCL WARN NET/OFI aws-ofi-nccl initialization failed\n",
      "\u001b[36m8bbbkc9zqw-algo-1-7uphg |\u001b[0m algo-1-7uphg:437:437 [1] ofi_init:1288 NCCL WARN NET/OFI Only EFA provider is supported\n",
      "\u001b[36m8bbbkc9zqw-algo-1-7uphg |\u001b[0m \n",
      "\u001b[36m8bbbkc9zqw-algo-1-7uphg |\u001b[0m algo-1-7uphg:437:437 [1] ofi_init:1339 NCCL WARN NET/OFI aws-ofi-nccl initialization failed\n",
      "\u001b[36m8bbbkc9zqw-algo-1-7uphg |\u001b[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]\n",
      "\u001b[36m8bbbkc9zqw-algo-1-7uphg |\u001b[0m LOCAL_RANK: 6 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]\n",
      "\u001b[36m8bbbkc9zqw-algo-1-7uphg |\u001b[0m LOCAL_RANK: 4 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]\n",
      "\u001b[36m8bbbkc9zqw-algo-1-7uphg |\u001b[0m LOCAL_RANK: 5 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]\n",
      "\u001b[36m8bbbkc9zqw-algo-1-7uphg |\u001b[0m LOCAL_RANK: 2 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]\n",
      "\u001b[36m8bbbkc9zqw-algo-1-7uphg |\u001b[0m LOCAL_RANK: 3 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]\n",
      "\u001b[36m8bbbkc9zqw-algo-1-7uphg |\u001b[0m LOCAL_RANK: 1 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]\n",
      "\u001b[36m8bbbkc9zqw-algo-1-7uphg |\u001b[0m LOCAL_RANK: 7 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]\n",
      "\u001b[36m8bbbkc9zqw-algo-1-7uphg |\u001b[0m | Name                               | Type                            | Params\n",
      "\u001b[36m8bbbkc9zqw-algo-1-7uphg |\u001b[0m ----------------------------------------------------------------------------------------\n",
      "\u001b[36m8bbbkc9zqw-algo-1-7uphg |\u001b[0m 0  | loss                               | QuantileLoss                    | 0     \n",
      "\u001b[36m8bbbkc9zqw-algo-1-7uphg |\u001b[0m 1  | logging_metrics                    | ModuleList                      | 0     \n",
      "\u001b[36m8bbbkc9zqw-algo-1-7uphg |\u001b[0m 2  | input_embeddings                   | MultiEmbedding                  | 1.3 K \n",
      "\u001b[36m8bbbkc9zqw-algo-1-7uphg |\u001b[0m 3  | prescalers                         | ModuleDict                      | 256   \n",
      "\u001b[36m8bbbkc9zqw-algo-1-7uphg |\u001b[0m 4  | static_variable_selection          | VariableSelectionNetwork        | 3.4 K \n",
      "\u001b[36m8bbbkc9zqw-algo-1-7uphg |\u001b[0m 5  | encoder_variable_selection         | VariableSelectionNetwork        | 8.0 K \n",
      "\u001b[36m8bbbkc9zqw-algo-1-7uphg |\u001b[0m 6  | decoder_variable_selection         | VariableSelectionNetwork        | 2.7 K \n",
      "\u001b[36m8bbbkc9zqw-algo-1-7uphg |\u001b[0m 7  | static_context_variable_selection  | GatedResidualNetwork            | 1.1 K \n",
      "\u001b[36m8bbbkc9zqw-algo-1-7uphg |\u001b[0m 8  | static_context_initial_hidden_lstm | GatedResidualNetwork            | 1.1 K \n",
      "\u001b[36m8bbbkc9zqw-algo-1-7uphg |\u001b[0m 9  | static_context_initial_cell_lstm   | GatedResidualNetwork            | 1.1 K \n",
      "\u001b[36m8bbbkc9zqw-algo-1-7uphg |\u001b[0m 10 | static_context_enrichment          | GatedResidualNetwork            | 1.1 K \n",
      "\u001b[36m8bbbkc9zqw-algo-1-7uphg |\u001b[0m 11 | lstm_encoder                       | LSTM                            | 2.2 K \n",
      "\u001b[36m8bbbkc9zqw-algo-1-7uphg |\u001b[0m 12 | lstm_decoder                       | LSTM                            | 2.2 K \n",
      "\u001b[36m8bbbkc9zqw-algo-1-7uphg |\u001b[0m 13 | post_lstm_gate_encoder             | GatedLinearUnit                 | 544   \n",
      "\u001b[36m8bbbkc9zqw-algo-1-7uphg |\u001b[0m 14 | post_lstm_add_norm_encoder         | AddNorm                         | 32    \n",
      "\u001b[36m8bbbkc9zqw-algo-1-7uphg |\u001b[0m 15 | static_enrichment                  | GatedResidualNetwork            | 1.4 K \n",
      "\u001b[36m8bbbkc9zqw-algo-1-7uphg |\u001b[0m 16 | multihead_attn                     | InterpretableMultiHeadAttention | 1.1 K \n",
      "\u001b[36m8bbbkc9zqw-algo-1-7uphg |\u001b[0m 17 | post_attn_gate_norm                | GateAddNorm                     | 576   \n",
      "\u001b[36m8bbbkc9zqw-algo-1-7uphg |\u001b[0m 18 | pos_wise_ff                        | GatedResidualNetwork            | 1.1 K \n",
      "\u001b[36m8bbbkc9zqw-algo-1-7uphg |\u001b[0m 19 | pre_output_gate_norm               | GateAddNorm                     | 576   \n",
      "\u001b[36m8bbbkc9zqw-algo-1-7uphg |\u001b[0m 20 | output_layer                       | Linear                          | 119   \n",
      "\u001b[36m8bbbkc9zqw-algo-1-7uphg |\u001b[0m ----------------------------------------------------------------------------------------\n",
      "\u001b[36m8bbbkc9zqw-algo-1-7uphg |\u001b[0m 29.7 K    Trainable params\n",
      "\u001b[36m8bbbkc9zqw-algo-1-7uphg |\u001b[0m 0         Non-trainable params\n",
      "\u001b[36m8bbbkc9zqw-algo-1-7uphg |\u001b[0m 29.7 K    Total params\n",
      "\u001b[36m8bbbkc9zqw-algo-1-7uphg |\u001b[0m 0.119     Total estimated model params size (MB)\n",
      "\u001b[36m8bbbkc9zqw-algo-1-7uphg |\u001b[0m Sanity Checking: 0it [00:00, ?it/s]\n",
      "\u001b[36m8bbbkc9zqw-algo-1-7uphg |\u001b[0m [2023-03-04 14:26:05.007 algo-1-7uphg:364 INFO utils.py:28] RULE_JOB_STOP_SIGNAL_FILENAME: None\n",
      "\u001b[36m8bbbkc9zqw-algo-1-7uphg |\u001b[0m [2023-03-04 14:26:05.007 algo-1-7uphg:810 INFO utils.py:28] RULE_JOB_STOP_SIGNAL_FILENAME: None\n",
      "\u001b[36m8bbbkc9zqw-algo-1-7uphg |\u001b[0m [2023-03-04 14:26:05.037 algo-1-7uphg:735 INFO utils.py:28] RULE_JOB_STOP_SIGNAL_FILENAME: None\n",
      "\u001b[36m8bbbkc9zqw-algo-1-7uphg |\u001b[0m [2023-03-04 14:26:05.038 algo-1-7uphg:585 INFO utils.py:28] RULE_JOB_STOP_SIGNAL_FILENAME: None\n",
      "\u001b[36m8bbbkc9zqw-algo-1-7uphg |\u001b[0m [2023-03-04 14:26:05.039 algo-1-7uphg:660 INFO utils.py:28] RULE_JOB_STOP_SIGNAL_FILENAME: None\n",
      "\u001b[36m8bbbkc9zqw-algo-1-7uphg |\u001b[0m [2023-03-04 14:26:05.040 algo-1-7uphg:510 INFO utils.py:28] RULE_JOB_STOP_SIGNAL_FILENAME: None\n",
      "\u001b[36m8bbbkc9zqw-algo-1-7uphg |\u001b[0m [2023-03-04 14:26:05.042 algo-1-7uphg:885 INFO utils.py:28] RULE_JOB_STOP_SIGNAL_FILENAME: None\n",
      "\u001b[36m8bbbkc9zqw-algo-1-7uphg |\u001b[0m [2023-03-04 14:26:05.042 algo-1-7uphg:437 INFO utils.py:28] RULE_JOB_STOP_SIGNAL_FILENAME: None\n",
      "\u001b[36m8bbbkc9zqw-algo-1-7uphg |\u001b[0m [2023-03-04 14:26:05.165 algo-1-7uphg:364 INFO profiler_config_parser.py:111] Unable to find config at /opt/ml/input/config/profilerconfig.json. Profiler is disabled.\n",
      "\u001b[36m8bbbkc9zqw-algo-1-7uphg |\u001b[0m [2023-03-04 14:26:05.165 algo-1-7uphg:810 INFO profiler_config_parser.py:111] Unable to find config at /opt/ml/input/config/profilerconfig.json. Profiler is disabled.\n",
      "\u001b[36m8bbbkc9zqw-algo-1-7uphg |\u001b[0m [2023-03-04 14:26:05.195 algo-1-7uphg:735 INFO profiler_config_parser.py:111] Unable to find config at /opt/ml/input/config/profilerconfig.json. Profiler is disabled.\n",
      "\u001b[36m8bbbkc9zqw-algo-1-7uphg |\u001b[0m [2023-03-04 14:26:05.196 algo-1-7uphg:585 INFO profiler_config_parser.py:111] Unable to find config at /opt/ml/input/config/profilerconfig.json. Profiler is disabled.\n",
      "\u001b[36m8bbbkc9zqw-algo-1-7uphg |\u001b[0m [2023-03-04 14:26:05.196 algo-1-7uphg:660 INFO profiler_config_parser.py:111] Unable to find config at /opt/ml/input/config/profilerconfig.json. Profiler is disabled.\n",
      "\u001b[36m8bbbkc9zqw-algo-1-7uphg |\u001b[0m [2023-03-04 14:26:05.200 algo-1-7uphg:510 INFO profiler_config_parser.py:111] Unable to find config at /opt/ml/input/config/profilerconfig.json. Profiler is disabled.\n",
      "\u001b[36m8bbbkc9zqw-algo-1-7uphg |\u001b[0m [2023-03-04 14:26:05.203 algo-1-7uphg:437 INFO profiler_config_parser.py:111] Unable to find config at /opt/ml/input/config/profilerconfig.json. Profiler is disabled.\n",
      "\u001b[36m8bbbkc9zqw-algo-1-7uphg |\u001b[0m [2023-03-04 14:26:05.205 algo-1-7uphg:885 INFO profiler_config_parser.py:111] Unable to find config at /opt/ml/input/config/profilerconfig.json. Profiler is disabled.\n",
      "\u001b[36m8bbbkc9zqw-algo-1-7uphg |\u001b[0m Sanity Checking:   0%|          | 0/1 [00:00<?, ?it/s]\n",
      "\u001b[36m8bbbkc9zqw-algo-1-7uphg |\u001b[0m Sanity Checking DataLoader 0:   0%|          | 0/1 [00:00<?, ?it/s]\n",
      "\u001b[36m8bbbkc9zqw-algo-1-7uphg |\u001b[0m Sanity Checking DataLoader 0: 100%|██████████| 1/1 [00:00<00:00,  1.28it/s]\n",
      "\u001b[36m8bbbkc9zqw-algo-1-7uphg |\u001b[0m Sanity Checking DataLoader 0: 100%|██████████| 1/1 [00:00<00:00,  1.28it/s]\n",
      "\u001b[36m8bbbkc9zqw-algo-1-7uphg |\u001b[0m \n",
      "\u001b[36m8bbbkc9zqw-algo-1-7uphg |\u001b[0m Training: 0it [00:00, ?it/s]\n",
      "\u001b[36m8bbbkc9zqw-algo-1-7uphg |\u001b[0m Training:   0%|          | 0/21 [00:00<?, ?it/s]\n",
      "\u001b[36m8bbbkc9zqw-algo-1-7uphg |\u001b[0m Epoch 0:   0%|          | 0/21 [00:00<?, ?it/s]\n",
      "\u001b[36m8bbbkc9zqw-algo-1-7uphg |\u001b[0m [W reducer.cpp:1251] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())\n",
      "\u001b[36m8bbbkc9zqw-algo-1-7uphg |\u001b[0m [W reducer.cpp:1251] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())\n",
      "\u001b[36m8bbbkc9zqw-algo-1-7uphg |\u001b[0m [W reducer.cpp:1251] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())\n",
      "\u001b[36m8bbbkc9zqw-algo-1-7uphg |\u001b[0m [W reducer.cpp:1251] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())\n",
      "\u001b[36m8bbbkc9zqw-algo-1-7uphg |\u001b[0m [W reducer.cpp:1251] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())\n",
      "\u001b[36m8bbbkc9zqw-algo-1-7uphg |\u001b[0m [W reducer.cpp:1251] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())\n",
      "\u001b[36m8bbbkc9zqw-algo-1-7uphg |\u001b[0m [W reducer.cpp:1251] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())\n",
      "\u001b[36m8bbbkc9zqw-algo-1-7uphg |\u001b[0m [W reducer.cpp:1251] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())\n",
      "\u001b[36m8bbbkc9zqw-algo-1-7uphg |\u001b[0m Epoch 0:   5%|▍         | 1/21 [00:00<00:09,  2.05it/s]\n",
      "\u001b[36m8bbbkc9zqw-algo-1-7uphg |\u001b[0m Epoch 0:   5%|▍         | 1/21 [00:00<00:09,  2.05it/s]\n",
      "\u001b[36m8bbbkc9zqw-algo-1-7uphg |\u001b[0m Epoch 0:   5%|▍         | 1/21 [00:00<00:09,  2.05it/s, loss=367, v_num=0, train_loss_step=367.0]\n",
      "\u001b[36m8bbbkc9zqw-algo-1-7uphg |\u001b[0m Epoch 0:  10%|▉         | 2/21 [00:00<00:07,  2.54it/s, loss=367, v_num=0, train_loss_step=367.0]\n",
      "\u001b[36m8bbbkc9zqw-algo-1-7uphg |\u001b[0m Epoch 0:  10%|▉         | 2/21 [00:00<00:07,  2.53it/s, loss=367, v_num=0, train_loss_step=367.0]\n",
      "\u001b[36m8bbbkc9zqw-algo-1-7uphg |\u001b[0m Epoch 0:  10%|▉         | 2/21 [00:00<00:07,  2.53it/s, loss=306, v_num=0, train_loss_step=245.0]\n",
      "\u001b[36m8bbbkc9zqw-algo-1-7uphg |\u001b[0m Epoch 0:  14%|█▍        | 3/21 [00:01<00:06,  2.73it/s, loss=306, v_num=0, train_loss_step=245.0]\n",
      "\u001b[36m8bbbkc9zqw-algo-1-7uphg |\u001b[0m Epoch 0:  14%|█▍        | 3/21 [00:01<00:06,  2.73it/s, loss=306, v_num=0, train_loss_step=245.0]\n",
      "\u001b[36m8bbbkc9zqw-algo-1-7uphg |\u001b[0m Epoch 0:  14%|█▍        | 3/21 [00:01<00:06,  2.73it/s, loss=331, v_num=0, train_loss_step=381.0]\n",
      "\u001b[36m8bbbkc9zqw-algo-1-7uphg |\u001b[0m Epoch 0:  19%|█▉        | 4/21 [00:01<00:05,  2.97it/s, loss=331, v_num=0, train_loss_step=381.0]\n",
      "\u001b[36m8bbbkc9zqw-algo-1-7uphg |\u001b[0m Epoch 0:  19%|█▉        | 4/21 [00:01<00:05,  2.97it/s, loss=331, v_num=0, train_loss_step=381.0]\n",
      "\u001b[36m8bbbkc9zqw-algo-1-7uphg |\u001b[0m Epoch 0:  19%|█▉        | 4/21 [00:01<00:05,  2.97it/s, loss=316, v_num=0, train_loss_step=273.0]\n",
      "\u001b[36m8bbbkc9zqw-algo-1-7uphg |\u001b[0m Epoch 0:  24%|██▍       | 5/21 [00:01<00:05,  3.10it/s, loss=316, v_num=0, train_loss_step=273.0]\n",
      "\u001b[36m8bbbkc9zqw-algo-1-7uphg |\u001b[0m Epoch 0:  24%|██▍       | 5/21 [00:01<00:05,  3.10it/s, loss=316, v_num=0, train_loss_step=273.0]\n",
      "\u001b[36m8bbbkc9zqw-algo-1-7uphg |\u001b[0m Epoch 0:  24%|██▍       | 5/21 [00:01<00:05,  3.10it/s, loss=323, v_num=0, train_loss_step=350.0]\n",
      "\u001b[36m8bbbkc9zqw-algo-1-7uphg |\u001b[0m Epoch 0:  29%|██▊       | 6/21 [00:01<00:04,  3.12it/s, loss=323, v_num=0, train_loss_step=350.0]\n",
      "\u001b[36m8bbbkc9zqw-algo-1-7uphg |\u001b[0m Epoch 0:  29%|██▊       | 6/21 [00:01<00:04,  3.12it/s, loss=323, v_num=0, train_loss_step=350.0]\n",
      "\u001b[36m8bbbkc9zqw-algo-1-7uphg |\u001b[0m Epoch 0:  29%|██▊       | 6/21 [00:01<00:04,  3.12it/s, loss=304, v_num=0, train_loss_step=208.0]\n",
      "\u001b[36m8bbbkc9zqw-algo-1-7uphg |\u001b[0m Epoch 0:  33%|███▎      | 7/21 [00:02<00:04,  3.19it/s, loss=304, v_num=0, train_loss_step=208.0]\n",
      "\u001b[36m8bbbkc9zqw-algo-1-7uphg |\u001b[0m Epoch 0:  33%|███▎      | 7/21 [00:02<00:04,  3.19it/s, loss=304, v_num=0, train_loss_step=208.0]\n",
      "\u001b[36m8bbbkc9zqw-algo-1-7uphg |\u001b[0m Epoch 0:  33%|███▎      | 7/21 [00:02<00:04,  3.19it/s, loss=302, v_num=0, train_loss_step=292.0]\n",
      "\u001b[36m8bbbkc9zqw-algo-1-7uphg |\u001b[0m Epoch 0:  38%|███▊      | 8/21 [00:02<00:04,  3.22it/s, loss=302, v_num=0, train_loss_step=292.0]\n",
      "\u001b[36m8bbbkc9zqw-algo-1-7uphg |\u001b[0m Epoch 0:  38%|███▊      | 8/21 [00:02<00:04,  3.22it/s, loss=302, v_num=0, train_loss_step=292.0]\n",
      "\u001b[36m8bbbkc9zqw-algo-1-7uphg |\u001b[0m Epoch 0:  38%|███▊      | 8/21 [00:02<00:04,  3.22it/s, loss=293, v_num=0, train_loss_step=225.0]\n",
      "\u001b[36m8bbbkc9zqw-algo-1-7uphg |\u001b[0m Epoch 0:  43%|████▎     | 9/21 [00:02<00:03,  3.25it/s, loss=293, v_num=0, train_loss_step=225.0]\n",
      "\u001b[36m8bbbkc9zqw-algo-1-7uphg |\u001b[0m Epoch 0:  43%|████▎     | 9/21 [00:02<00:03,  3.25it/s, loss=293, v_num=0, train_loss_step=225.0]\n",
      "\u001b[36m8bbbkc9zqw-algo-1-7uphg |\u001b[0m Epoch 0:  43%|████▎     | 9/21 [00:02<00:03,  3.25it/s, loss=297, v_num=0, train_loss_step=332.0]\n",
      "\u001b[36m8bbbkc9zqw-algo-1-7uphg |\u001b[0m Epoch 0:  48%|████▊     | 10/21 [00:03<00:03,  3.28it/s, loss=297, v_num=0, train_loss_step=332.0]\n",
      "\u001b[36m8bbbkc9zqw-algo-1-7uphg |\u001b[0m Epoch 0:  48%|████▊     | 10/21 [00:03<00:03,  3.28it/s, loss=297, v_num=0, train_loss_step=332.0]\n",
      "\u001b[36m8bbbkc9zqw-algo-1-7uphg |\u001b[0m Epoch 0:  48%|████▊     | 10/21 [00:03<00:03,  3.28it/s, loss=293, v_num=0, train_loss_step=254.0]\n",
      "\u001b[36m8bbbkc9zqw-algo-1-7uphg |\u001b[0m Epoch 0:  52%|█████▏    | 11/21 [00:03<00:03,  3.29it/s, loss=293, v_num=0, train_loss_step=254.0]\n",
      "\u001b[36m8bbbkc9zqw-algo-1-7uphg |\u001b[0m Epoch 0:  52%|█████▏    | 11/21 [00:03<00:03,  3.29it/s, loss=293, v_num=0, train_loss_step=254.0]\n",
      "\u001b[36m8bbbkc9zqw-algo-1-7uphg |\u001b[0m Epoch 0:  52%|█████▏    | 11/21 [00:03<00:03,  3.29it/s, loss=285, v_num=0, train_loss_step=209.0]\n",
      "\u001b[36m8bbbkc9zqw-algo-1-7uphg |\u001b[0m Epoch 0:  57%|█████▋    | 12/21 [00:03<00:02,  3.22it/s, loss=285, v_num=0, train_loss_step=209.0]\n",
      "\u001b[36m8bbbkc9zqw-algo-1-7uphg |\u001b[0m Epoch 0:  57%|█████▋    | 12/21 [00:03<00:02,  3.22it/s, loss=285, v_num=0, train_loss_step=209.0]\n",
      "\u001b[36m8bbbkc9zqw-algo-1-7uphg |\u001b[0m Epoch 0:  57%|█████▋    | 12/21 [00:03<00:02,  3.22it/s, loss=282, v_num=0, train_loss_step=253.0]\n",
      "\u001b[36m8bbbkc9zqw-algo-1-7uphg |\u001b[0m Epoch 0:  62%|██████▏   | 13/21 [00:04<00:02,  3.23it/s, loss=282, v_num=0, train_loss_step=253.0]\n",
      "\u001b[36m8bbbkc9zqw-algo-1-7uphg |\u001b[0m Epoch 0:  62%|██████▏   | 13/21 [00:04<00:02,  3.23it/s, loss=282, v_num=0, train_loss_step=253.0]\n",
      "\u001b[36m8bbbkc9zqw-algo-1-7uphg |\u001b[0m Epoch 0:  62%|██████▏   | 13/21 [00:04<00:02,  3.23it/s, loss=279, v_num=0, train_loss_step=242.0]\n",
      "\u001b[36m8bbbkc9zqw-algo-1-7uphg |\u001b[0m Epoch 0:  67%|██████▋   | 14/21 [00:04<00:02,  3.24it/s, loss=279, v_num=0, train_loss_step=242.0]\n",
      "\u001b[36m8bbbkc9zqw-algo-1-7uphg |\u001b[0m Epoch 0:  67%|██████▋   | 14/21 [00:04<00:02,  3.24it/s, loss=279, v_num=0, train_loss_step=242.0]\n",
      "\u001b[36m8bbbkc9zqw-algo-1-7uphg |\u001b[0m Epoch 0:  67%|██████▋   | 14/21 [00:04<00:02,  3.24it/s, loss=277, v_num=0, train_loss_step=249.0]\n",
      "\u001b[36m8bbbkc9zqw-algo-1-7uphg |\u001b[0m Epoch 0:  71%|███████▏  | 15/21 [00:04<00:01,  3.25it/s, loss=277, v_num=0, train_loss_step=249.0]\n",
      "\u001b[36m8bbbkc9zqw-algo-1-7uphg |\u001b[0m Epoch 0:  71%|███████▏  | 15/21 [00:04<00:01,  3.25it/s, loss=277, v_num=0, train_loss_step=249.0]\n",
      "\u001b[36m8bbbkc9zqw-algo-1-7uphg |\u001b[0m Epoch 0:  71%|███████▏  | 15/21 [00:04<00:01,  3.25it/s, loss=274, v_num=0, train_loss_step=236.0]\n",
      "\u001b[36m8bbbkc9zqw-algo-1-7uphg |\u001b[0m Epoch 0:  76%|███████▌  | 16/21 [00:04<00:01,  3.26it/s, loss=274, v_num=0, train_loss_step=236.0]\n",
      "\u001b[36m8bbbkc9zqw-algo-1-7uphg |\u001b[0m Epoch 0:  76%|███████▌  | 16/21 [00:04<00:01,  3.26it/s, loss=274, v_num=0, train_loss_step=236.0]\n",
      "\u001b[36m8bbbkc9zqw-algo-1-7uphg |\u001b[0m Epoch 0:  76%|███████▌  | 16/21 [00:04<00:01,  3.26it/s, loss=272, v_num=0, train_loss_step=232.0]\n",
      "\u001b[36m8bbbkc9zqw-algo-1-7uphg |\u001b[0m Epoch 0:  81%|████████  | 17/21 [00:05<00:01,  3.26it/s, loss=272, v_num=0, train_loss_step=232.0]\n",
      "\u001b[36m8bbbkc9zqw-algo-1-7uphg |\u001b[0m Epoch 0:  81%|████████  | 17/21 [00:05<00:01,  3.26it/s, loss=272, v_num=0, train_loss_step=232.0]\n",
      "\u001b[36m8bbbkc9zqw-algo-1-7uphg |\u001b[0m Epoch 0:  81%|████████  | 17/21 [00:05<00:01,  3.26it/s, loss=268, v_num=0, train_loss_step=212.0]\n",
      "\u001b[36m8bbbkc9zqw-algo-1-7uphg |\u001b[0m Epoch 0:  86%|████████▌ | 18/21 [00:05<00:00,  3.25it/s, loss=268, v_num=0, train_loss_step=212.0]\n",
      "\u001b[36m8bbbkc9zqw-algo-1-7uphg |\u001b[0m Epoch 0:  86%|████████▌ | 18/21 [00:05<00:00,  3.25it/s, loss=268, v_num=0, train_loss_step=212.0]\n",
      "\u001b[36m8bbbkc9zqw-algo-1-7uphg |\u001b[0m Epoch 0:  86%|████████▌ | 18/21 [00:05<00:00,  3.25it/s, loss=263, v_num=0, train_loss_step=168.0]\n",
      "\u001b[36m8bbbkc9zqw-algo-1-7uphg |\u001b[0m Epoch 0:  90%|█████████ | 19/21 [00:05<00:00,  3.26it/s, loss=263, v_num=0, train_loss_step=168.0]\n",
      "\u001b[36m8bbbkc9zqw-algo-1-7uphg |\u001b[0m Epoch 0:  90%|█████████ | 19/21 [00:05<00:00,  3.26it/s, loss=263, v_num=0, train_loss_step=168.0]\n",
      "\u001b[36m8bbbkc9zqw-algo-1-7uphg |\u001b[0m Epoch 0:  90%|█████████ | 19/21 [00:05<00:00,  3.26it/s, loss=257, v_num=0, train_loss_step=155.0]\n",
      "\u001b[36m8bbbkc9zqw-algo-1-7uphg |\u001b[0m Epoch 0:  95%|█████████▌| 20/21 [00:06<00:00,  3.26it/s, loss=257, v_num=0, train_loss_step=155.0]\n",
      "\u001b[36m8bbbkc9zqw-algo-1-7uphg |\u001b[0m Epoch 0:  95%|█████████▌| 20/21 [00:06<00:00,  3.26it/s, loss=257, v_num=0, train_loss_step=155.0]\n",
      "\u001b[36m8bbbkc9zqw-algo-1-7uphg |\u001b[0m Epoch 0:  95%|█████████▌| 20/21 [00:06<00:00,  3.26it/s, loss=255, v_num=0, train_loss_step=228.0]\n",
      "\u001b[36m8bbbkc9zqw-algo-1-7uphg |\u001b[0m Validation: 0it [00:00, ?it/s]\n",
      "\u001b[36m8bbbkc9zqw-algo-1-7uphg |\u001b[0m \u001b[A\n",
      "\u001b[36m8bbbkc9zqw-algo-1-7uphg |\u001b[0m \n",
      "\u001b[36m8bbbkc9zqw-algo-1-7uphg |\u001b[0m Validation:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "\u001b[36m8bbbkc9zqw-algo-1-7uphg |\u001b[0m Validation DataLoader 0:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "\u001b[36m8bbbkc9zqw-algo-1-7uphg |\u001b[0m \n",
      "\u001b[36m8bbbkc9zqw-algo-1-7uphg |\u001b[0m Validation DataLoader 0: 100%|██████████| 1/1 [00:00<00:00,  3.61it/s]\u001b[A\n",
      "\u001b[36m8bbbkc9zqw-algo-1-7uphg |\u001b[0m Validation DataLoader 0: 100%|██████████| 1/1 [00:00<00:00,  3.61it/s]\u001b[A\n",
      "\u001b[36m8bbbkc9zqw-algo-1-7uphg |\u001b[0m Epoch 0: 100%|██████████| 21/21 [00:06<00:00,  3.25it/s, loss=255, v_num=0, train_loss_step=228.0]\n",
      "\u001b[36m8bbbkc9zqw-algo-1-7uphg |\u001b[0m Epoch 0: 100%|██████████| 21/21 [00:06<00:00,  3.25it/s, loss=255, v_num=0, train_loss_step=228.0]\n",
      "\u001b[36m8bbbkc9zqw-algo-1-7uphg |\u001b[0m Epoch 0: 100%|██████████| 21/21 [00:07<00:00,  2.92it/s, loss=255, v_num=0, train_loss_step=228.0, val_loss=284.0]\n",
      "\u001b[36m8bbbkc9zqw-algo-1-7uphg |\u001b[0m \u001b[A\n",
      "\u001b[36m8bbbkc9zqw-algo-1-7uphg |\u001b[0m Epoch 0: 100%|██████████| 21/21 [00:07<00:00,  2.91it/s, loss=255, v_num=0, train_loss_step=228.0, val_loss=284.0, train_loss_epoch=264.0]\n",
      "\u001b[36m8bbbkc9zqw-algo-1-7uphg |\u001b[0m Epoch 0:   0%|          | 0/21 [00:00<?, ?it/s, loss=255, v_num=0, train_loss_step=228.0, val_loss=284.0, train_loss_epoch=264.0]\n",
      "\u001b[36m8bbbkc9zqw-algo-1-7uphg |\u001b[0m Epoch 1:   0%|          | 0/21 [00:00<?, ?it/s, loss=255, v_num=0, train_loss_step=228.0, val_loss=284.0, train_loss_epoch=264.0]\n",
      "\u001b[36m8bbbkc9zqw-algo-1-7uphg |\u001b[0m Epoch 1:   5%|▍         | 1/21 [00:00<00:04,  4.07it/s, loss=255, v_num=0, train_loss_step=228.0, val_loss=284.0, train_loss_epoch=264.0]\n",
      "\u001b[36m8bbbkc9zqw-algo-1-7uphg |\u001b[0m Epoch 1:   5%|▍         | 1/21 [00:00<00:04,  4.06it/s, loss=255, v_num=0, train_loss_step=228.0, val_loss=284.0, train_loss_epoch=264.0]\n",
      "\u001b[36m8bbbkc9zqw-algo-1-7uphg |\u001b[0m Epoch 1:   5%|▍         | 1/21 [00:00<00:04,  4.06it/s, loss=245, v_num=0, train_loss_step=157.0, val_loss=284.0, train_loss_epoch=264.0]\n",
      "\u001b[36m8bbbkc9zqw-algo-1-7uphg |\u001b[0m Epoch 1:  10%|▉         | 2/21 [00:00<00:04,  3.95it/s, loss=245, v_num=0, train_loss_step=157.0, val_loss=284.0, train_loss_epoch=264.0]\n",
      "\u001b[36m8bbbkc9zqw-algo-1-7uphg |\u001b[0m Epoch 1:  10%|▉         | 2/21 [00:00<00:04,  3.95it/s, loss=245, v_num=0, train_loss_step=157.0, val_loss=284.0, train_loss_epoch=264.0]\n",
      "\u001b[36m8bbbkc9zqw-algo-1-7uphg |\u001b[0m Epoch 1:  10%|▉         | 2/21 [00:00<00:04,  3.94it/s, loss=244, v_num=0, train_loss_step=229.0, val_loss=284.0, train_loss_epoch=264.0]\n",
      "\u001b[36m8bbbkc9zqw-algo-1-7uphg |\u001b[0m Epoch 1:  14%|█▍        | 3/21 [00:00<00:04,  3.91it/s, loss=244, v_num=0, train_loss_step=229.0, val_loss=284.0, train_loss_epoch=264.0]\n",
      "\u001b[36m8bbbkc9zqw-algo-1-7uphg |\u001b[0m Epoch 1:  14%|█▍        | 3/21 [00:00<00:04,  3.91it/s, loss=244, v_num=0, train_loss_step=229.0, val_loss=284.0, train_loss_epoch=264.0]\n",
      "\u001b[36m8bbbkc9zqw-algo-1-7uphg |\u001b[0m Epoch 1:  14%|█▍        | 3/21 [00:00<00:04,  3.91it/s, loss=237, v_num=0, train_loss_step=230.0, val_loss=284.0, train_loss_epoch=264.0]\n",
      "\u001b[36m8bbbkc9zqw-algo-1-7uphg |\u001b[0m Epoch 1:  19%|█▉        | 4/21 [00:01<00:05,  3.38it/s, loss=237, v_num=0, train_loss_step=230.0, val_loss=284.0, train_loss_epoch=264.0]\n",
      "\u001b[36m8bbbkc9zqw-algo-1-7uphg |\u001b[0m Epoch 1:  19%|█▉        | 4/21 [00:01<00:05,  3.38it/s, loss=237, v_num=0, train_loss_step=230.0, val_loss=284.0, train_loss_epoch=264.0]\n",
      "\u001b[36m8bbbkc9zqw-algo-1-7uphg |\u001b[0m Epoch 1:  19%|█▉        | 4/21 [00:01<00:05,  3.38it/s, loss=230, v_num=0, train_loss_step=143.0, val_loss=284.0, train_loss_epoch=264.0]\n",
      "\u001b[36m8bbbkc9zqw-algo-1-7uphg |\u001b[0m Epoch 1:  24%|██▍       | 5/21 [00:01<00:04,  3.32it/s, loss=230, v_num=0, train_loss_step=143.0, val_loss=284.0, train_loss_epoch=264.0]\n",
      "\u001b[36m8bbbkc9zqw-algo-1-7uphg |\u001b[0m Epoch 1:  24%|██▍       | 5/21 [00:01<00:04,  3.32it/s, loss=230, v_num=0, train_loss_step=143.0, val_loss=284.0, train_loss_epoch=264.0]\n",
      "\u001b[36m8bbbkc9zqw-algo-1-7uphg |\u001b[0m Epoch 1:  24%|██▍       | 5/21 [00:01<00:04,  3.32it/s, loss=222, v_num=0, train_loss_step=190.0, val_loss=284.0, train_loss_epoch=264.0]\n",
      "\u001b[36m8bbbkc9zqw-algo-1-7uphg |\u001b[0m Epoch 1:  29%|██▊       | 6/21 [00:01<00:04,  3.39it/s, loss=222, v_num=0, train_loss_step=190.0, val_loss=284.0, train_loss_epoch=264.0]\n",
      "\u001b[36m8bbbkc9zqw-algo-1-7uphg |\u001b[0m Epoch 1:  29%|██▊       | 6/21 [00:01<00:04,  3.39it/s, loss=222, v_num=0, train_loss_step=190.0, val_loss=284.0, train_loss_epoch=264.0]\n",
      "\u001b[36m8bbbkc9zqw-algo-1-7uphg |\u001b[0m Epoch 1:  29%|██▊       | 6/21 [00:01<00:04,  3.39it/s, loss=223, v_num=0, train_loss_step=216.0, val_loss=284.0, train_loss_epoch=264.0]\n",
      "\u001b[36m8bbbkc9zqw-algo-1-7uphg |\u001b[0m Epoch 1:  33%|███▎      | 7/21 [00:02<00:04,  3.44it/s, loss=223, v_num=0, train_loss_step=216.0, val_loss=284.0, train_loss_epoch=264.0]\n",
      "\u001b[36m8bbbkc9zqw-algo-1-7uphg |\u001b[0m Epoch 1:  33%|███▎      | 7/21 [00:02<00:04,  3.44it/s, loss=223, v_num=0, train_loss_step=216.0, val_loss=284.0, train_loss_epoch=264.0]\n",
      "\u001b[36m8bbbkc9zqw-algo-1-7uphg |\u001b[0m Epoch 1:  33%|███▎      | 7/21 [00:02<00:04,  3.44it/s, loss=218, v_num=0, train_loss_step=201.0, val_loss=284.0, train_loss_epoch=264.0]\n",
      "\u001b[36m8bbbkc9zqw-algo-1-7uphg |\u001b[0m Epoch 1:  38%|███▊      | 8/21 [00:02<00:03,  3.43it/s, loss=218, v_num=0, train_loss_step=201.0, val_loss=284.0, train_loss_epoch=264.0]\n",
      "\u001b[36m8bbbkc9zqw-algo-1-7uphg |\u001b[0m Epoch 1:  38%|███▊      | 8/21 [00:02<00:03,  3.43it/s, loss=218, v_num=0, train_loss_step=201.0, val_loss=284.0, train_loss_epoch=264.0]\n",
      "\u001b[36m8bbbkc9zqw-algo-1-7uphg |\u001b[0m Epoch 1:  38%|███▊      | 8/21 [00:02<00:03,  3.43it/s, loss=215, v_num=0, train_loss_step=159.0, val_loss=284.0, train_loss_epoch=264.0]\n",
      "\u001b[36m8bbbkc9zqw-algo-1-7uphg |\u001b[0m Epoch 1:  43%|████▎     | 9/21 [00:02<00:03,  3.45it/s, loss=215, v_num=0, train_loss_step=159.0, val_loss=284.0, train_loss_epoch=264.0]\n",
      "\u001b[36m8bbbkc9zqw-algo-1-7uphg |\u001b[0m Epoch 1:  43%|████▎     | 9/21 [00:02<00:03,  3.45it/s, loss=215, v_num=0, train_loss_step=159.0, val_loss=284.0, train_loss_epoch=264.0]\n",
      "\u001b[36m8bbbkc9zqw-algo-1-7uphg |\u001b[0m Epoch 1:  43%|████▎     | 9/21 [00:02<00:03,  3.45it/s, loss=208, v_num=0, train_loss_step=196.0, val_loss=284.0, train_loss_epoch=264.0]\n",
      "\u001b[36m8bbbkc9zqw-algo-1-7uphg |\u001b[0m Epoch 1:  48%|████▊     | 10/21 [00:02<00:03,  3.46it/s, loss=208, v_num=0, train_loss_step=196.0, val_loss=284.0, train_loss_epoch=264.0]\n",
      "\u001b[36m8bbbkc9zqw-algo-1-7uphg |\u001b[0m Epoch 1:  48%|████▊     | 10/21 [00:02<00:03,  3.46it/s, loss=208, v_num=0, train_loss_step=196.0, val_loss=284.0, train_loss_epoch=264.0]\n",
      "\u001b[36m8bbbkc9zqw-algo-1-7uphg |\u001b[0m Epoch 1:  48%|████▊     | 10/21 [00:02<00:03,  3.46it/s, loss=202, v_num=0, train_loss_step=140.0, val_loss=284.0, train_loss_epoch=264.0]\n",
      "\u001b[36m8bbbkc9zqw-algo-1-7uphg |\u001b[0m Epoch 1:  52%|█████▏    | 11/21 [00:03<00:02,  3.48it/s, loss=202, v_num=0, train_loss_step=140.0, val_loss=284.0, train_loss_epoch=264.0]\n",
      "\u001b[36m8bbbkc9zqw-algo-1-7uphg |\u001b[0m Epoch 1:  52%|█████▏    | 11/21 [00:03<00:02,  3.48it/s, loss=202, v_num=0, train_loss_step=140.0, val_loss=284.0, train_loss_epoch=264.0]\n",
      "\u001b[36m8bbbkc9zqw-algo-1-7uphg |\u001b[0m Epoch 1:  52%|█████▏    | 11/21 [00:03<00:02,  3.48it/s, loss=200, v_num=0, train_loss_step=161.0, val_loss=284.0, train_loss_epoch=264.0]\n",
      "\u001b[36m8bbbkc9zqw-algo-1-7uphg |\u001b[0m Epoch 1:  57%|█████▋    | 12/21 [00:03<00:02,  3.49it/s, loss=200, v_num=0, train_loss_step=161.0, val_loss=284.0, train_loss_epoch=264.0]\n",
      "\u001b[36m8bbbkc9zqw-algo-1-7uphg |\u001b[0m Epoch 1:  57%|█████▋    | 12/21 [00:03<00:02,  3.49it/s, loss=200, v_num=0, train_loss_step=161.0, val_loss=284.0, train_loss_epoch=264.0]\n",
      "\u001b[36m8bbbkc9zqw-algo-1-7uphg |\u001b[0m Epoch 1:  57%|█████▋    | 12/21 [00:03<00:02,  3.49it/s, loss=196, v_num=0, train_loss_step=169.0, val_loss=284.0, train_loss_epoch=264.0]\n",
      "\u001b[36m8bbbkc9zqw-algo-1-7uphg |\u001b[0m Epoch 1:  62%|██████▏   | 13/21 [00:03<00:02,  3.51it/s, loss=196, v_num=0, train_loss_step=169.0, val_loss=284.0, train_loss_epoch=264.0]\n",
      "\u001b[36m8bbbkc9zqw-algo-1-7uphg |\u001b[0m Epoch 1:  62%|██████▏   | 13/21 [00:03<00:02,  3.51it/s, loss=196, v_num=0, train_loss_step=169.0, val_loss=284.0, train_loss_epoch=264.0]\n",
      "\u001b[36m8bbbkc9zqw-algo-1-7uphg |\u001b[0m Epoch 1:  62%|██████▏   | 13/21 [00:03<00:02,  3.51it/s, loss=197, v_num=0, train_loss_step=275.0, val_loss=284.0, train_loss_epoch=264.0]\n",
      "\u001b[36m8bbbkc9zqw-algo-1-7uphg |\u001b[0m Epoch 1:  67%|██████▋   | 14/21 [00:03<00:01,  3.52it/s, loss=197, v_num=0, train_loss_step=275.0, val_loss=284.0, train_loss_epoch=264.0]\n",
      "\u001b[36m8bbbkc9zqw-algo-1-7uphg |\u001b[0m Epoch 1:  67%|██████▋   | 14/21 [00:03<00:01,  3.52it/s, loss=197, v_num=0, train_loss_step=275.0, val_loss=284.0, train_loss_epoch=264.0]\n",
      "\u001b[36m8bbbkc9zqw-algo-1-7uphg |\u001b[0m Epoch 1:  67%|██████▋   | 14/21 [00:03<00:01,  3.52it/s, loss=192, v_num=0, train_loss_step=146.0, val_loss=284.0, train_loss_epoch=264.0]\n",
      "\u001b[36m8bbbkc9zqw-algo-1-7uphg |\u001b[0m Epoch 1:  71%|███████▏  | 15/21 [00:04<00:01,  3.51it/s, loss=192, v_num=0, train_loss_step=146.0, val_loss=284.0, train_loss_epoch=264.0]\n",
      "\u001b[36m8bbbkc9zqw-algo-1-7uphg |\u001b[0m Epoch 1:  71%|███████▏  | 15/21 [00:04<00:01,  3.51it/s, loss=192, v_num=0, train_loss_step=146.0, val_loss=284.0, train_loss_epoch=264.0]\n",
      "\u001b[36m8bbbkc9zqw-algo-1-7uphg |\u001b[0m Epoch 1:  71%|███████▏  | 15/21 [00:04<00:01,  3.51it/s, loss=190, v_num=0, train_loss_step=185.0, val_loss=284.0, train_loss_epoch=264.0]\n",
      "\u001b[36m8bbbkc9zqw-algo-1-7uphg |\u001b[0m Epoch 1:  76%|███████▌  | 16/21 [00:04<00:01,  3.50it/s, loss=190, v_num=0, train_loss_step=185.0, val_loss=284.0, train_loss_epoch=264.0]\n",
      "\u001b[36m8bbbkc9zqw-algo-1-7uphg |\u001b[0m Epoch 1:  76%|███████▌  | 16/21 [00:04<00:01,  3.50it/s, loss=190, v_num=0, train_loss_step=185.0, val_loss=284.0, train_loss_epoch=264.0]\n",
      "\u001b[36m8bbbkc9zqw-algo-1-7uphg |\u001b[0m Epoch 1:  76%|███████▌  | 16/21 [00:04<00:01,  3.50it/s, loss=185, v_num=0, train_loss_step=136.0, val_loss=284.0, train_loss_epoch=264.0]\n",
      "\u001b[36m8bbbkc9zqw-algo-1-7uphg |\u001b[0m Epoch 1:  81%|████████  | 17/21 [00:04<00:01,  3.49it/s, loss=185, v_num=0, train_loss_step=136.0, val_loss=284.0, train_loss_epoch=264.0]\n",
      "\u001b[36m8bbbkc9zqw-algo-1-7uphg |\u001b[0m Epoch 1:  81%|████████  | 17/21 [00:04<00:01,  3.49it/s, loss=185, v_num=0, train_loss_step=136.0, val_loss=284.0, train_loss_epoch=264.0]\n",
      "\u001b[36m8bbbkc9zqw-algo-1-7uphg |\u001b[0m Epoch 1:  81%|████████  | 17/21 [00:04<00:01,  3.49it/s, loss=182, v_num=0, train_loss_step=166.0, val_loss=284.0, train_loss_epoch=264.0]\n",
      "\u001b[36m8bbbkc9zqw-algo-1-7uphg |\u001b[0m Epoch 1:  86%|████████▌ | 18/21 [00:05<00:00,  3.49it/s, loss=182, v_num=0, train_loss_step=166.0, val_loss=284.0, train_loss_epoch=264.0]\n",
      "\u001b[36m8bbbkc9zqw-algo-1-7uphg |\u001b[0m Epoch 1:  86%|████████▌ | 18/21 [00:05<00:00,  3.49it/s, loss=182, v_num=0, train_loss_step=166.0, val_loss=284.0, train_loss_epoch=264.0]\n",
      "\u001b[36m8bbbkc9zqw-algo-1-7uphg |\u001b[0m Epoch 1:  86%|████████▌ | 18/21 [00:05<00:00,  3.49it/s, loss=184, v_num=0, train_loss_step=195.0, val_loss=284.0, train_loss_epoch=264.0]\n",
      "\u001b[36m8bbbkc9zqw-algo-1-7uphg |\u001b[0m Epoch 1:  90%|█████████ | 19/21 [00:05<00:00,  3.48it/s, loss=184, v_num=0, train_loss_step=195.0, val_loss=284.0, train_loss_epoch=264.0]\n",
      "\u001b[36m8bbbkc9zqw-algo-1-7uphg |\u001b[0m Epoch 1:  90%|█████████ | 19/21 [00:05<00:00,  3.48it/s, loss=184, v_num=0, train_loss_step=195.0, val_loss=284.0, train_loss_epoch=264.0]\n",
      "\u001b[36m8bbbkc9zqw-algo-1-7uphg |\u001b[0m Epoch 1:  90%|█████████ | 19/21 [00:05<00:00,  3.48it/s, loss=183, v_num=0, train_loss_step=148.0, val_loss=284.0, train_loss_epoch=264.0]\n",
      "\u001b[36m8bbbkc9zqw-algo-1-7uphg |\u001b[0m Epoch 1:  95%|█████████▌| 20/21 [00:05<00:00,  3.47it/s, loss=183, v_num=0, train_loss_step=148.0, val_loss=284.0, train_loss_epoch=264.0]\n",
      "\u001b[36m8bbbkc9zqw-algo-1-7uphg |\u001b[0m Epoch 1:  95%|█████████▌| 20/21 [00:05<00:00,  3.47it/s, loss=183, v_num=0, train_loss_step=148.0, val_loss=284.0, train_loss_epoch=264.0]\n",
      "\u001b[36m8bbbkc9zqw-algo-1-7uphg |\u001b[0m Epoch 1:  95%|█████████▌| 20/21 [00:05<00:00,  3.47it/s, loss=180, v_num=0, train_loss_step=159.0, val_loss=284.0, train_loss_epoch=264.0]\n",
      "\u001b[36m8bbbkc9zqw-algo-1-7uphg |\u001b[0m \n",
      "\u001b[36m8bbbkc9zqw-algo-1-7uphg |\u001b[0m Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "\u001b[36m8bbbkc9zqw-algo-1-7uphg |\u001b[0m \n",
      "\u001b[36m8bbbkc9zqw-algo-1-7uphg |\u001b[0m Validation:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "\u001b[36m8bbbkc9zqw-algo-1-7uphg |\u001b[0m \n",
      "\u001b[36m8bbbkc9zqw-algo-1-7uphg |\u001b[0m Validation DataLoader 0:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "\u001b[36m8bbbkc9zqw-algo-1-7uphg |\u001b[0m \n",
      "\u001b[36m8bbbkc9zqw-algo-1-7uphg |\u001b[0m Validation DataLoader 0: 100%|██████████| 1/1 [00:00<00:00,  3.21it/s]\u001b[A\n",
      "\u001b[36m8bbbkc9zqw-algo-1-7uphg |\u001b[0m \n",
      "\u001b[36m8bbbkc9zqw-algo-1-7uphg |\u001b[0m Validation DataLoader 0: 100%|██████████| 1/1 [00:00<00:00,  3.21it/s]\u001b[A\n",
      "\u001b[36m8bbbkc9zqw-algo-1-7uphg |\u001b[0m Epoch 1: 100%|██████████| 21/21 [00:06<00:00,  3.43it/s, loss=180, v_num=0, train_loss_step=159.0, val_loss=284.0, train_loss_epoch=264.0]\n",
      "\u001b[36m8bbbkc9zqw-algo-1-7uphg |\u001b[0m Epoch 1: 100%|██████████| 21/21 [00:06<00:00,  3.43it/s, loss=180, v_num=0, train_loss_step=159.0, val_loss=284.0, train_loss_epoch=264.0]\n",
      "\u001b[36m8bbbkc9zqw-algo-1-7uphg |\u001b[0m Epoch 1: 100%|██████████| 21/21 [00:06<00:00,  3.03it/s, loss=180, v_num=0, train_loss_step=159.0, val_loss=218.0, train_loss_epoch=264.0]\n",
      "\u001b[36m8bbbkc9zqw-algo-1-7uphg |\u001b[0m \u001b[A\n",
      "\u001b[36m8bbbkc9zqw-algo-1-7uphg |\u001b[0m Epoch 1: 100%|██████████| 21/21 [00:06<00:00,  3.03it/s, loss=180, v_num=0, train_loss_step=159.0, val_loss=218.0, train_loss_epoch=171.0]\n",
      "\u001b[36m8bbbkc9zqw-algo-1-7uphg |\u001b[0m Epoch 1: 100%|██████████| 21/21 [00:07<00:00,  2.97it/s, loss=180, v_num=0, train_loss_step=159.0, val_loss=218.0, train_loss_epoch=171.0]\n",
      "\u001b[36m8bbbkc9zqw-algo-1-7uphg |\u001b[0m 2023-03-04 14:26:24,267 sagemaker-training-toolkit INFO     Waiting for the process to finish and give a return code.\n",
      "\u001b[36m8bbbkc9zqw-algo-1-7uphg |\u001b[0m 2023-03-04 14:26:24,267 sagemaker-training-toolkit INFO     Done waiting for a return code. Received 0 from exiting process.\n",
      "\u001b[36m8bbbkc9zqw-algo-1-7uphg |\u001b[0m 2023-03-04 14:26:24,267 sagemaker-training-toolkit INFO     Reporting training SUCCESS\n",
      "\u001b[36m8bbbkc9zqw-algo-1-7uphg exited with code 0\n",
      "\u001b[0mAborting on container exit...\n",
      "===== Job Complete =====\n"
     ]
    }
   ],
   "source": [
    "from sagemaker.pytorch import PyTorch\n",
    "import os\n",
    "import subprocess\n",
    "\n",
    "\n",
    "local_estimator = PyTorch(\n",
    "    entry_point=\"TFT_Train.py\",    \n",
    "    source_dir='src',    \n",
    "    role=role,\n",
    "    framework_version='1.12.1',    \n",
    "    py_version='py38',        \n",
    "    instance_count=1,\n",
    "    instance_type=instance_type, # local_gpu or local 지정\n",
    "    session = sagemaker.LocalSession(), # 로컬 세션을 사용합니다.\n",
    "    hyperparameters= hyperparameters               \n",
    "    \n",
    ")\n",
    "local_estimator.fit()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d49513c-1e54-4fae-b1c6-6fe5ff5839a9",
   "metadata": {
    "tags": []
   },
   "source": [
    "# 3. SageMaker Cloud Mode\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5d1bb25-6912-4e62-a46a-e01682f20d8c",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 파라미터 셋업"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a6e47078-88f2-470a-adcc-82ae4c0305a9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "instance_type = 'ml.g4dn.12xlarge' # AMD Radeon Pro V520 4장 GPU\n",
    "\n",
    "hyperparameters = {'epochs': epochs, \n",
    "                    }  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "10515ee2-b28f-49d6-9d8f-d09bdb3f9ecd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sagemaker.pytorch import PyTorch\n",
    "import os\n",
    "\n",
    "estimator = PyTorch(\n",
    "    entry_point=\"TFT_Train.py\",    \n",
    "    source_dir='src',    \n",
    "    role=role,\n",
    "    framework_version='1.12.1',    \n",
    "    py_version='py38',     \n",
    "    instance_count=1,\n",
    "    instance_type=instance_type, # local_gpu or local 지정\n",
    "    session = sagemaker.Session(),\n",
    "    hyperparameters= hyperparameters               \n",
    "    \n",
    ")\n",
    "estimator.fit(wait=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "66e74128-03f0-48d1-b279-29f25d29e559",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-03-04 14:39:33 Starting - Starting the training job...\n",
      "2023-03-04 14:39:57 Starting - Preparing the instances for trainingProfilerReport-1677940773: InProgress\n",
      "......\n",
      "2023-03-04 14:41:03 Downloading - Downloading input data\n",
      "2023-03-04 14:41:03 Training - Downloading the training image..................\n",
      "2023-03-04 14:43:59 Training - Training image download completed. Training in progress.......\u001b[34mbash: cannot set terminal process group (-1): Inappropriate ioctl for device\u001b[0m\n",
      "\u001b[34mbash: no job control in this shell\u001b[0m\n",
      "\u001b[34m2023-03-04 14:44:50,525 sagemaker-training-toolkit INFO     Imported framework sagemaker_pytorch_container.training\u001b[0m\n",
      "\u001b[34m2023-03-04 14:44:50,561 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\u001b[0m\n",
      "\u001b[34m2023-03-04 14:44:50,571 sagemaker_pytorch_container.training INFO     Block until all host DNS lookups succeed.\u001b[0m\n",
      "\u001b[34m2023-03-04 14:44:50,573 sagemaker_pytorch_container.training INFO     Invoking user training script.\u001b[0m\n",
      "\u001b[34m2023-03-04 14:44:50,785 sagemaker-training-toolkit INFO     Installing dependencies from requirements.txt:\u001b[0m\n",
      "\u001b[34m/opt/conda/bin/python3.8 -m pip install -r requirements.txt\u001b[0m\n",
      "\u001b[34mCollecting pytorch-forecasting==0.10.3\u001b[0m\n",
      "\u001b[34mDownloading pytorch_forecasting-0.10.3-py3-none-any.whl (141 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 141.4/141.4 kB 4.6 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mCollecting pytorch-lightning==1.6.3\u001b[0m\n",
      "\u001b[34mDownloading pytorch_lightning-1.6.3-py3-none-any.whl (584 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 584.0/584.0 kB 28.8 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pyarrow==11.0.0 in /opt/conda/lib/python3.8/site-packages (from -r requirements.txt (line 5)) (11.0.0)\u001b[0m\n",
      "\u001b[34mCollecting tensorboard==2.12.0\u001b[0m\n",
      "\u001b[34mDownloading tensorboard-2.12.0-py3-none-any.whl (5.6 MB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 5.6/5.6 MB 97.6 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mCollecting scikit-learn<1.2,>=0.24\u001b[0m\n",
      "\u001b[34mDownloading scikit_learn-1.1.3-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (31.2 MB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 31.2/31.2 MB 64.5 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: torch<2.0,>=1.7 in /opt/conda/lib/python3.8/site-packages (from pytorch-forecasting==0.10.3->-r requirements.txt (line 2)) (1.12.1+cu113)\u001b[0m\n",
      "\u001b[34mCollecting optuna<3.0.0,>=2.3.0\u001b[0m\n",
      "\u001b[34mDownloading optuna-2.10.1-py3-none-any.whl (308 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 308.2/308.2 kB 45.9 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: matplotlib in /opt/conda/lib/python3.8/site-packages (from pytorch-forecasting==0.10.3->-r requirements.txt (line 2)) (3.7.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: scipy<2.0,>=1.8 in /opt/conda/lib/python3.8/site-packages (from pytorch-forecasting==0.10.3->-r requirements.txt (line 2)) (1.10.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pandas<2.0.0,>=1.3.0 in /opt/conda/lib/python3.8/site-packages (from pytorch-forecasting==0.10.3->-r requirements.txt (line 2)) (1.5.3)\u001b[0m\n",
      "\u001b[34mCollecting statsmodels\u001b[0m\n",
      "\u001b[34mDownloading statsmodels-0.13.5-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (9.9 MB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 9.9/9.9 MB 124.9 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: PyYAML>=5.4 in /opt/conda/lib/python3.8/site-packages (from pytorch-lightning==1.6.3->-r requirements.txt (line 4)) (5.4.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: packaging>=17.0 in /opt/conda/lib/python3.8/site-packages (from pytorch-lightning==1.6.3->-r requirements.txt (line 4)) (23.0)\u001b[0m\n",
      "\u001b[34mCollecting torchmetrics>=0.4.1\u001b[0m\n",
      "\u001b[34mDownloading torchmetrics-0.11.3-py3-none-any.whl (518 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 518.6/518.6 kB 76.8 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mCollecting pyDeprecate<0.4.0,>=0.3.1\u001b[0m\n",
      "\u001b[34mDownloading pyDeprecate-0.3.2-py3-none-any.whl (10 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: fsspec[http]!=2021.06.0,>=2021.05.0 in /opt/conda/lib/python3.8/site-packages (from pytorch-lightning==1.6.3->-r requirements.txt (line 4)) (2023.1.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: numpy>=1.17.2 in /opt/conda/lib/python3.8/site-packages (from pytorch-lightning==1.6.3->-r requirements.txt (line 4)) (1.23.5)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: tqdm>=4.57.0 in /opt/conda/lib/python3.8/site-packages (from pytorch-lightning==1.6.3->-r requirements.txt (line 4)) (4.64.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: typing-extensions>=4.0.0 in /opt/conda/lib/python3.8/site-packages (from pytorch-lightning==1.6.3->-r requirements.txt (line 4)) (4.4.0)\u001b[0m\n",
      "\u001b[34mCollecting google-auth<3,>=1.6.3\u001b[0m\n",
      "\u001b[34mDownloading google_auth-2.16.2-py2.py3-none-any.whl (177 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 177.2/177.2 kB 46.4 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mCollecting absl-py>=0.4\u001b[0m\n",
      "\u001b[34mDownloading absl_py-1.4.0-py3-none-any.whl (126 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 126.5/126.5 kB 34.6 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mCollecting tensorboard-data-server<0.8.0,>=0.7.0\u001b[0m\n",
      "\u001b[34mDownloading tensorboard_data_server-0.7.0-py3-none-manylinux2014_x86_64.whl (6.6 MB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 6.6/6.6 MB 134.4 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mCollecting tensorboard-plugin-wit>=1.6.0\u001b[0m\n",
      "\u001b[34mDownloading tensorboard_plugin_wit-1.8.1-py3-none-any.whl (781 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 781.3/781.3 kB 73.5 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mCollecting markdown>=2.6.8\u001b[0m\n",
      "\u001b[34mDownloading Markdown-3.4.1-py3-none-any.whl (93 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 93.3/93.3 kB 21.2 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mCollecting grpcio>=1.48.2\u001b[0m\n",
      "\u001b[34mDownloading grpcio-1.51.3-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.8 MB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 4.8/4.8 MB 130.8 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mCollecting google-auth-oauthlib<0.5,>=0.4.1\u001b[0m\n",
      "\u001b[34mDownloading google_auth_oauthlib-0.4.6-py2.py3-none-any.whl (18 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: protobuf>=3.19.6 in /opt/conda/lib/python3.8/site-packages (from tensorboard==2.12.0->-r requirements.txt (line 6)) (3.20.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: requests<3,>=2.21.0 in /opt/conda/lib/python3.8/site-packages (from tensorboard==2.12.0->-r requirements.txt (line 6)) (2.28.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: werkzeug>=1.0.1 in /opt/conda/lib/python3.8/site-packages (from tensorboard==2.12.0->-r requirements.txt (line 6)) (2.2.3)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: setuptools>=41.0.0 in /opt/conda/lib/python3.8/site-packages (from tensorboard==2.12.0->-r requirements.txt (line 6)) (65.6.3)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: wheel>=0.26 in /opt/conda/lib/python3.8/site-packages (from tensorboard==2.12.0->-r requirements.txt (line 6)) (0.38.4)\u001b[0m\n",
      "\u001b[34mCollecting aiohttp!=4.0.0a0,!=4.0.0a1\u001b[0m\n",
      "\u001b[34mDownloading aiohttp-3.8.4-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.0 MB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1.0/1.0 MB 86.6 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mCollecting pyasn1-modules>=0.2.1\u001b[0m\n",
      "\u001b[34mDownloading pyasn1_modules-0.2.8-py2.py3-none-any.whl (155 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 155.3/155.3 kB 36.1 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: six>=1.9.0 in /opt/conda/lib/python3.8/site-packages (from google-auth<3,>=1.6.3->tensorboard==2.12.0->-r requirements.txt (line 6)) (1.16.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: rsa<5,>=3.1.4 in /opt/conda/lib/python3.8/site-packages (from google-auth<3,>=1.6.3->tensorboard==2.12.0->-r requirements.txt (line 6)) (4.7.2)\u001b[0m\n",
      "\u001b[34mCollecting cachetools<6.0,>=2.0.0\u001b[0m\n",
      "\u001b[34mDownloading cachetools-5.3.0-py3-none-any.whl (9.3 kB)\u001b[0m\n",
      "\u001b[34mCollecting requests-oauthlib>=0.7.0\u001b[0m\n",
      "\u001b[34mDownloading requests_oauthlib-1.3.1-py2.py3-none-any.whl (23 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: importlib-metadata>=4.4 in /opt/conda/lib/python3.8/site-packages (from markdown>=2.6.8->tensorboard==2.12.0->-r requirements.txt (line 6)) (4.13.0)\u001b[0m\n",
      "\u001b[34mCollecting cliff\u001b[0m\n",
      "\u001b[34mDownloading cliff-4.2.0-py3-none-any.whl (81 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 81.0/81.0 kB 16.3 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mCollecting cmaes>=0.8.2\u001b[0m\n",
      "\u001b[34mDownloading cmaes-0.9.1-py3-none-any.whl (21 kB)\u001b[0m\n",
      "\u001b[34mCollecting colorlog\u001b[0m\n",
      "\u001b[34mDownloading colorlog-6.7.0-py2.py3-none-any.whl (11 kB)\u001b[0m\n",
      "\u001b[34mCollecting alembic\u001b[0m\n",
      "\u001b[34mDownloading alembic-1.9.4-py3-none-any.whl (210 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 210.5/210.5 kB 44.4 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mCollecting sqlalchemy>=1.1.0\u001b[0m\n",
      "\u001b[34mDownloading SQLAlchemy-2.0.4-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.8 MB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 2.8/2.8 MB 120.4 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: python-dateutil>=2.8.1 in /opt/conda/lib/python3.8/site-packages (from pandas<2.0.0,>=1.3.0->pytorch-forecasting==0.10.3->-r requirements.txt (line 2)) (2.8.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.8/site-packages (from pandas<2.0.0,>=1.3.0->pytorch-forecasting==0.10.3->-r requirements.txt (line 2)) (2022.7.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.8/site-packages (from requests<3,>=2.21.0->tensorboard==2.12.0->-r requirements.txt (line 6)) (1.26.14)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.8/site-packages (from requests<3,>=2.21.0->tensorboard==2.12.0->-r requirements.txt (line 6)) (3.4)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.8/site-packages (from requests<3,>=2.21.0->tensorboard==2.12.0->-r requirements.txt (line 6)) (2022.12.7)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.8/site-packages (from requests<3,>=2.21.0->tensorboard==2.12.0->-r requirements.txt (line 6)) (2.1.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: threadpoolctl>=2.0.0 in /opt/conda/lib/python3.8/site-packages (from scikit-learn<1.2,>=0.24->pytorch-forecasting==0.10.3->-r requirements.txt (line 2)) (3.1.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: joblib>=1.0.0 in /opt/conda/lib/python3.8/site-packages (from scikit-learn<1.2,>=0.24->pytorch-forecasting==0.10.3->-r requirements.txt (line 2)) (1.2.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: MarkupSafe>=2.1.1 in /opt/conda/lib/python3.8/site-packages (from werkzeug>=1.0.1->tensorboard==2.12.0->-r requirements.txt (line 6)) (2.1.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pillow>=6.2.0 in /opt/conda/lib/python3.8/site-packages (from matplotlib->pytorch-forecasting==0.10.3->-r requirements.txt (line 2)) (9.4.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: contourpy>=1.0.1 in /opt/conda/lib/python3.8/site-packages (from matplotlib->pytorch-forecasting==0.10.3->-r requirements.txt (line 2)) (1.0.7)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pyparsing>=2.3.1 in /opt/conda/lib/python3.8/site-packages (from matplotlib->pytorch-forecasting==0.10.3->-r requirements.txt (line 2)) (3.0.9)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: kiwisolver>=1.0.1 in /opt/conda/lib/python3.8/site-packages (from matplotlib->pytorch-forecasting==0.10.3->-r requirements.txt (line 2)) (1.4.4)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: fonttools>=4.22.0 in /opt/conda/lib/python3.8/site-packages (from matplotlib->pytorch-forecasting==0.10.3->-r requirements.txt (line 2)) (4.38.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: cycler>=0.10 in /opt/conda/lib/python3.8/site-packages (from matplotlib->pytorch-forecasting==0.10.3->-r requirements.txt (line 2)) (0.11.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: importlib-resources>=3.2.0 in /opt/conda/lib/python3.8/site-packages (from matplotlib->pytorch-forecasting==0.10.3->-r requirements.txt (line 2)) (5.10.2)\u001b[0m\n",
      "\u001b[34mCollecting patsy>=0.5.2\u001b[0m\n",
      "\u001b[34mDownloading patsy-0.5.3-py2.py3-none-any.whl (233 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 233.8/233.8 kB 46.0 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mCollecting aiosignal>=1.1.2\u001b[0m\n",
      "\u001b[34mDownloading aiosignal-1.3.1-py3-none-any.whl (7.6 kB)\u001b[0m\n",
      "\u001b[34mCollecting multidict<7.0,>=4.5\u001b[0m\n",
      "\u001b[34mDownloading multidict-6.0.4-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (121 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 121.3/121.3 kB 31.7 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mCollecting yarl<2.0,>=1.0\u001b[0m\n",
      "\u001b[34mDownloading yarl-1.8.2-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (262 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 262.1/262.1 kB 50.1 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mCollecting frozenlist>=1.1.1\u001b[0m\n",
      "\u001b[34mDownloading frozenlist-1.3.3-cp38-cp38-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (161 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 161.3/161.3 kB 41.0 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.8/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning==1.6.3->-r requirements.txt (line 4)) (22.2.0)\u001b[0m\n",
      "\u001b[34mCollecting async-timeout<5.0,>=4.0.0a3\u001b[0m\n",
      "\u001b[34mDownloading async_timeout-4.0.2-py3-none-any.whl (5.8 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.8/site-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard==2.12.0->-r requirements.txt (line 6)) (3.13.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /opt/conda/lib/python3.8/site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard==2.12.0->-r requirements.txt (line 6)) (0.4.8)\u001b[0m\n",
      "\u001b[34mCollecting oauthlib>=3.0.0\u001b[0m\n",
      "\u001b[34mDownloading oauthlib-3.2.2-py3-none-any.whl (151 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 151.7/151.7 kB 44.5 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: greenlet!=0.4.17 in /opt/conda/lib/python3.8/site-packages (from sqlalchemy>=1.1.0->optuna<3.0.0,>=2.3.0->pytorch-forecasting==0.10.3->-r requirements.txt (line 2)) (2.0.2)\u001b[0m\n",
      "\u001b[34mCollecting Mako\u001b[0m\n",
      "\u001b[34mDownloading Mako-1.2.4-py3-none-any.whl (78 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 78.7/78.7 kB 19.8 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mCollecting autopage>=0.4.0\u001b[0m\n",
      "\u001b[34mDownloading autopage-0.5.1-py3-none-any.whl (29 kB)\u001b[0m\n",
      "\u001b[34mCollecting PrettyTable>=0.7.2\u001b[0m\n",
      "\u001b[34mDownloading prettytable-3.6.0-py3-none-any.whl (27 kB)\u001b[0m\n",
      "\u001b[34mCollecting stevedore>=2.0.1\u001b[0m\n",
      "\u001b[34mDownloading stevedore-5.0.0-py3-none-any.whl (49 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 49.6/49.6 kB 17.0 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mCollecting cmd2>=1.0.0\u001b[0m\n",
      "\u001b[34mDownloading cmd2-2.4.3-py3-none-any.whl (147 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 147.2/147.2 kB 42.0 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mCollecting pyperclip>=1.6\u001b[0m\n",
      "\u001b[34mDownloading pyperclip-1.8.2.tar.gz (20 kB)\u001b[0m\n",
      "\u001b[34mPreparing metadata (setup.py): started\u001b[0m\n",
      "\u001b[34mPreparing metadata (setup.py): finished with status 'done'\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: wcwidth>=0.1.7 in /opt/conda/lib/python3.8/site-packages (from cmd2>=1.0.0->cliff->optuna<3.0.0,>=2.3.0->pytorch-forecasting==0.10.3->-r requirements.txt (line 2)) (0.2.6)\u001b[0m\n",
      "\u001b[34mCollecting pbr!=2.1.0,>=2.0.0\u001b[0m\n",
      "\u001b[34mDownloading pbr-5.11.1-py2.py3-none-any.whl (112 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 112.7/112.7 kB 21.0 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mBuilding wheels for collected packages: pyperclip\u001b[0m\n",
      "\u001b[34mBuilding wheel for pyperclip (setup.py): started\u001b[0m\n",
      "\u001b[34mBuilding wheel for pyperclip (setup.py): finished with status 'done'\u001b[0m\n",
      "\u001b[34mCreated wheel for pyperclip: filename=pyperclip-1.8.2-py3-none-any.whl size=11124 sha256=e3830b3813132fcecd9258fb95517765a049289cdd12020b793da74e40b95182\u001b[0m\n",
      "\u001b[34mStored in directory: /root/.cache/pip/wheels/7f/1a/65/84ff8c386bec21fca6d220ea1f5498a0367883a78dd5ba6122\u001b[0m\n",
      "\u001b[34mSuccessfully built pyperclip\u001b[0m\n",
      "\u001b[34mInstalling collected packages: tensorboard-plugin-wit, pyperclip, tensorboard-data-server, sqlalchemy, pyDeprecate, pyasn1-modules, PrettyTable, pbr, patsy, oauthlib, multidict, Mako, grpcio, frozenlist, colorlog, cmd2, cmaes, cachetools, autopage, async-timeout, absl-py, yarl, torchmetrics, stevedore, scikit-learn, requests-oauthlib, markdown, google-auth, alembic, aiosignal, statsmodels, google-auth-oauthlib, cliff, aiohttp, tensorboard, optuna, pytorch-lightning, pytorch-forecasting\u001b[0m\n",
      "\u001b[34mAttempting uninstall: scikit-learn\u001b[0m\n",
      "\u001b[34mFound existing installation: scikit-learn 1.2.1\u001b[0m\n",
      "\u001b[34mUninstalling scikit-learn-1.2.1:\u001b[0m\n",
      "\u001b[34mSuccessfully uninstalled scikit-learn-1.2.1\u001b[0m\n",
      "\u001b[34mSuccessfully installed Mako-1.2.4 PrettyTable-3.6.0 absl-py-1.4.0 aiohttp-3.8.4 aiosignal-1.3.1 alembic-1.9.4 async-timeout-4.0.2 autopage-0.5.1 cachetools-5.3.0 cliff-4.2.0 cmaes-0.9.1 cmd2-2.4.3 colorlog-6.7.0 frozenlist-1.3.3 google-auth-2.16.2 google-auth-oauthlib-0.4.6 grpcio-1.51.3 markdown-3.4.1 multidict-6.0.4 oauthlib-3.2.2 optuna-2.10.1 patsy-0.5.3 pbr-5.11.1 pyDeprecate-0.3.2 pyasn1-modules-0.2.8 pyperclip-1.8.2 pytorch-forecasting-0.10.3 pytorch-lightning-1.6.3 requests-oauthlib-1.3.1 scikit-learn-1.1.3 sqlalchemy-2.0.4 statsmodels-0.13.5 stevedore-5.0.0 tensorboard-2.12.0 tensorboard-data-server-0.7.0 tensorboard-plugin-wit-1.8.1 torchmetrics-0.11.3 yarl-1.8.2\u001b[0m\n",
      "\u001b[34mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n",
      "\u001b[34m[notice] A new release of pip is available: 23.0 -> 23.0.1\u001b[0m\n",
      "\u001b[34m[notice] To update, run: pip install --upgrade pip\u001b[0m\n",
      "\u001b[34m2023-03-04 14:45:06,237 sagemaker-training-toolkit INFO     Waiting for the process to finish and give a return code.\u001b[0m\n",
      "\u001b[34m2023-03-04 14:45:06,237 sagemaker-training-toolkit INFO     Done waiting for a return code. Received 0 from exiting process.\u001b[0m\n",
      "\u001b[34m2023-03-04 14:45:06,276 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\u001b[0m\n",
      "\u001b[34m2023-03-04 14:45:06,322 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\u001b[0m\n",
      "\u001b[34m2023-03-04 14:45:06,370 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\u001b[0m\n",
      "\u001b[34m2023-03-04 14:45:06,380 sagemaker-training-toolkit INFO     Invoking user script\u001b[0m\n",
      "\u001b[34mTraining Env:\u001b[0m\n",
      "\u001b[34m{\n",
      "    \"additional_framework_parameters\": {},\n",
      "    \"channel_input_dirs\": {},\n",
      "    \"current_host\": \"algo-1\",\n",
      "    \"current_instance_group\": \"homogeneousCluster\",\n",
      "    \"current_instance_group_hosts\": [\n",
      "        \"algo-1\"\n",
      "    ],\n",
      "    \"current_instance_type\": \"ml.g4dn.12xlarge\",\n",
      "    \"distribution_hosts\": [],\n",
      "    \"distribution_instance_groups\": [],\n",
      "    \"framework_module\": \"sagemaker_pytorch_container.training:main\",\n",
      "    \"hosts\": [\n",
      "        \"algo-1\"\n",
      "    ],\n",
      "    \"hyperparameters\": {\n",
      "        \"epochs\": 2\n",
      "    },\n",
      "    \"input_config_dir\": \"/opt/ml/input/config\",\n",
      "    \"input_data_config\": {},\n",
      "    \"input_dir\": \"/opt/ml/input\",\n",
      "    \"instance_groups\": [\n",
      "        \"homogeneousCluster\"\n",
      "    ],\n",
      "    \"instance_groups_dict\": {\n",
      "        \"homogeneousCluster\": {\n",
      "            \"instance_group_name\": \"homogeneousCluster\",\n",
      "            \"instance_type\": \"ml.g4dn.12xlarge\",\n",
      "            \"hosts\": [\n",
      "                \"algo-1\"\n",
      "            ]\n",
      "        }\n",
      "    },\n",
      "    \"is_hetero\": false,\n",
      "    \"is_master\": true,\n",
      "    \"is_modelparallel_enabled\": null,\n",
      "    \"is_smddpmprun_installed\": true,\n",
      "    \"job_name\": \"pytorch-training-2023-03-04-14-39-32-806\",\n",
      "    \"log_level\": 20,\n",
      "    \"master_hostname\": \"algo-1\",\n",
      "    \"model_dir\": \"/opt/ml/model\",\n",
      "    \"module_dir\": \"s3://sagemaker-us-east-1-057716757052/pytorch-training-2023-03-04-14-39-32-806/source/sourcedir.tar.gz\",\n",
      "    \"module_name\": \"TFT_Train\",\n",
      "    \"network_interface_name\": \"eth0\",\n",
      "    \"num_cpus\": 48,\n",
      "    \"num_gpus\": 4,\n",
      "    \"num_neurons\": 0,\n",
      "    \"output_data_dir\": \"/opt/ml/output/data\",\n",
      "    \"output_dir\": \"/opt/ml/output\",\n",
      "    \"output_intermediate_dir\": \"/opt/ml/output/intermediate\",\n",
      "    \"resource_config\": {\n",
      "        \"current_host\": \"algo-1\",\n",
      "        \"current_instance_type\": \"ml.g4dn.12xlarge\",\n",
      "        \"current_group_name\": \"homogeneousCluster\",\n",
      "        \"hosts\": [\n",
      "            \"algo-1\"\n",
      "        ],\n",
      "        \"instance_groups\": [\n",
      "            {\n",
      "                \"instance_group_name\": \"homogeneousCluster\",\n",
      "                \"instance_type\": \"ml.g4dn.12xlarge\",\n",
      "                \"hosts\": [\n",
      "                    \"algo-1\"\n",
      "                ]\n",
      "            }\n",
      "        ],\n",
      "        \"network_interface_name\": \"eth0\"\n",
      "    },\n",
      "    \"user_entry_point\": \"TFT_Train.py\"\u001b[0m\n",
      "\u001b[34m}\u001b[0m\n",
      "\u001b[34mEnvironment variables:\u001b[0m\n",
      "\u001b[34mSM_HOSTS=[\"algo-1\"]\u001b[0m\n",
      "\u001b[34mSM_NETWORK_INTERFACE_NAME=eth0\u001b[0m\n",
      "\u001b[34mSM_HPS={\"epochs\":2}\u001b[0m\n",
      "\u001b[34mSM_USER_ENTRY_POINT=TFT_Train.py\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_PARAMS={}\u001b[0m\n",
      "\u001b[34mSM_RESOURCE_CONFIG={\"current_group_name\":\"homogeneousCluster\",\"current_host\":\"algo-1\",\"current_instance_type\":\"ml.g4dn.12xlarge\",\"hosts\":[\"algo-1\"],\"instance_groups\":[{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.g4dn.12xlarge\"}],\"network_interface_name\":\"eth0\"}\u001b[0m\n",
      "\u001b[34mSM_INPUT_DATA_CONFIG={}\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DATA_DIR=/opt/ml/output/data\u001b[0m\n",
      "\u001b[34mSM_CHANNELS=[]\u001b[0m\n",
      "\u001b[34mSM_CURRENT_HOST=algo-1\u001b[0m\n",
      "\u001b[34mSM_CURRENT_INSTANCE_TYPE=ml.g4dn.12xlarge\u001b[0m\n",
      "\u001b[34mSM_CURRENT_INSTANCE_GROUP=homogeneousCluster\u001b[0m\n",
      "\u001b[34mSM_CURRENT_INSTANCE_GROUP_HOSTS=[\"algo-1\"]\u001b[0m\n",
      "\u001b[34mSM_INSTANCE_GROUPS=[\"homogeneousCluster\"]\u001b[0m\n",
      "\u001b[34mSM_INSTANCE_GROUPS_DICT={\"homogeneousCluster\":{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.g4dn.12xlarge\"}}\u001b[0m\n",
      "\u001b[34mSM_DISTRIBUTION_INSTANCE_GROUPS=[]\u001b[0m\n",
      "\u001b[34mSM_IS_HETERO=false\u001b[0m\n",
      "\u001b[34mSM_MODULE_NAME=TFT_Train\u001b[0m\n",
      "\u001b[34mSM_LOG_LEVEL=20\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_MODULE=sagemaker_pytorch_container.training:main\u001b[0m\n",
      "\u001b[34mSM_INPUT_DIR=/opt/ml/input\u001b[0m\n",
      "\u001b[34mSM_INPUT_CONFIG_DIR=/opt/ml/input/config\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DIR=/opt/ml/output\u001b[0m\n",
      "\u001b[34mSM_NUM_CPUS=48\u001b[0m\n",
      "\u001b[34mSM_NUM_GPUS=4\u001b[0m\n",
      "\u001b[34mSM_NUM_NEURONS=0\u001b[0m\n",
      "\u001b[34mSM_MODEL_DIR=/opt/ml/model\u001b[0m\n",
      "\u001b[34mSM_MODULE_DIR=s3://sagemaker-us-east-1-057716757052/pytorch-training-2023-03-04-14-39-32-806/source/sourcedir.tar.gz\u001b[0m\n",
      "\u001b[34mSM_TRAINING_ENV={\"additional_framework_parameters\":{},\"channel_input_dirs\":{},\"current_host\":\"algo-1\",\"current_instance_group\":\"homogeneousCluster\",\"current_instance_group_hosts\":[\"algo-1\"],\"current_instance_type\":\"ml.g4dn.12xlarge\",\"distribution_hosts\":[],\"distribution_instance_groups\":[],\"framework_module\":\"sagemaker_pytorch_container.training:main\",\"hosts\":[\"algo-1\"],\"hyperparameters\":{\"epochs\":2},\"input_config_dir\":\"/opt/ml/input/config\",\"input_data_config\":{},\"input_dir\":\"/opt/ml/input\",\"instance_groups\":[\"homogeneousCluster\"],\"instance_groups_dict\":{\"homogeneousCluster\":{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.g4dn.12xlarge\"}},\"is_hetero\":false,\"is_master\":true,\"is_modelparallel_enabled\":null,\"is_smddpmprun_installed\":true,\"job_name\":\"pytorch-training-2023-03-04-14-39-32-806\",\"log_level\":20,\"master_hostname\":\"algo-1\",\"model_dir\":\"/opt/ml/model\",\"module_dir\":\"s3://sagemaker-us-east-1-057716757052/pytorch-training-2023-03-04-14-39-32-806/source/sourcedir.tar.gz\",\"module_name\":\"TFT_Train\",\"network_interface_name\":\"eth0\",\"num_cpus\":48,\"num_gpus\":4,\"num_neurons\":0,\"output_data_dir\":\"/opt/ml/output/data\",\"output_dir\":\"/opt/ml/output\",\"output_intermediate_dir\":\"/opt/ml/output/intermediate\",\"resource_config\":{\"current_group_name\":\"homogeneousCluster\",\"current_host\":\"algo-1\",\"current_instance_type\":\"ml.g4dn.12xlarge\",\"hosts\":[\"algo-1\"],\"instance_groups\":[{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.g4dn.12xlarge\"}],\"network_interface_name\":\"eth0\"},\"user_entry_point\":\"TFT_Train.py\"}\u001b[0m\n",
      "\u001b[34mSM_USER_ARGS=[\"--epochs\",\"2\"]\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_INTERMEDIATE_DIR=/opt/ml/output/intermediate\u001b[0m\n",
      "\u001b[34mSM_HP_EPOCHS=2\u001b[0m\n",
      "\u001b[34mPYTHONPATH=/opt/ml/code:/opt/conda/bin:/opt/conda/lib/python38.zip:/opt/conda/lib/python3.8:/opt/conda/lib/python3.8/lib-dynload:/opt/conda/lib/python3.8/site-packages:/opt/conda/lib/python3.8/site-packages/smdebug-1.0.24b20230214-py3.8.egg:/opt/conda/lib/python3.8/site-packages/pyinstrument-3.4.2-py3.8.egg:/opt/conda/lib/python3.8/site-packages/pyinstrument_cext-0.2.4-py3.8-linux-x86_64.egg:/opt/conda/lib/python3.8/site-packages/flash_attn-0.1-py3.8-linux-x86_64.egg:/opt/conda/lib/python3.8/site-packages/einops-0.6.0-py3.8.egg\u001b[0m\n",
      "\u001b[34mInvoking script with the following command:\u001b[0m\n",
      "\u001b[34m/opt/conda/bin/python3.8 TFT_Train.py --epochs 2\u001b[0m\n",
      "\u001b[34m2023-03-04 14:45:08,777 sagemaker-training-toolkit INFO     Exceptions not imported for SageMaker TF as Tensorflow is not installed.\u001b[0m\n",
      "\u001b[34mNot running on notebook\u001b[0m\n",
      "\u001b[34m***** Arguments *****\u001b[0m\n",
      "\u001b[34mepochs=2\u001b[0m\n",
      "\u001b[34mseed=100\u001b[0m\n",
      "\u001b[34mtrain_batch_size=64\u001b[0m\n",
      "\u001b[34mmodel_dir=/opt/ml/model\u001b[0m\n",
      "\u001b[34mn_gpus=4\u001b[0m\n",
      "\u001b[34mGPU available: True, used: True\u001b[0m\n",
      "\u001b[34mTPU available: False, using: 0 TPU cores\u001b[0m\n",
      "\u001b[34mIPU available: False, using: 0 IPUs\u001b[0m\n",
      "\u001b[34mHPU available: False, using: 0 HPUs\u001b[0m\n",
      "\u001b[34mNumber of parameters in network: 29.7k\u001b[0m\n",
      "\u001b[34mInitializing distributed: GLOBAL_RANK: 0, MEMBER: 1/4\u001b[0m\n",
      "\u001b[34mInitializing distributed: GLOBAL_RANK: 1, MEMBER: 2/4\u001b[0m\n",
      "\u001b[34mInitializing distributed: GLOBAL_RANK: 2, MEMBER: 3/4\u001b[0m\n",
      "\u001b[34mInitializing distributed: GLOBAL_RANK: 3, MEMBER: 4/4\u001b[0m\n",
      "\u001b[34m----------------------------------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[34mdistributed_backend=nccl\u001b[0m\n",
      "\u001b[34mAll distributed processes registered. Starting with 4 processes\u001b[0m\n",
      "\u001b[34m----------------------------------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[34mMissing logger folder: lightning_logs/lightning_logs\u001b[0m\n",
      "\u001b[34mMissing logger folder: lightning_logs/lightning_logs\u001b[0m\n",
      "\u001b[34mMissing logger folder: lightning_logs/lightning_logs\u001b[0m\n",
      "\u001b[34mMissing logger folder: lightning_logs/lightning_logs\u001b[0m\n",
      "\u001b[34mNCCL version 2.10.3+cuda11.3\u001b[0m\n",
      "\u001b[34mLOCAL_RANK: 2 - CUDA_VISIBLE_DEVICES: [0,1,2,3]\u001b[0m\n",
      "\u001b[34mLOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3]\u001b[0m\n",
      "\u001b[34mLOCAL_RANK: 3 - CUDA_VISIBLE_DEVICES: [0,1,2,3]\u001b[0m\n",
      "\u001b[34mLOCAL_RANK: 1 - CUDA_VISIBLE_DEVICES: [0,1,2,3]\u001b[0m\n",
      "\u001b[34m| Name                               | Type                            | Params\u001b[0m\n",
      "\u001b[34m----------------------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[34m0  | loss                               | QuantileLoss                    | 0     \u001b[0m\n",
      "\u001b[34m1  | logging_metrics                    | ModuleList                      | 0     \u001b[0m\n",
      "\u001b[34m2  | input_embeddings                   | MultiEmbedding                  | 1.3 K \u001b[0m\n",
      "\u001b[34m3  | prescalers                         | ModuleDict                      | 256   \u001b[0m\n",
      "\u001b[34m4  | static_variable_selection          | VariableSelectionNetwork        | 3.4 K \u001b[0m\n",
      "\u001b[34m5  | encoder_variable_selection         | VariableSelectionNetwork        | 8.0 K \u001b[0m\n",
      "\u001b[34m6  | decoder_variable_selection         | VariableSelectionNetwork        | 2.7 K \u001b[0m\n",
      "\u001b[34m7  | static_context_variable_selection  | GatedResidualNetwork            | 1.1 K \u001b[0m\n",
      "\u001b[34m8  | static_context_initial_hidden_lstm | GatedResidualNetwork            | 1.1 K \u001b[0m\n",
      "\u001b[34m9  | static_context_initial_cell_lstm   | GatedResidualNetwork            | 1.1 K \u001b[0m\n",
      "\u001b[34m10 | static_context_enrichment          | GatedResidualNetwork            | 1.1 K \u001b[0m\n",
      "\u001b[34m11 | lstm_encoder                       | LSTM                            | 2.2 K \u001b[0m\n",
      "\u001b[34m12 | lstm_decoder                       | LSTM                            | 2.2 K \u001b[0m\n",
      "\u001b[34m13 | post_lstm_gate_encoder             | GatedLinearUnit                 | 544   \u001b[0m\n",
      "\u001b[34m14 | post_lstm_add_norm_encoder         | AddNorm                         | 32    \u001b[0m\n",
      "\u001b[34m15 | static_enrichment                  | GatedResidualNetwork            | 1.4 K \u001b[0m\n",
      "\u001b[34m16 | multihead_attn                     | InterpretableMultiHeadAttention | 1.1 K \u001b[0m\n",
      "\u001b[34m17 | post_attn_gate_norm                | GateAddNorm                     | 576   \u001b[0m\n",
      "\u001b[34m18 | pos_wise_ff                        | GatedResidualNetwork            | 1.1 K \u001b[0m\n",
      "\u001b[34m19 | pre_output_gate_norm               | GateAddNorm                     | 576   \u001b[0m\n",
      "\u001b[34m20 | output_layer                       | Linear                          | 119   \u001b[0m\n",
      "\u001b[34m----------------------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[34m29.7 K    Trainable params\u001b[0m\n",
      "\u001b[34m0         Non-trainable params\u001b[0m\n",
      "\u001b[34m29.7 K    Total params\u001b[0m\n",
      "\u001b[34m0.119     Total estimated model params size (MB)\u001b[0m\n",
      "\u001b[34mSanity Checking: 0it [00:00, ?it/s]\u001b[0m\n",
      "\u001b[34m[2023-03-04 14:45:31.271 algo-1:361 INFO utils.py:28] RULE_JOB_STOP_SIGNAL_FILENAME: None\u001b[0m\n",
      "\u001b[34m[2023-03-04 14:45:31.272 algo-1:302 INFO utils.py:28] RULE_JOB_STOP_SIGNAL_FILENAME: None\u001b[0m\n",
      "\u001b[34m[2023-03-04 14:45:31.272 algo-1:420 INFO utils.py:28] RULE_JOB_STOP_SIGNAL_FILENAME: None\u001b[0m\n",
      "\u001b[34m[2023-03-04 14:45:31.273 algo-1:245 INFO utils.py:28] RULE_JOB_STOP_SIGNAL_FILENAME: None\u001b[0m\n",
      "\u001b[34m[2023-03-04 14:45:31.401 algo-1:361 INFO profiler_config_parser.py:111] User has disabled profiler.\u001b[0m\n",
      "\u001b[34m[2023-03-04 14:45:31.402 algo-1:361 INFO json_config.py:92] Creating hook from json_config at /opt/ml/input/config/debughookconfig.json.\u001b[0m\n",
      "\u001b[34m[2023-03-04 14:45:31.402 algo-1:420 INFO profiler_config_parser.py:111] User has disabled profiler.\u001b[0m\n",
      "\u001b[34m[2023-03-04 14:45:31.402 algo-1:302 INFO profiler_config_parser.py:111] User has disabled profiler.\u001b[0m\n",
      "\u001b[34m[2023-03-04 14:45:31.402 algo-1:361 INFO hook.py:206] tensorboard_dir has not been set for the hook. SMDebug will not be exporting tensorboard summaries.\u001b[0m\n",
      "\u001b[34m[2023-03-04 14:45:31.403 algo-1:361 INFO hook.py:259] Saving to /opt/ml/output/tensors\u001b[0m\n",
      "\u001b[34m[2023-03-04 14:45:31.403 algo-1:361 INFO state_store.py:77] The checkpoint config file /opt/ml/input/config/checkpointconfig.json does not exist.\u001b[0m\n",
      "\u001b[34m[2023-03-04 14:45:31.403 algo-1:420 INFO json_config.py:92] Creating hook from json_config at /opt/ml/input/config/debughookconfig.json.\u001b[0m\n",
      "\u001b[34m[2023-03-04 14:45:31.403 algo-1:302 INFO json_config.py:92] Creating hook from json_config at /opt/ml/input/config/debughookconfig.json.\u001b[0m\n",
      "\u001b[34m[2023-03-04 14:45:31.404 algo-1:420 INFO hook.py:206] tensorboard_dir has not been set for the hook. SMDebug will not be exporting tensorboard summaries.\u001b[0m\n",
      "\u001b[34m[2023-03-04 14:45:31.404 algo-1:245 INFO profiler_config_parser.py:111] User has disabled profiler.\u001b[0m\n",
      "\u001b[34m[2023-03-04 14:45:31.404 algo-1:302 INFO hook.py:206] tensorboard_dir has not been set for the hook. SMDebug will not be exporting tensorboard summaries.\u001b[0m\n",
      "\u001b[34m[2023-03-04 14:45:31.404 algo-1:420 INFO hook.py:259] Saving to /opt/ml/output/tensors\u001b[0m\n",
      "\u001b[34m[2023-03-04 14:45:31.404 algo-1:420 INFO state_store.py:77] The checkpoint config file /opt/ml/input/config/checkpointconfig.json does not exist.\u001b[0m\n",
      "\u001b[34m[2023-03-04 14:45:31.404 algo-1:302 INFO hook.py:259] Saving to /opt/ml/output/tensors\u001b[0m\n",
      "\u001b[34m[2023-03-04 14:45:31.404 algo-1:302 INFO state_store.py:77] The checkpoint config file /opt/ml/input/config/checkpointconfig.json does not exist.\u001b[0m\n",
      "\u001b[34m[2023-03-04 14:45:31.405 algo-1:245 INFO json_config.py:92] Creating hook from json_config at /opt/ml/input/config/debughookconfig.json.\u001b[0m\n",
      "\u001b[34m[2023-03-04 14:45:31.405 algo-1:245 INFO hook.py:206] tensorboard_dir has not been set for the hook. SMDebug will not be exporting tensorboard summaries.\u001b[0m\n",
      "\u001b[34m[2023-03-04 14:45:31.406 algo-1:245 INFO hook.py:259] Saving to /opt/ml/output/tensors\u001b[0m\n",
      "\u001b[34m[2023-03-04 14:45:31.406 algo-1:245 INFO state_store.py:77] The checkpoint config file /opt/ml/input/config/checkpointconfig.json does not exist.\u001b[0m\n",
      "\u001b[34mSanity Checking:   0%|          | 0/1 [00:00<?, ?it/s]#015Sanity Checking DataLoader 0:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34mSanity Checking DataLoader 0: 100%|██████████| 1/1 [00:00<00:00,  1.84it/s]\u001b[0m\n",
      "\u001b[34mSanity Checking DataLoader 0: 100%|██████████| 1/1 [00:00<00:00,  1.84it/s]\u001b[0m\n",
      "\u001b[34mTraining: 0it [00:00, ?it/s]\u001b[0m\n",
      "\u001b[34mTraining:   0%|          | 0/41 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34mEpoch 0:   0%|          | 0/41 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34m[W reducer.cpp:1251] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())\u001b[0m\n",
      "\u001b[34m[W reducer.cpp:1251] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())\u001b[0m\n",
      "\u001b[34m[W reducer.cpp:1251] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())\u001b[0m\n",
      "\u001b[34m[W reducer.cpp:1251] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())\u001b[0m\n",
      "\u001b[34mEpoch 0:   2%|▏         | 1/41 [00:00<00:13,  3.07it/s]\u001b[0m\n",
      "\u001b[34mEpoch 0:   2%|▏         | 1/41 [00:00<00:13,  3.07it/s]\u001b[0m\n",
      "\u001b[34mEpoch 0:   2%|▏         | 1/41 [00:00<00:13,  3.07it/s, loss=349, v_num=0, train_loss_step=349.0]\u001b[0m\n",
      "\u001b[34mEpoch 0:   5%|▍         | 2/41 [00:00<00:09,  4.05it/s, loss=349, v_num=0, train_loss_step=349.0]\u001b[0m\n",
      "\u001b[34mEpoch 0:   5%|▍         | 2/41 [00:00<00:09,  4.05it/s, loss=349, v_num=0, train_loss_step=349.0]\u001b[0m\n",
      "\u001b[34mEpoch 0:   5%|▍         | 2/41 [00:00<00:09,  4.04it/s, loss=357, v_num=0, train_loss_step=365.0]\u001b[0m\n",
      "\u001b[34mEpoch 0:   7%|▋         | 3/41 [00:00<00:08,  4.50it/s, loss=357, v_num=0, train_loss_step=365.0]#015Epoch 0:   7%|▋         | 3/41 [00:00<00:08,  4.50it/s, loss=357, v_num=0, train_loss_step=365.0]\u001b[0m\n",
      "\u001b[34mEpoch 0:   7%|▋         | 3/41 [00:00<00:08,  4.49it/s, loss=368, v_num=0, train_loss_step=391.0]\u001b[0m\n",
      "\u001b[34mEpoch 0:  10%|▉         | 4/41 [00:00<00:07,  4.79it/s, loss=368, v_num=0, train_loss_step=391.0]\u001b[0m\n",
      "\u001b[34mEpoch 0:  10%|▉         | 4/41 [00:00<00:07,  4.79it/s, loss=368, v_num=0, train_loss_step=391.0]\u001b[0m\n",
      "\u001b[34mEpoch 0:  10%|▉         | 4/41 [00:00<00:07,  4.79it/s, loss=339, v_num=0, train_loss_step=250.0]\u001b[0m\n",
      "\u001b[34mEpoch 0:  12%|█▏        | 5/41 [00:01<00:07,  4.99it/s, loss=339, v_num=0, train_loss_step=250.0]\u001b[0m\n",
      "\u001b[34mEpoch 0:  12%|█▏        | 5/41 [00:01<00:07,  4.99it/s, loss=339, v_num=0, train_loss_step=250.0]\u001b[0m\n",
      "\u001b[34mEpoch 0:  12%|█▏        | 5/41 [00:01<00:07,  4.99it/s, loss=353, v_num=0, train_loss_step=408.0]\u001b[0m\n",
      "\u001b[34mEpoch 0:  15%|█▍        | 6/41 [00:01<00:06,  5.07it/s, loss=353, v_num=0, train_loss_step=408.0]#015Epoch 0:  15%|█▍        | 6/41 [00:01<00:06,  5.06it/s, loss=353, v_num=0, train_loss_step=408.0]\u001b[0m\n",
      "\u001b[34mEpoch 0:  15%|█▍        | 6/41 [00:01<00:06,  5.06it/s, loss=347, v_num=0, train_loss_step=321.0]\u001b[0m\n",
      "\u001b[34mEpoch 0:  17%|█▋        | 7/41 [00:01<00:06,  5.15it/s, loss=347, v_num=0, train_loss_step=321.0]#015Epoch 0:  17%|█▋        | 7/41 [00:01<00:06,  5.14it/s, loss=347, v_num=0, train_loss_step=321.0]\u001b[0m\n",
      "\u001b[34mEpoch 0:  17%|█▋        | 7/41 [00:01<00:06,  5.14it/s, loss=337, v_num=0, train_loss_step=273.0]\u001b[0m\n",
      "\u001b[34mEpoch 0:  20%|█▉        | 8/41 [00:01<00:06,  5.22it/s, loss=337, v_num=0, train_loss_step=273.0]\u001b[0m\n",
      "\u001b[34mEpoch 0:  20%|█▉        | 8/41 [00:01<00:06,  5.22it/s, loss=337, v_num=0, train_loss_step=273.0]\u001b[0m\n",
      "\u001b[34mEpoch 0:  20%|█▉        | 8/41 [00:01<00:06,  5.22it/s, loss=332, v_num=0, train_loss_step=301.0]\u001b[0m\n",
      "\u001b[34mEpoch 0:  22%|██▏       | 9/41 [00:01<00:06,  5.27it/s, loss=332, v_num=0, train_loss_step=301.0]\u001b[0m\n",
      "\u001b[34mEpoch 0:  22%|██▏       | 9/41 [00:01<00:06,  5.27it/s, loss=332, v_num=0, train_loss_step=301.0]\u001b[0m\n",
      "\u001b[34mEpoch 0:  22%|██▏       | 9/41 [00:01<00:06,  5.27it/s, loss=338, v_num=0, train_loss_step=388.0]\u001b[0m\n",
      "\u001b[34mEpoch 0:  24%|██▍       | 10/41 [00:01<00:05,  5.32it/s, loss=338, v_num=0, train_loss_step=388.0]\u001b[0m\n",
      "\u001b[34mEpoch 0:  24%|██▍       | 10/41 [00:01<00:05,  5.31it/s, loss=338, v_num=0, train_loss_step=388.0]\u001b[0m\n",
      "\u001b[34mEpoch 0:  24%|██▍       | 10/41 [00:01<00:05,  5.31it/s, loss=333, v_num=0, train_loss_step=282.0]\u001b[0m\n",
      "\u001b[34mEpoch 0:  27%|██▋       | 11/41 [00:02<00:05,  5.35it/s, loss=333, v_num=0, train_loss_step=282.0]#015Epoch 0:  27%|██▋       | 11/41 [00:02<00:05,  5.35it/s, loss=333, v_num=0, train_loss_step=282.0]\u001b[0m\n",
      "\u001b[34mEpoch 0:  27%|██▋       | 11/41 [00:02<00:05,  5.35it/s, loss=329, v_num=0, train_loss_step=289.0]\u001b[0m\n",
      "\u001b[34mEpoch 0:  29%|██▉       | 12/41 [00:02<00:05,  5.36it/s, loss=329, v_num=0, train_loss_step=289.0]#015Epoch 0:  29%|██▉       | 12/41 [00:02<00:05,  5.36it/s, loss=329, v_num=0, train_loss_step=289.0]\u001b[0m\n",
      "\u001b[34mEpoch 0:  29%|██▉       | 12/41 [00:02<00:05,  5.36it/s, loss=317, v_num=0, train_loss_step=190.0]\u001b[0m\n",
      "\u001b[34mEpoch 0:  32%|███▏      | 13/41 [00:02<00:05,  5.39it/s, loss=317, v_num=0, train_loss_step=190.0]\u001b[0m\n",
      "\u001b[34mEpoch 0:  32%|███▏      | 13/41 [00:02<00:05,  5.39it/s, loss=317, v_num=0, train_loss_step=190.0]\u001b[0m\n",
      "\u001b[34mEpoch 0:  32%|███▏      | 13/41 [00:02<00:05,  5.39it/s, loss=312, v_num=0, train_loss_step=249.0]\u001b[0m\n",
      "\u001b[34mEpoch 0:  34%|███▍      | 14/41 [00:02<00:04,  5.41it/s, loss=312, v_num=0, train_loss_step=249.0]\u001b[0m\n",
      "\u001b[34mEpoch 0:  34%|███▍      | 14/41 [00:02<00:04,  5.41it/s, loss=312, v_num=0, train_loss_step=249.0]\u001b[0m\n",
      "\u001b[34mEpoch 0:  34%|███▍      | 14/41 [00:02<00:04,  5.41it/s, loss=309, v_num=0, train_loss_step=274.0]\u001b[0m\n",
      "\u001b[34mEpoch 0:  37%|███▋      | 15/41 [00:02<00:04,  5.43it/s, loss=309, v_num=0, train_loss_step=274.0]#015Epoch 0:  37%|███▋      | 15/41 [00:02<00:04,  5.43it/s, loss=309, v_num=0, train_loss_step=274.0]\u001b[0m\n",
      "\u001b[34mEpoch 0:  37%|███▋      | 15/41 [00:02<00:04,  5.43it/s, loss=302, v_num=0, train_loss_step=193.0]\u001b[0m\n",
      "\u001b[34mEpoch 0:  39%|███▉      | 16/41 [00:02<00:04,  5.45it/s, loss=302, v_num=0, train_loss_step=193.0]#015Epoch 0:  39%|███▉      | 16/41 [00:02<00:04,  5.45it/s, loss=302, v_num=0, train_loss_step=193.0]\u001b[0m\n",
      "\u001b[34mEpoch 0:  39%|███▉      | 16/41 [00:02<00:04,  5.45it/s, loss=297, v_num=0, train_loss_step=225.0]\u001b[0m\n",
      "\u001b[34mEpoch 0:  41%|████▏     | 17/41 [00:03<00:04,  5.45it/s, loss=297, v_num=0, train_loss_step=225.0]\u001b[0m\n",
      "\u001b[34mEpoch 0:  41%|████▏     | 17/41 [00:03<00:04,  5.45it/s, loss=297, v_num=0, train_loss_step=225.0]\u001b[0m\n",
      "\u001b[34mEpoch 0:  41%|████▏     | 17/41 [00:03<00:04,  5.45it/s, loss=296, v_num=0, train_loss_step=276.0]\u001b[0m\n",
      "\u001b[34mEpoch 0:  44%|████▍     | 18/41 [00:03<00:04,  5.46it/s, loss=296, v_num=0, train_loss_step=276.0]\u001b[0m\n",
      "\u001b[34mEpoch 0:  44%|████▍     | 18/41 [00:03<00:04,  5.46it/s, loss=296, v_num=0, train_loss_step=276.0]\u001b[0m\n",
      "\u001b[34mEpoch 0:  44%|████▍     | 18/41 [00:03<00:04,  5.45it/s, loss=293, v_num=0, train_loss_step=247.0]\u001b[0m\n",
      "\u001b[34mEpoch 0:  46%|████▋     | 19/41 [00:03<00:04,  5.48it/s, loss=293, v_num=0, train_loss_step=247.0]\u001b[0m\n",
      "\u001b[34mEpoch 0:  46%|████▋     | 19/41 [00:03<00:04,  5.48it/s, loss=293, v_num=0, train_loss_step=247.0]\u001b[0m\n",
      "\u001b[34mEpoch 0:  46%|████▋     | 19/41 [00:03<00:04,  5.48it/s, loss=289, v_num=0, train_loss_step=215.0]\u001b[0m\n",
      "\u001b[34mEpoch 0:  49%|████▉     | 20/41 [00:03<00:03,  5.49it/s, loss=289, v_num=0, train_loss_step=215.0]#015Epoch 0:  49%|████▉     | 20/41 [00:03<00:03,  5.49it/s, loss=289, v_num=0, train_loss_step=215.0]\u001b[0m\n",
      "\u001b[34mEpoch 0:  49%|████▉     | 20/41 [00:03<00:03,  5.49it/s, loss=284, v_num=0, train_loss_step=194.0]\u001b[0m\n",
      "\u001b[34mEpoch 0:  51%|█████     | 21/41 [00:03<00:03,  5.50it/s, loss=284, v_num=0, train_loss_step=194.0]#015Epoch 0:  51%|█████     | 21/41 [00:03<00:03,  5.50it/s, loss=284, v_num=0, train_loss_step=194.0]\u001b[0m\n",
      "\u001b[34mEpoch 0:  51%|█████     | 21/41 [00:03<00:03,  5.50it/s, loss=277, v_num=0, train_loss_step=207.0]\u001b[0m\n",
      "\u001b[34mEpoch 0:  54%|█████▎    | 22/41 [00:03<00:03,  5.51it/s, loss=277, v_num=0, train_loss_step=207.0]\u001b[0m\n",
      "\u001b[34mEpoch 0:  54%|█████▎    | 22/41 [00:03<00:03,  5.51it/s, loss=277, v_num=0, train_loss_step=207.0]\u001b[0m\n",
      "\u001b[34mEpoch 0:  54%|█████▎    | 22/41 [00:03<00:03,  5.51it/s, loss=269, v_num=0, train_loss_step=206.0]\u001b[0m\n",
      "\u001b[34mEpoch 0:  56%|█████▌    | 23/41 [00:04<00:03,  5.52it/s, loss=269, v_num=0, train_loss_step=206.0]\u001b[0m\n",
      "\u001b[34mEpoch 0:  56%|█████▌    | 23/41 [00:04<00:03,  5.52it/s, loss=269, v_num=0, train_loss_step=206.0]\u001b[0m\n",
      "\u001b[34mEpoch 0:  56%|█████▌    | 23/41 [00:04<00:03,  5.52it/s, loss=261, v_num=0, train_loss_step=232.0]\u001b[0m\n",
      "\u001b[34mEpoch 0:  59%|█████▊    | 24/41 [00:04<00:03,  5.52it/s, loss=261, v_num=0, train_loss_step=232.0]\u001b[0m\n",
      "\u001b[34mEpoch 0:  59%|█████▊    | 24/41 [00:04<00:03,  5.52it/s, loss=261, v_num=0, train_loss_step=232.0]\u001b[0m\n",
      "\u001b[34mEpoch 0:  59%|█████▊    | 24/41 [00:04<00:03,  5.52it/s, loss=260, v_num=0, train_loss_step=234.0]\u001b[0m\n",
      "\u001b[34mEpoch 0:  61%|██████    | 25/41 [00:04<00:02,  5.53it/s, loss=260, v_num=0, train_loss_step=234.0]#015Epoch 0:  61%|██████    | 25/41 [00:04<00:02,  5.53it/s, loss=260, v_num=0, train_loss_step=234.0]\u001b[0m\n",
      "\u001b[34mEpoch 0:  61%|██████    | 25/41 [00:04<00:02,  5.53it/s, loss=252, v_num=0, train_loss_step=240.0]\u001b[0m\n",
      "\u001b[34mEpoch 0:  63%|██████▎   | 26/41 [00:04<00:02,  5.54it/s, loss=252, v_num=0, train_loss_step=240.0]\u001b[0m\n",
      "\u001b[34mEpoch 0:  63%|██████▎   | 26/41 [00:04<00:02,  5.54it/s, loss=252, v_num=0, train_loss_step=240.0]\u001b[0m\n",
      "\u001b[34mEpoch 0:  63%|██████▎   | 26/41 [00:04<00:02,  5.54it/s, loss=245, v_num=0, train_loss_step=194.0]\u001b[0m\n",
      "\u001b[34mEpoch 0:  66%|██████▌   | 27/41 [00:04<00:02,  5.55it/s, loss=245, v_num=0, train_loss_step=194.0]#015Epoch 0:  66%|██████▌   | 27/41 [00:04<00:02,  5.55it/s, loss=245, v_num=0, train_loss_step=194.0]\u001b[0m\n",
      "\u001b[34mEpoch 0:  66%|██████▌   | 27/41 [00:04<00:02,  5.55it/s, loss=242, v_num=0, train_loss_step=213.0]\u001b[0m\n",
      "\u001b[34mEpoch 0:  68%|██████▊   | 28/41 [00:05<00:02,  5.56it/s, loss=242, v_num=0, train_loss_step=213.0]#015Epoch 0:  68%|██████▊   | 28/41 [00:05<00:02,  5.56it/s, loss=242, v_num=0, train_loss_step=213.0]\u001b[0m\n",
      "\u001b[34mEpoch 0:  68%|██████▊   | 28/41 [00:05<00:02,  5.56it/s, loss=238, v_num=0, train_loss_step=205.0]\u001b[0m\n",
      "\u001b[34mEpoch 0:  71%|███████   | 29/41 [00:05<00:02,  5.56it/s, loss=238, v_num=0, train_loss_step=205.0]#015Epoch 0:  71%|███████   | 29/41 [00:05<00:02,  5.56it/s, loss=238, v_num=0, train_loss_step=205.0]\u001b[0m\n",
      "\u001b[34mEpoch 0:  71%|███████   | 29/41 [00:05<00:02,  5.56it/s, loss=227, v_num=0, train_loss_step=167.0]\u001b[0m\n",
      "\u001b[34mEpoch 0:  73%|███████▎  | 30/41 [00:05<00:01,  5.56it/s, loss=227, v_num=0, train_loss_step=167.0]\u001b[0m\n",
      "\u001b[34mEpoch 0:  73%|███████▎  | 30/41 [00:05<00:01,  5.56it/s, loss=227, v_num=0, train_loss_step=167.0]\u001b[0m\n",
      "\u001b[34mEpoch 0:  73%|███████▎  | 30/41 [00:05<00:01,  5.56it/s, loss=220, v_num=0, train_loss_step=148.0]\u001b[0m\n",
      "\u001b[34mEpoch 0:  76%|███████▌  | 31/41 [00:05<00:01,  5.57it/s, loss=220, v_num=0, train_loss_step=148.0]\u001b[0m\n",
      "\u001b[34mEpoch 0:  76%|███████▌  | 31/41 [00:05<00:01,  5.57it/s, loss=220, v_num=0, train_loss_step=148.0]\u001b[0m\n",
      "\u001b[34mEpoch 0:  76%|███████▌  | 31/41 [00:05<00:01,  5.57it/s, loss=215, v_num=0, train_loss_step=189.0]\u001b[0m\n",
      "\u001b[34mEpoch 0:  78%|███████▊  | 32/41 [00:05<00:01,  5.57it/s, loss=215, v_num=0, train_loss_step=189.0]#015Epoch 0:  78%|███████▊  | 32/41 [00:05<00:01,  5.57it/s, loss=215, v_num=0, train_loss_step=189.0]\u001b[0m\n",
      "\u001b[34mEpoch 0:  78%|███████▊  | 32/41 [00:05<00:01,  5.57it/s, loss=214, v_num=0, train_loss_step=170.0]\u001b[0m\n",
      "\u001b[34mEpoch 0:  80%|████████  | 33/41 [00:05<00:01,  5.58it/s, loss=214, v_num=0, train_loss_step=170.0]#015Epoch 0:  80%|████████  | 33/41 [00:05<00:01,  5.58it/s, loss=214, v_num=0, train_loss_step=170.0]\u001b[0m\n",
      "\u001b[34mEpoch 0:  80%|████████  | 33/41 [00:05<00:01,  5.58it/s, loss=210, v_num=0, train_loss_step=179.0]\u001b[0m\n",
      "\u001b[34mEpoch 0:  83%|████████▎ | 34/41 [00:06<00:01,  5.58it/s, loss=210, v_num=0, train_loss_step=179.0]\u001b[0m\n",
      "\u001b[34mEpoch 0:  83%|████████▎ | 34/41 [00:06<00:01,  5.58it/s, loss=210, v_num=0, train_loss_step=179.0]\u001b[0m\n",
      "\u001b[34mEpoch 0:  83%|████████▎ | 34/41 [00:06<00:01,  5.58it/s, loss=204, v_num=0, train_loss_step=148.0]\u001b[0m\n",
      "\u001b[34mEpoch 0:  85%|████████▌ | 35/41 [00:06<00:01,  5.59it/s, loss=204, v_num=0, train_loss_step=148.0]#015Epoch 0:  85%|████████▌ | 35/41 [00:06<00:01,  5.59it/s, loss=204, v_num=0, train_loss_step=148.0]\u001b[0m\n",
      "\u001b[34mEpoch 0:  85%|████████▌ | 35/41 [00:06<00:01,  5.59it/s, loss=202, v_num=0, train_loss_step=159.0]\u001b[0m\n",
      "\u001b[34mEpoch 0:  88%|████████▊ | 36/41 [00:06<00:00,  5.58it/s, loss=202, v_num=0, train_loss_step=159.0]#015Epoch 0:  88%|████████▊ | 36/41 [00:06<00:00,  5.58it/s, loss=202, v_num=0, train_loss_step=159.0]\u001b[0m\n",
      "\u001b[34mEpoch 0:  88%|████████▊ | 36/41 [00:06<00:00,  5.58it/s, loss=200, v_num=0, train_loss_step=177.0]\u001b[0m\n",
      "\u001b[34mEpoch 0:  90%|█████████ | 37/41 [00:06<00:00,  5.59it/s, loss=200, v_num=0, train_loss_step=177.0]\u001b[0m\n",
      "\u001b[34mEpoch 0:  90%|█████████ | 37/41 [00:06<00:00,  5.59it/s, loss=200, v_num=0, train_loss_step=177.0]\u001b[0m\n",
      "\u001b[34mEpoch 0:  90%|█████████ | 37/41 [00:06<00:00,  5.59it/s, loss=193, v_num=0, train_loss_step=130.0]\u001b[0m\n",
      "\u001b[34mEpoch 0:  93%|█████████▎| 38/41 [00:06<00:00,  5.59it/s, loss=193, v_num=0, train_loss_step=130.0]#015Epoch 0:  93%|█████████▎| 38/41 [00:06<00:00,  5.59it/s, loss=193, v_num=0, train_loss_step=130.0]\u001b[0m\n",
      "\u001b[34mEpoch 0:  93%|█████████▎| 38/41 [00:06<00:00,  5.59it/s, loss=189, v_num=0, train_loss_step=181.0]\u001b[0m\n",
      "\u001b[34mEpoch 0:  95%|█████████▌| 39/41 [00:06<00:00,  5.60it/s, loss=189, v_num=0, train_loss_step=181.0]\u001b[0m\n",
      "\u001b[34mEpoch 0:  95%|█████████▌| 39/41 [00:06<00:00,  5.60it/s, loss=189, v_num=0, train_loss_step=181.0]\u001b[0m\n",
      "\u001b[34mEpoch 0:  95%|█████████▌| 39/41 [00:06<00:00,  5.60it/s, loss=187, v_num=0, train_loss_step=176.0]\u001b[0m\n",
      "\u001b[34mEpoch 0:  98%|█████████▊| 40/41 [00:07<00:00,  5.60it/s, loss=187, v_num=0, train_loss_step=176.0]\u001b[0m\n",
      "\u001b[34mEpoch 0:  98%|█████████▊| 40/41 [00:07<00:00,  5.60it/s, loss=187, v_num=0, train_loss_step=176.0]\u001b[0m\n",
      "\u001b[34mEpoch 0:  98%|█████████▊| 40/41 [00:07<00:00,  5.60it/s, loss=185, v_num=0, train_loss_step=144.0]\u001b[0m\n",
      "\u001b[34mValidation: 0it [00:00, ?it/s]#033[A\u001b[0m\n",
      "\u001b[34mValidation:   0%|          | 0/1 [00:00<?, ?it/s]#033[A\u001b[0m\n",
      "\u001b[34mValidation DataLoader 0:   0%|          | 0/1 [00:00<?, ?it/s]#033[A\u001b[0m\n",
      "\u001b[34mValidation DataLoader 0: 100%|██████████| 1/1 [00:00<00:00,  3.42it/s]#033[A\u001b[0m\n",
      "\u001b[34mValidation DataLoader 0: 100%|██████████| 1/1 [00:00<00:00,  3.42it/s]#033[A\u001b[0m\n",
      "\u001b[34mEpoch 0: 100%|██████████| 41/41 [00:07<00:00,  5.47it/s, loss=185, v_num=0, train_loss_step=144.0]\u001b[0m\n",
      "\u001b[34mEpoch 0: 100%|██████████| 41/41 [00:07<00:00,  5.47it/s, loss=185, v_num=0, train_loss_step=144.0]\u001b[0m\n",
      "\u001b[34mEpoch 0: 100%|██████████| 41/41 [00:08<00:00,  5.08it/s, loss=185, v_num=0, train_loss_step=144.0, val_loss=219.0]\u001b[0m\n",
      "\u001b[34m#015                                                                      #033[A\u001b[0m\n",
      "\u001b[34mEpoch 0: 100%|██████████| 41/41 [00:08<00:00,  5.08it/s, loss=185, v_num=0, train_loss_step=144.0, val_loss=219.0, train_loss_epoch=232.0]\u001b[0m\n",
      "\u001b[34mEpoch 0:   0%|          | 0/41 [00:00<?, ?it/s, loss=185, v_num=0, train_loss_step=144.0, val_loss=219.0, train_loss_epoch=232.0]\u001b[0m\n",
      "\u001b[34mEpoch 1:   0%|          | 0/41 [00:00<?, ?it/s, loss=185, v_num=0, train_loss_step=144.0, val_loss=219.0, train_loss_epoch=232.0]\u001b[0m\n",
      "\u001b[34mEpoch 1:   2%|▏         | 1/41 [00:00<00:06,  5.91it/s, loss=185, v_num=0, train_loss_step=144.0, val_loss=219.0, train_loss_epoch=232.0]\u001b[0m\n",
      "\u001b[34mEpoch 1:   2%|▏         | 1/41 [00:00<00:06,  5.90it/s, loss=185, v_num=0, train_loss_step=144.0, val_loss=219.0, train_loss_epoch=232.0]\u001b[0m\n",
      "\u001b[34mEpoch 1:   2%|▏         | 1/41 [00:00<00:06,  5.89it/s, loss=182, v_num=0, train_loss_step=139.0, val_loss=219.0, train_loss_epoch=232.0]\u001b[0m\n",
      "\u001b[34mEpoch 1:   5%|▍         | 2/41 [00:00<00:06,  5.65it/s, loss=182, v_num=0, train_loss_step=139.0, val_loss=219.0, train_loss_epoch=232.0]\u001b[0m\n",
      "\u001b[34mEpoch 1:   5%|▍         | 2/41 [00:00<00:06,  5.65it/s, loss=182, v_num=0, train_loss_step=139.0, val_loss=219.0, train_loss_epoch=232.0]\u001b[0m\n",
      "\u001b[34mEpoch 1:   5%|▍         | 2/41 [00:00<00:06,  5.64it/s, loss=177, v_num=0, train_loss_step=113.0, val_loss=219.0, train_loss_epoch=232.0]\u001b[0m\n",
      "\u001b[34mEpoch 1:   7%|▋         | 3/41 [00:00<00:06,  5.68it/s, loss=177, v_num=0, train_loss_step=113.0, val_loss=219.0, train_loss_epoch=232.0]\u001b[0m\n",
      "\u001b[34mEpoch 1:   7%|▋         | 3/41 [00:00<00:06,  5.68it/s, loss=177, v_num=0, train_loss_step=113.0, val_loss=219.0, train_loss_epoch=232.0]\u001b[0m\n",
      "\u001b[34mEpoch 1:   7%|▋         | 3/41 [00:00<00:06,  5.68it/s, loss=173, v_num=0, train_loss_step=157.0, val_loss=219.0, train_loss_epoch=232.0]\u001b[0m\n",
      "\u001b[34mEpoch 1:  10%|▉         | 4/41 [00:00<00:06,  5.71it/s, loss=173, v_num=0, train_loss_step=157.0, val_loss=219.0, train_loss_epoch=232.0]\u001b[0m\n",
      "\u001b[34mEpoch 1:  10%|▉         | 4/41 [00:00<00:06,  5.70it/s, loss=173, v_num=0, train_loss_step=157.0, val_loss=219.0, train_loss_epoch=232.0]\u001b[0m\n",
      "\u001b[34mEpoch 1:  10%|▉         | 4/41 [00:00<00:06,  5.70it/s, loss=170, v_num=0, train_loss_step=161.0, val_loss=219.0, train_loss_epoch=232.0]\u001b[0m\n",
      "\u001b[34mEpoch 1:  12%|█▏        | 5/41 [00:00<00:06,  5.71it/s, loss=170, v_num=0, train_loss_step=161.0, val_loss=219.0, train_loss_epoch=232.0]#015Epoch 1:  12%|█▏        | 5/41 [00:00<00:06,  5.71it/s, loss=170, v_num=0, train_loss_step=161.0, val_loss=219.0, train_loss_epoch=232.0]\u001b[0m\n",
      "\u001b[34mEpoch 1:  12%|█▏        | 5/41 [00:00<00:06,  5.71it/s, loss=168, v_num=0, train_loss_step=207.0, val_loss=219.0, train_loss_epoch=232.0]\u001b[0m\n",
      "\u001b[34mEpoch 1:  15%|█▍        | 6/41 [00:01<00:06,  5.72it/s, loss=168, v_num=0, train_loss_step=207.0, val_loss=219.0, train_loss_epoch=232.0]\u001b[0m\n",
      "\u001b[34mEpoch 1:  15%|█▍        | 6/41 [00:01<00:06,  5.72it/s, loss=168, v_num=0, train_loss_step=207.0, val_loss=219.0, train_loss_epoch=232.0]\u001b[0m\n",
      "\u001b[34mEpoch 1:  15%|█▍        | 6/41 [00:01<00:06,  5.72it/s, loss=165, v_num=0, train_loss_step=132.0, val_loss=219.0, train_loss_epoch=232.0]\u001b[0m\n",
      "\u001b[34mEpoch 1:  17%|█▋        | 7/41 [00:01<00:05,  5.72it/s, loss=165, v_num=0, train_loss_step=132.0, val_loss=219.0, train_loss_epoch=232.0]\u001b[0m\n",
      "\u001b[34mEpoch 1:  17%|█▋        | 7/41 [00:01<00:05,  5.72it/s, loss=165, v_num=0, train_loss_step=132.0, val_loss=219.0, train_loss_epoch=232.0]\u001b[0m\n",
      "\u001b[34mEpoch 1:  17%|█▋        | 7/41 [00:01<00:05,  5.72it/s, loss=160, v_num=0, train_loss_step=116.0, val_loss=219.0, train_loss_epoch=232.0]\u001b[0m\n",
      "\u001b[34mEpoch 1:  20%|█▉        | 8/41 [00:01<00:05,  5.68it/s, loss=160, v_num=0, train_loss_step=116.0, val_loss=219.0, train_loss_epoch=232.0]\u001b[0m\n",
      "\u001b[34mEpoch 1:  20%|█▉        | 8/41 [00:01<00:05,  5.68it/s, loss=160, v_num=0, train_loss_step=116.0, val_loss=219.0, train_loss_epoch=232.0]\u001b[0m\n",
      "\u001b[34mEpoch 1:  20%|█▉        | 8/41 [00:01<00:05,  5.68it/s, loss=156, v_num=0, train_loss_step=137.0, val_loss=219.0, train_loss_epoch=232.0]\u001b[0m\n",
      "\u001b[34mEpoch 1:  22%|██▏       | 9/41 [00:01<00:05,  5.69it/s, loss=156, v_num=0, train_loss_step=137.0, val_loss=219.0, train_loss_epoch=232.0]#015Epoch 1:  22%|██▏       | 9/41 [00:01<00:05,  5.68it/s, loss=156, v_num=0, train_loss_step=137.0, val_loss=219.0, train_loss_epoch=232.0]\u001b[0m\n",
      "\u001b[34mEpoch 1:  22%|██▏       | 9/41 [00:01<00:05,  5.68it/s, loss=156, v_num=0, train_loss_step=164.0, val_loss=219.0, train_loss_epoch=232.0]\u001b[0m\n",
      "\u001b[34mEpoch 1:  24%|██▍       | 10/41 [00:01<00:05,  5.69it/s, loss=156, v_num=0, train_loss_step=164.0, val_loss=219.0, train_loss_epoch=232.0]\u001b[0m\n",
      "\u001b[34mEpoch 1:  24%|██▍       | 10/41 [00:01<00:05,  5.69it/s, loss=156, v_num=0, train_loss_step=164.0, val_loss=219.0, train_loss_epoch=232.0]\u001b[0m\n",
      "\u001b[34mEpoch 1:  24%|██▍       | 10/41 [00:01<00:05,  5.69it/s, loss=155, v_num=0, train_loss_step=128.0, val_loss=219.0, train_loss_epoch=232.0]\u001b[0m\n",
      "\u001b[34mEpoch 1:  27%|██▋       | 11/41 [00:01<00:05,  5.70it/s, loss=155, v_num=0, train_loss_step=128.0, val_loss=219.0, train_loss_epoch=232.0]#015Epoch 1:  27%|██▋       | 11/41 [00:01<00:05,  5.69it/s, loss=155, v_num=0, train_loss_step=128.0, val_loss=219.0, train_loss_epoch=232.0]\u001b[0m\n",
      "\u001b[34mEpoch 1:  27%|██▋       | 11/41 [00:01<00:05,  5.69it/s, loss=155, v_num=0, train_loss_step=172.0, val_loss=219.0, train_loss_epoch=232.0]\u001b[0m\n",
      "\u001b[34mEpoch 1:  29%|██▉       | 12/41 [00:02<00:05,  5.70it/s, loss=155, v_num=0, train_loss_step=172.0, val_loss=219.0, train_loss_epoch=232.0]\u001b[0m\n",
      "\u001b[34mEpoch 1:  29%|██▉       | 12/41 [00:02<00:05,  5.70it/s, loss=155, v_num=0, train_loss_step=172.0, val_loss=219.0, train_loss_epoch=232.0]\u001b[0m\n",
      "\u001b[34mEpoch 1:  29%|██▉       | 12/41 [00:02<00:05,  5.70it/s, loss=155, v_num=0, train_loss_step=183.0, val_loss=219.0, train_loss_epoch=232.0]\u001b[0m\n",
      "\u001b[34mEpoch 1:  32%|███▏      | 13/41 [00:02<00:04,  5.70it/s, loss=155, v_num=0, train_loss_step=183.0, val_loss=219.0, train_loss_epoch=232.0]#015Epoch 1:  32%|███▏      | 13/41 [00:02<00:04,  5.70it/s, loss=155, v_num=0, train_loss_step=183.0, val_loss=219.0, train_loss_epoch=232.0]\u001b[0m\n",
      "\u001b[34mEpoch 1:  32%|███▏      | 13/41 [00:02<00:04,  5.70it/s, loss=155, v_num=0, train_loss_step=169.0, val_loss=219.0, train_loss_epoch=232.0]\u001b[0m\n",
      "\u001b[34mEpoch 1:  34%|███▍      | 14/41 [00:02<00:04,  5.68it/s, loss=155, v_num=0, train_loss_step=169.0, val_loss=219.0, train_loss_epoch=232.0]#015Epoch 1:  34%|███▍      | 14/41 [00:02<00:04,  5.68it/s, loss=155, v_num=0, train_loss_step=169.0, val_loss=219.0, train_loss_epoch=232.0]\u001b[0m\n",
      "\u001b[34mEpoch 1:  34%|███▍      | 14/41 [00:02<00:04,  5.68it/s, loss=154, v_num=0, train_loss_step=136.0, val_loss=219.0, train_loss_epoch=232.0]\u001b[0m\n",
      "\u001b[34mEpoch 1:  37%|███▋      | 15/41 [00:02<00:04,  5.68it/s, loss=154, v_num=0, train_loss_step=136.0, val_loss=219.0, train_loss_epoch=232.0]#015Epoch 1:  37%|███▋      | 15/41 [00:02<00:04,  5.68it/s, loss=154, v_num=0, train_loss_step=136.0, val_loss=219.0, train_loss_epoch=232.0]\u001b[0m\n",
      "\u001b[34mEpoch 1:  37%|███▋      | 15/41 [00:02<00:04,  5.68it/s, loss=152, v_num=0, train_loss_step=109.0, val_loss=219.0, train_loss_epoch=232.0]\u001b[0m\n",
      "\u001b[34mEpoch 1:  39%|███▉      | 16/41 [00:02<00:04,  5.69it/s, loss=152, v_num=0, train_loss_step=109.0, val_loss=219.0, train_loss_epoch=232.0]\u001b[0m\n",
      "\u001b[34mEpoch 1:  39%|███▉      | 16/41 [00:02<00:04,  5.69it/s, loss=152, v_num=0, train_loss_step=109.0, val_loss=219.0, train_loss_epoch=232.0]\u001b[0m\n",
      "\u001b[34mEpoch 1:  39%|███▉      | 16/41 [00:02<00:04,  5.69it/s, loss=151, v_num=0, train_loss_step=164.0, val_loss=219.0, train_loss_epoch=232.0]\u001b[0m\n",
      "\u001b[34mEpoch 1:  41%|████▏     | 17/41 [00:02<00:04,  5.69it/s, loss=151, v_num=0, train_loss_step=164.0, val_loss=219.0, train_loss_epoch=232.0]\u001b[0m\n",
      "\u001b[34mEpoch 1:  41%|████▏     | 17/41 [00:02<00:04,  5.69it/s, loss=151, v_num=0, train_loss_step=164.0, val_loss=219.0, train_loss_epoch=232.0]\u001b[0m\n",
      "\u001b[34mEpoch 1:  41%|████▏     | 17/41 [00:02<00:04,  5.69it/s, loss=152, v_num=0, train_loss_step=149.0, val_loss=219.0, train_loss_epoch=232.0]\u001b[0m\n",
      "\u001b[34mEpoch 1:  44%|████▍     | 18/41 [00:03<00:04,  5.69it/s, loss=152, v_num=0, train_loss_step=149.0, val_loss=219.0, train_loss_epoch=232.0]\u001b[0m\n",
      "\u001b[34mEpoch 1:  44%|████▍     | 18/41 [00:03<00:04,  5.69it/s, loss=152, v_num=0, train_loss_step=149.0, val_loss=219.0, train_loss_epoch=232.0]\u001b[0m\n",
      "\u001b[34mEpoch 1:  44%|████▍     | 18/41 [00:03<00:04,  5.69it/s, loss=150, v_num=0, train_loss_step=146.0, val_loss=219.0, train_loss_epoch=232.0]\u001b[0m\n",
      "\u001b[34mEpoch 1:  46%|████▋     | 19/41 [00:03<00:03,  5.70it/s, loss=150, v_num=0, train_loss_step=146.0, val_loss=219.0, train_loss_epoch=232.0]#015Epoch 1:  46%|████▋     | 19/41 [00:03<00:03,  5.70it/s, loss=150, v_num=0, train_loss_step=146.0, val_loss=219.0, train_loss_epoch=232.0]\u001b[0m\n",
      "\u001b[34mEpoch 1:  46%|████▋     | 19/41 [00:03<00:03,  5.69it/s, loss=146, v_num=0, train_loss_step=86.60, val_loss=219.0, train_loss_epoch=232.0]\u001b[0m\n",
      "\u001b[34mEpoch 1:  49%|████▉     | 20/41 [00:03<00:03,  5.68it/s, loss=146, v_num=0, train_loss_step=86.60, val_loss=219.0, train_loss_epoch=232.0]#015Epoch 1:  49%|████▉     | 20/41 [00:03<00:03,  5.68it/s, loss=146, v_num=0, train_loss_step=86.60, val_loss=219.0, train_loss_epoch=232.0]\u001b[0m\n",
      "\u001b[34mEpoch 1:  49%|████▉     | 20/41 [00:03<00:03,  5.68it/s, loss=144, v_num=0, train_loss_step=117.0, val_loss=219.0, train_loss_epoch=232.0]\u001b[0m\n",
      "\u001b[34mEpoch 1:  51%|█████     | 21/41 [00:03<00:03,  5.68it/s, loss=144, v_num=0, train_loss_step=117.0, val_loss=219.0, train_loss_epoch=232.0]#015Epoch 1:  51%|█████     | 21/41 [00:03<00:03,  5.68it/s, loss=144, v_num=0, train_loss_step=117.0, val_loss=219.0, train_loss_epoch=232.0]\u001b[0m\n",
      "\u001b[34mEpoch 1:  51%|█████     | 21/41 [00:03<00:03,  5.68it/s, loss=143, v_num=0, train_loss_step=118.0, val_loss=219.0, train_loss_epoch=232.0]\u001b[0m\n",
      "\u001b[34mEpoch 1:  54%|█████▎    | 22/41 [00:03<00:03,  5.68it/s, loss=143, v_num=0, train_loss_step=118.0, val_loss=219.0, train_loss_epoch=232.0]#015Epoch 1:  54%|█████▎    | 22/41 [00:03<00:03,  5.68it/s, loss=143, v_num=0, train_loss_step=118.0, val_loss=219.0, train_loss_epoch=232.0]\u001b[0m\n",
      "\u001b[34mEpoch 1:  54%|█████▎    | 22/41 [00:03<00:03,  5.68it/s, loss=146, v_num=0, train_loss_step=165.0, val_loss=219.0, train_loss_epoch=232.0]\u001b[0m\n",
      "\u001b[34mEpoch 1:  56%|█████▌    | 23/41 [00:04<00:03,  5.68it/s, loss=146, v_num=0, train_loss_step=165.0, val_loss=219.0, train_loss_epoch=232.0]\u001b[0m\n",
      "\u001b[34mEpoch 1:  56%|█████▌    | 23/41 [00:04<00:03,  5.68it/s, loss=146, v_num=0, train_loss_step=165.0, val_loss=219.0, train_loss_epoch=232.0]\u001b[0m\n",
      "\u001b[34mEpoch 1:  56%|█████▌    | 23/41 [00:04<00:03,  5.68it/s, loss=145, v_num=0, train_loss_step=138.0, val_loss=219.0, train_loss_epoch=232.0]\u001b[0m\n",
      "\u001b[34mEpoch 1:  59%|█████▊    | 24/41 [00:04<00:02,  5.69it/s, loss=145, v_num=0, train_loss_step=138.0, val_loss=219.0, train_loss_epoch=232.0]#015Epoch 1:  59%|█████▊    | 24/41 [00:04<00:02,  5.69it/s, loss=145, v_num=0, train_loss_step=138.0, val_loss=219.0, train_loss_epoch=232.0]\u001b[0m\n",
      "\u001b[34mEpoch 1:  59%|█████▊    | 24/41 [00:04<00:02,  5.69it/s, loss=143, v_num=0, train_loss_step=127.0, val_loss=219.0, train_loss_epoch=232.0]\u001b[0m\n",
      "\u001b[34mEpoch 1:  61%|██████    | 25/41 [00:04<00:02,  5.69it/s, loss=143, v_num=0, train_loss_step=127.0, val_loss=219.0, train_loss_epoch=232.0]#015Epoch 1:  61%|██████    | 25/41 [00:04<00:02,  5.69it/s, loss=143, v_num=0, train_loss_step=127.0, val_loss=219.0, train_loss_epoch=232.0]\u001b[0m\n",
      "\u001b[34mEpoch 1:  61%|██████    | 25/41 [00:04<00:02,  5.69it/s, loss=144, v_num=0, train_loss_step=224.0, val_loss=219.0, train_loss_epoch=232.0]\u001b[0m\n",
      "\u001b[34mEpoch 1:  63%|██████▎   | 26/41 [00:04<00:02,  5.68it/s, loss=144, v_num=0, train_loss_step=224.0, val_loss=219.0, train_loss_epoch=232.0]#015Epoch 1:  63%|██████▎   | 26/41 [00:04<00:02,  5.68it/s, loss=144, v_num=0, train_loss_step=224.0, val_loss=219.0, train_loss_epoch=232.0]\u001b[0m\n",
      "\u001b[34mEpoch 1:  63%|██████▎   | 26/41 [00:04<00:02,  5.68it/s, loss=145, v_num=0, train_loss_step=144.0, val_loss=219.0, train_loss_epoch=232.0]\u001b[0m\n",
      "\u001b[34mEpoch 1:  66%|██████▌   | 27/41 [00:04<00:02,  5.68it/s, loss=145, v_num=0, train_loss_step=144.0, val_loss=219.0, train_loss_epoch=232.0]\u001b[0m\n",
      "\u001b[34mEpoch 1:  66%|██████▌   | 27/41 [00:04<00:02,  5.68it/s, loss=145, v_num=0, train_loss_step=144.0, val_loss=219.0, train_loss_epoch=232.0]\u001b[0m\n",
      "\u001b[34mEpoch 1:  66%|██████▌   | 27/41 [00:04<00:02,  5.68it/s, loss=145, v_num=0, train_loss_step=134.0, val_loss=219.0, train_loss_epoch=232.0]\u001b[0m\n",
      "\u001b[34mEpoch 1:  68%|██████▊   | 28/41 [00:04<00:02,  5.68it/s, loss=145, v_num=0, train_loss_step=134.0, val_loss=219.0, train_loss_epoch=232.0]#015Epoch 1:  68%|██████▊   | 28/41 [00:04<00:02,  5.68it/s, loss=145, v_num=0, train_loss_step=134.0, val_loss=219.0, train_loss_epoch=232.0]\u001b[0m\n",
      "\u001b[34mEpoch 1:  68%|██████▊   | 28/41 [00:04<00:02,  5.68it/s, loss=146, v_num=0, train_loss_step=138.0, val_loss=219.0, train_loss_epoch=232.0]\u001b[0m\n",
      "\u001b[34mEpoch 1:  71%|███████   | 29/41 [00:05<00:02,  5.68it/s, loss=146, v_num=0, train_loss_step=138.0, val_loss=219.0, train_loss_epoch=232.0]\u001b[0m\n",
      "\u001b[34mEpoch 1:  71%|███████   | 29/41 [00:05<00:02,  5.68it/s, loss=146, v_num=0, train_loss_step=138.0, val_loss=219.0, train_loss_epoch=232.0]\u001b[0m\n",
      "\u001b[34mEpoch 1:  71%|███████   | 29/41 [00:05<00:02,  5.68it/s, loss=146, v_num=0, train_loss_step=173.0, val_loss=219.0, train_loss_epoch=232.0]\u001b[0m\n",
      "\u001b[34mEpoch 1:  73%|███████▎  | 30/41 [00:05<00:01,  5.69it/s, loss=146, v_num=0, train_loss_step=173.0, val_loss=219.0, train_loss_epoch=232.0]#015Epoch 1:  73%|███████▎  | 30/41 [00:05<00:01,  5.69it/s, loss=146, v_num=0, train_loss_step=173.0, val_loss=219.0, train_loss_epoch=232.0]\u001b[0m\n",
      "\u001b[34mEpoch 1:  73%|███████▎  | 30/41 [00:05<00:01,  5.69it/s, loss=145, v_num=0, train_loss_step=106.0, val_loss=219.0, train_loss_epoch=232.0]\u001b[0m\n",
      "\u001b[34mEpoch 1:  76%|███████▌  | 31/41 [00:05<00:01,  5.69it/s, loss=145, v_num=0, train_loss_step=106.0, val_loss=219.0, train_loss_epoch=232.0]#015Epoch 1:  76%|███████▌  | 31/41 [00:05<00:01,  5.69it/s, loss=145, v_num=0, train_loss_step=106.0, val_loss=219.0, train_loss_epoch=232.0]\u001b[0m\n",
      "\u001b[34mEpoch 1:  76%|███████▌  | 31/41 [00:05<00:01,  5.69it/s, loss=142, v_num=0, train_loss_step=107.0, val_loss=219.0, train_loss_epoch=232.0]\u001b[0m\n",
      "\u001b[34mEpoch 1:  78%|███████▊  | 32/41 [00:05<00:01,  5.68it/s, loss=142, v_num=0, train_loss_step=107.0, val_loss=219.0, train_loss_epoch=232.0]\u001b[0m\n",
      "\u001b[34mEpoch 1:  78%|███████▊  | 32/41 [00:05<00:01,  5.68it/s, loss=142, v_num=0, train_loss_step=107.0, val_loss=219.0, train_loss_epoch=232.0]\u001b[0m\n",
      "\u001b[34mEpoch 1:  78%|███████▊  | 32/41 [00:05<00:01,  5.68it/s, loss=139, v_num=0, train_loss_step=121.0, val_loss=219.0, train_loss_epoch=232.0]\u001b[0m\n",
      "\u001b[34mEpoch 1:  80%|████████  | 33/41 [00:05<00:01,  5.68it/s, loss=139, v_num=0, train_loss_step=121.0, val_loss=219.0, train_loss_epoch=232.0]\u001b[0m\n",
      "\u001b[34mEpoch 1:  80%|████████  | 33/41 [00:05<00:01,  5.68it/s, loss=139, v_num=0, train_loss_step=121.0, val_loss=219.0, train_loss_epoch=232.0]\u001b[0m\n",
      "\u001b[34mEpoch 1:  80%|████████  | 33/41 [00:05<00:01,  5.68it/s, loss=138, v_num=0, train_loss_step=156.0, val_loss=219.0, train_loss_epoch=232.0]\u001b[0m\n",
      "\u001b[34mEpoch 1:  83%|████████▎ | 34/41 [00:05<00:01,  5.68it/s, loss=138, v_num=0, train_loss_step=156.0, val_loss=219.0, train_loss_epoch=232.0]#015Epoch 1:  83%|████████▎ | 34/41 [00:05<00:01,  5.68it/s, loss=138, v_num=0, train_loss_step=156.0, val_loss=219.0, train_loss_epoch=232.0]\u001b[0m\n",
      "\u001b[34mEpoch 1:  83%|████████▎ | 34/41 [00:05<00:01,  5.68it/s, loss=138, v_num=0, train_loss_step=134.0, val_loss=219.0, train_loss_epoch=232.0]\u001b[0m\n",
      "\u001b[34mEpoch 1:  85%|████████▌ | 35/41 [00:06<00:01,  5.69it/s, loss=138, v_num=0, train_loss_step=134.0, val_loss=219.0, train_loss_epoch=232.0]#015Epoch 1:  85%|████████▌ | 35/41 [00:06<00:01,  5.69it/s, loss=138, v_num=0, train_loss_step=134.0, val_loss=219.0, train_loss_epoch=232.0]\u001b[0m\n",
      "\u001b[34mEpoch 1:  85%|████████▌ | 35/41 [00:06<00:01,  5.69it/s, loss=142, v_num=0, train_loss_step=189.0, val_loss=219.0, train_loss_epoch=232.0]\u001b[0m\n",
      "\u001b[34mEpoch 1:  88%|████████▊ | 36/41 [00:06<00:00,  5.69it/s, loss=142, v_num=0, train_loss_step=189.0, val_loss=219.0, train_loss_epoch=232.0]\u001b[0m\n",
      "\u001b[34mEpoch 1:  88%|████████▊ | 36/41 [00:06<00:00,  5.69it/s, loss=142, v_num=0, train_loss_step=189.0, val_loss=219.0, train_loss_epoch=232.0]\u001b[0m\n",
      "\u001b[34mEpoch 1:  88%|████████▊ | 36/41 [00:06<00:00,  5.69it/s, loss=142, v_num=0, train_loss_step=159.0, val_loss=219.0, train_loss_epoch=232.0]\u001b[0m\n",
      "\u001b[34mEpoch 1:  90%|█████████ | 37/41 [00:06<00:00,  5.69it/s, loss=142, v_num=0, train_loss_step=159.0, val_loss=219.0, train_loss_epoch=232.0]#015Epoch 1:  90%|█████████ | 37/41 [00:06<00:00,  5.69it/s, loss=142, v_num=0, train_loss_step=159.0, val_loss=219.0, train_loss_epoch=232.0]\u001b[0m\n",
      "\u001b[34mEpoch 1:  90%|█████████ | 37/41 [00:06<00:00,  5.69it/s, loss=141, v_num=0, train_loss_step=137.0, val_loss=219.0, train_loss_epoch=232.0]\u001b[0m\n",
      "\u001b[34mEpoch 1:  93%|█████████▎| 38/41 [00:06<00:00,  5.69it/s, loss=141, v_num=0, train_loss_step=137.0, val_loss=219.0, train_loss_epoch=232.0]#015Epoch 1:  93%|█████████▎| 38/41 [00:06<00:00,  5.68it/s, loss=141, v_num=0, train_loss_step=137.0, val_loss=219.0, train_loss_epoch=232.0]\u001b[0m\n",
      "\u001b[34mEpoch 1:  93%|█████████▎| 38/41 [00:06<00:00,  5.68it/s, loss=142, v_num=0, train_loss_step=157.0, val_loss=219.0, train_loss_epoch=232.0]\u001b[0m\n",
      "\u001b[34mEpoch 1:  95%|█████████▌| 39/41 [00:06<00:00,  5.69it/s, loss=142, v_num=0, train_loss_step=157.0, val_loss=219.0, train_loss_epoch=232.0]#015Epoch 1:  95%|█████████▌| 39/41 [00:06<00:00,  5.69it/s, loss=142, v_num=0, train_loss_step=157.0, val_loss=219.0, train_loss_epoch=232.0]\u001b[0m\n",
      "\u001b[34mEpoch 1:  95%|█████████▌| 39/41 [00:06<00:00,  5.69it/s, loss=146, v_num=0, train_loss_step=179.0, val_loss=219.0, train_loss_epoch=232.0]\u001b[0m\n",
      "\u001b[34mEpoch 1:  98%|█████████▊| 40/41 [00:07<00:00,  5.69it/s, loss=146, v_num=0, train_loss_step=179.0, val_loss=219.0, train_loss_epoch=232.0]#015Epoch 1:  98%|█████████▊| 40/41 [00:07<00:00,  5.69it/s, loss=146, v_num=0, train_loss_step=179.0, val_loss=219.0, train_loss_epoch=232.0]\u001b[0m\n",
      "\u001b[34mEpoch 1:  98%|█████████▊| 40/41 [00:07<00:00,  5.69it/s, loss=145, v_num=0, train_loss_step=102.0, val_loss=219.0, train_loss_epoch=232.0]\u001b[0m\n",
      "\u001b[34mValidation: 0it [00:00, ?it/s]#033[A\u001b[0m\n",
      "\u001b[34mValidation:   0%|          | 0/1 [00:00<?, ?it/s]#033[A\u001b[0m\n",
      "\u001b[34mValidation DataLoader 0:   0%|          | 0/1 [00:00<?, ?it/s]#033[A\u001b[0m\n",
      "\u001b[34mValidation DataLoader 0: 100%|██████████| 1/1 [00:00<00:00,  4.82it/s]#033[A\u001b[0m\n",
      "\u001b[34mValidation DataLoader 0: 100%|██████████| 1/1 [00:00<00:00,  4.82it/s]#033[A\u001b[0m\n",
      "\u001b[34mEpoch 1: 100%|██████████| 41/41 [00:07<00:00,  5.61it/s, loss=145, v_num=0, train_loss_step=102.0, val_loss=219.0, train_loss_epoch=232.0]\u001b[0m\n",
      "\u001b[34mEpoch 1: 100%|██████████| 41/41 [00:07<00:00,  5.61it/s, loss=145, v_num=0, train_loss_step=102.0, val_loss=219.0, train_loss_epoch=232.0]\u001b[0m\n",
      "\u001b[34mEpoch 1: 100%|██████████| 41/41 [00:08<00:00,  5.12it/s, loss=145, v_num=0, train_loss_step=102.0, val_loss=188.0, train_loss_epoch=232.0]\u001b[0m\n",
      "\u001b[34m#015                                                                      #033[A\u001b[0m\n",
      "\u001b[34mEpoch 1: 100%|██████████| 41/41 [00:08<00:00,  5.11it/s, loss=145, v_num=0, train_loss_step=102.0, val_loss=188.0, train_loss_epoch=146.0]\u001b[0m\n",
      "\u001b[34mEpoch 1: 100%|██████████| 41/41 [00:08<00:00,  5.06it/s, loss=145, v_num=0, train_loss_step=102.0, val_loss=188.0, train_loss_epoch=146.0]\u001b[0m\n",
      "\u001b[34m2023-03-04 14:45:51,163 sagemaker-training-toolkit INFO     Waiting for the process to finish and give a return code.\u001b[0m\n",
      "\u001b[34m2023-03-04 14:45:51,163 sagemaker-training-toolkit INFO     Done waiting for a return code. Received 0 from exiting process.\u001b[0m\n",
      "\u001b[34m2023-03-04 14:45:51,163 sagemaker-training-toolkit INFO     Reporting training SUCCESS\u001b[0m\n",
      "\n",
      "2023-03-04 14:46:06 Uploading - Uploading generated training model\n",
      "2023-03-04 14:46:06 Completed - Training job completed\n",
      "Training seconds: 317\n",
      "Billable seconds: 317\n"
     ]
    }
   ],
   "source": [
    "estimator.logs()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "deb0cd08-bf35-4fb4-96cf-b89ae2f8f421",
   "metadata": {},
   "source": [
    "# 4. 모델 가중치 파일 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b6f718a0-c680-4570-8113-1a3c27521d27",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model artifact: \n",
      " s3://sagemaker-us-east-1-057716757052/pytorch-training-2023-03-04-14-39-32-806/output/model.tar.gz\n"
     ]
    }
   ],
   "source": [
    "print(\"model artifact: \\n\", estimator.model_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3564849c-9754-4a10-b417-86ec80a74d0f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
